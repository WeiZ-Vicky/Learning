{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">Spark SQL</span>\n",
    "<p>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<li>Apache Spark module for <span style=\"color:red\">structured data processing</span></li>\n",
    "<li>Implemented through two data structures</li>\n",
    "<ul>\n",
    "    <li><b>Datasets</b>: A distributed collection of data. Datasets are based on RDDs but are strongly typed and contain a optimized execution engine for fast execution. Available only in Scala and Java (mainly because Python is untyped)</li>\n",
    "    <li><b>Dataframes</b>: a dataset organized into named columns. Available in all all APIs.It is untyped.</li>\n",
    "    <li>The key difference between the two is that datasets are typed while dataframes are untyped</li>\n",
    "</ul>\n",
    "<li>Spark SQL uses DataFrames to provide structure to a data object</li>\n",
    "        <ul>\n",
    "            <li>A spark dataframe is a table like structure </li>\n",
    "            <li>Data is organized in <span style=\"color:blue\">named columns</span></li>\n",
    "   <li>dataframes are distributed across nodes in a cluster</li>\n",
    "        <li> dataframes are in-memory data objects</li>\n",
    "    <li>dataframes support lazy evaluation (transformations and actions)</li>\n",
    "        </ul>\n",
    "<li>Spark SQL provides SQL syntax to query dataframes</li>\n",
    "\n",
    "<li>Dataframes and SQL queries are optimized for execution speed</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Spark Session</span>\n",
    "<p>\n",
    "    <li>The entry point for a spark application</li>\n",
    "    <li>Early versions of Spark were RDD based and had only a Spark Context</li>\n",
    "    <li>Later, support was added for SQL in an SQLContext and for Hive (a data warehousing framework) in a HiveContext</li>\n",
    "    <li>SparkSession combines all these into a single global context that contains support for RDDs (spark context) and SQL (Sql context)</li>\n",
    "    <li>A spark session automatically creates a spark context. Jupyter notebooks don't need to create either</li>\n",
    "    <li>set config options (in the example below, we're setting the size of a partition in MB when shuffling data</li>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.56.168.254:4041\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1667759003977)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/06 13:23:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "sparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6200ca28\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val sparkSession = SparkSession\n",
    "                    .builder()\n",
    "                    .appName(\"myApp\")\n",
    "                    .config(\"spark.sql.shuffle.partitions\", 500)\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@1526c7d4\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.SparkContext = org.apache.spark.SparkContext@1526c7d4\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6200ca28\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6200ca28\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Creating dataframes</span>\n",
    "<p>\n",
    "<li>From a Scala sequence</li>\n",
    "<li>From an RDD</li>\n",
    "<li>Reading data from a file (csv, json, other formats)</li>\n",
    "<li>Scala Spark dataframes are composed of Row objects</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">From a scala sequence or an RDD</span>\n",
    "<li><span style=\"color:blue\">toDF</span> converts a scala sequence or an rdd into a data frame</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: Seq[(String, Int)] = List((Megan,200000), (Gao,450000), (Antonio,120000))\n",
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:26\n",
       "df_from_data: org.apache.spark.sql.DataFrame = [Name: string, Income: int]\n",
       "df_from_rdd: org.apache.spark.sql.DataFrame = [Name: string, Income: int]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq((\"Megan\",200000),(\"Gao\",450000),(\"Antonio\",120000))//ordered,immutable collection\n",
    "val rdd = sc.parallelize(data)\n",
    "val df_from_data = data.toDF(\"Name\",\"Income\")\n",
    "val df_from_rdd = rdd.toDF(\"Name\",\"Income\")\n",
    "\n",
    "//step 1,2, and 4. actually only have one step, \n",
    "//because rdd is not initiated, and action will determine what exactly transformation useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Income: integer (nullable = false)\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Income: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_data.printSchema\n",
    "df_from_rdd.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">from a csv file</span>\n",
    "<p>\n",
    "    <div class=\"list\">\n",
    "<li>The spark function <span style=\"color:blue\">read</span> reads the contents of a file into a dataframe</li>\n",
    "<li>The .format option specifies the format (csv, json, user-defined format)</li>\n",
    "        <li>Specify a header if the csv file has a (single) header line</li>\n",
    "        <li>Specify \"inferschema\" if you want spark to figure out the data types</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name,salary\r\n",
      "Michael,3000\r\n",
      "Andy,4500\r\n",
      "Justin,3500\r\n",
      "Berta,4000\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat employees.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [name: string, salary: int]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read //Jupyter automatically creates a sparksession with the identifer \"spark\"\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferschema\",\"true\")//\"Michael\" is a string, 3000 is an interger\n",
    "   .csv(\"employees.csv\")\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true),StructField(salary,IntegerType,true))\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">From a json file</span>\n",
    "<p>\n",
    "<li><span style=\"color:blue\">read.json</span> reads json files</li>\n",
    "<li>JSON: Must be single line json files</li>\n",
    "<li>Each record must be in a single line with no line breaks</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px\">\n",
    "<b>Single line json (This is OK!)</b>\n",
    "    <p>\n",
    "{\"name\":\"Michael\", \"salary\":3000,\"positions\":{\"NYC\":1,\"Palo Alto\":2}}<br>\n",
    "{\"name\":\"Andy\", \"salary\":4500,\"positions\":{\"NYC\":1,\"Palo Alto\":2}}<br>\n",
    "{\"name\":\"Justin\", \"salary\":3500,\"positions\":{\"NYC\":1,\"Palo Alto\":2}}<br>\n",
    "{\"name\":\"Berta\", \"salary\":4000,\"positions\":{\"NYC\":1,\"Palo Alto\":2}}<br>\n",
    "    \n",
    "    \n",
    "<b>Multi-line json (This is NOT OK!)</b>\n",
    "        <p>\n",
    "{\"name\":\"Michael\", \"salary\":3000,\"positions\":{<br>\n",
    "    \"NYC\":1,<br>\n",
    "    \"Palo Alto\":2}}<br>\n",
    "{\"name\":\"Andy\", \"salary\":4500,\"positions\":{<br>\n",
    "    \"NYC\":1,<br>\n",
    "    \"Palo Alto\":2}}<br>\n",
    "{\"name\":\"Justin\", \"salary\":3500,\"positions\":{<br>\n",
    "    \"NYC\":1,<br>\n",
    "    \"Palo Alto\":2}}<br>\n",
    "{\"name\":\"Berta\", \"salary\":4000,\"positions\":{<br>\n",
    "    \"NYC\":1,<br>\n",
    "    \"Palo Alto\":2}}<br>\n",
    "    </p>\n",
    "    </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Michael\", \"salary\":3000,\"positions\":{\"NYC\":1,\"Palo Alto\":2}}\r\n",
      "{\"name\":\"Andy\", \"salary\":4500,\"positions\":{\"NYC\":1}}\r\n",
      "{\"name\":\"Justin\", \"salary\":3500,\"positions\":{\"NYC\":1,\"Palo Alto\":2}}\r\n",
      "{\"name\":\"Berta\", \"salary\":4000,\"positions\":{\"NYC\":1,\"Palo Alto\":2}}\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat employees_singleline.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- positions: struct (nullable = true)\n",
      " |    |-- NYC: long (nullable = true)\n",
      " |    |-- Palo Alto: long (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [name: string, positions: struct<NYC: bigint, Palo Alto: bigint> ... 1 more field]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"employees_singleline.json\")\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true),StructField(positions,StructType(StructField(NYC,LongType,true),StructField(Palo Alto,LongType,true)),true),StructField(salary,LongType,true))\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+\n",
      "|   name|positions|salary|\n",
      "+-------+---------+------+\n",
      "|Michael|   {1, 2}|  3000|\n",
      "|   Andy|{1, null}|  4500|\n",
      "| Justin|   {1, 2}|  3500|\n",
      "|  Berta|   {1, 2}|  4000|\n",
      "+-------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">Programmatically specifying the schema</span>\n",
    "<li>Often, data arrives in a Spark application without a mechanism to infer the schema</li>\n",
    "<li>Since each df row is in a Spark Row object, it is possible to construct the schema programmatically</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.56.160.213:4046\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1671153413116)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "data: Array[String] = Array(John,33,245233.234, Jill,23,132987.22, Qing,54,782344.22, Rahul,50,389223,54)\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Array(\"John,33,245233.234\",\"Jill,23,132987.22\",\"Qing,54,782344.22\",\"Rahul,50,389223,54\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python float\n",
    "- Pandas float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:large\">Construct a schema</span>\n",
    "<p>\n",
    "        <li>import SPARK SQL types (StringType, IntegerType, etc.)</li>\n",
    "        <li>import SPARK SQL Row object</li>\n",
    "        <li>construct a StructField type for each column</li>\n",
    "        <li>construct a StructType for the schema</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.Row\n",
       "f1: org.apache.spark.sql.types.StructField = StructField(name,StringType,false)\n",
       "f2: org.apache.spark.sql.types.StructField = StructField(age,IntegerType,true)\n",
       "f3: org.apache.spark.sql.types.StructField = StructField(income,DoubleType,true)\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,false),StructField(age,IntegerType,true),StructField(income,DoubleType,true))\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._ //spark dataframe type\n",
    "import org.apache.spark.sql.Row //put f1,f2,f3 as row type\n",
    "//for each column\n",
    "val f1 = StructField(\"name\",StringType,nullable=false)\n",
    "val f2 = StructField(\"age\",IntegerType,nullable=true)\n",
    "val f3 = StructField(\"income\",DoubleType,nullable=true)\n",
    "\n",
    "val schema = StructType(Array(f1,f2,f3))//f1,f2,f3 in one row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:large\">Make Row objects and construct the dataframe</span>\n",
    "<p>\n",
    "        <li>make sure that the data types are correct in the rowRDD. They have to match the schema!</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Array[String] = Array(John,33,245233.234, Jill,23,132987.22, Qing,54,782344.22, Rahul,50,389223,54)\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- income: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[9] at map at <console>:33\n",
       "rowRdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[10] at map at <console>:34\n",
       "df: org.apache.spark.sql.DataFrame = [name: string, age: int ... 1 more field]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(data).map(r => r.split(\",\"))\n",
    "val rowRdd = rdd.map(a => Row(a(0),a(1).toInt,a(2).toDouble))\n",
    "val df = spark.createDataFrame(rowRdd,schema)\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| name|age|    income|\n",
      "+-----+---+----------+\n",
      "| John| 33|245233.234|\n",
      "| Jill| 23| 132987.22|\n",
      "| Qing| 54| 782344.22|\n",
      "|Rahul| 50|  389223.0|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Array[Array[String]] = Array(Array(John, 33, 245233.234), Array(Jill, 23, 132987.22), Array(Qing, 54, 782344.22), Array(Rahul, 50, 389223, 54))\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Array[org.apache.spark.sql.Row] = Array([John,33,245233.234], [Jill,23,132987.22], [Qing,54,782344.22], [Rahul,50,389223.0])\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowRdd.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Use array style indexing to extract data from a Row RDD</li>\n",
    "<li>Example: convert each Row object into a Scala List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Array[List[Any]] = Array(List(John, 33), List(Jill, 23), List(Qing, 54), List(Rahul, 50))\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowRdd.map(t=>List(t(0),t(1))).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: Array[org.apache.spark.sql.Row] = Array([John,33,245233.234], [Jill,23,132987.22], [Qing,54,782344.22], [Rahul,50,389223.0])\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowRdd.collect()\n",
    "// (1)(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Working with dataframes</span>\n",
    "<p>\n",
    "        <li><span style=\"color:blue\">df.schema</span> displays and returns the schema </li>\n",
    "        <li><span style=\"color:blue\">df.columns</span> returns the column names in an Array</li>\n",
    "        <li><span style=\"color:blue\">df.show</span> shows the dataframe in tabular format</li>\n",
    "        <ul>\n",
    "            <li>by default, show only displays 20 rows</li>\n",
    "            <li><span style=\"color:blue\">show with limit</span> controls the display length</li>            \n",
    "            <li><span style=\"color:blue\">take</span> also works, as does <span style=\"color:blue\">first</span></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- income: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "col_names: Array[String] = Array(name, age, income)\n",
       "res17: Array[String] = Array(name, age, income)\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val col_names = df.columns\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| name|age|    income|\n",
      "+-----+---+----------+\n",
      "| John| 33|245233.234|\n",
      "| Jill| 23| 132987.22|\n",
      "| Qing| 54| 782344.22|\n",
      "|Rahul| 50|  389223.0|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:large\">Seeing only a few rows</span>\n",
    "<li>use <span style=\"color:blue\">limit</span> to see a few rows</li>\n",
    "<li>you can also use <span style=\"color:blue\">take</span> but that will use rdd style display (no structure information)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "|name|age|    income|\n",
      "+----+---+----------+\n",
      "|John| 33|245233.234|\n",
      "|Jill| 23| 132987.22|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(2).show//default show 20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: Array[org.apache.spark.sql.Row] = Array([John,33,245233.234], [Jill,23,132987.22], [Qing,54,782344.22], [Rahul,50,389223.0])\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Dataframe operations</span>\n",
    "<li><span style=\"color:blue\">select</span> chooses data from columns and can run grouped and aggregate operations on the dataframe</li>\n",
    "<li>select returns a dataframe</li>\n",
    "<li>select is a transformation and not an action</li>\n",
    "<li>A \\$ sign in front of a column name lets you use the column values like a variable</li>\n",
    "<li><span style=\"color:blue\">groupby</span> groups data on a column and can be used to get summary stats</li>\n",
    "<li><span style=\"color:blue\">filter</span> selects rows from a dataframe based on a condition</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "|John|\n",
      "|Jill|\n",
      "|John|\n",
      "|Jill|\n",
      "+----+\n",
      "\n",
      "+----+-----+\n",
      "|Name|Score|\n",
      "+----+-----+\n",
      "|John|   10|\n",
      "|Jill|    8|\n",
      "|John|    3|\n",
      "|Jill|    9|\n",
      "+----+-----+\n",
      "\n",
      "+----+------------------------+\n",
      "|Name|((Score * 100.0) / 10.0)|\n",
      "+----+------------------------+\n",
      "|John|                   100.0|\n",
      "|Jill|                    80.0|\n",
      "|John|                    30.0|\n",
      "|Jill|                    90.0|\n",
      "+----+------------------------+\n",
      "\n",
      "+----+----------+\n",
      "|Name|percentage|\n",
      "+----+----------+\n",
      "|John|     100.0|\n",
      "|Jill|      80.0|\n",
      "|John|      30.0|\n",
      "|Jill|      90.0|\n",
      "+----+----------+\n",
      "\n",
      "+----+----------+\n",
      "|Name|avg(Score)|\n",
      "+----+----------+\n",
      "|John|       6.5|\n",
      "|Jill|       8.5|\n",
      "+----+----------+\n",
      "\n",
      "+----+----+-----+\n",
      "|Name|Quiz|Score|\n",
      "+----+----+-----+\n",
      "|John|  Q1|   10|\n",
      "|Jill|  Q2|    9|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x: Array[(String, String, Int)] = Array((John,Q1,10), (Jill,Q1,8), (John,Q2,3), (Jill,Q2,9))\n",
       "rdd: org.apache.spark.rdd.RDD[(String, String, Int)] = ParallelCollectionRDD[28] at parallelize at <console>:35\n",
       "df: org.apache.spark.sql.DataFrame = [Name: string, Quiz: string ... 1 more field]\n",
       "names: org.apache.spark.sql.DataFrame = [Name: string]\n",
       "scores: org.apache.spark.sql.DataFrame = [Name: string, Score: int]\n",
       "percentages: org.apache.spark.sql.DataFrame = [Name: string, ((Score * 100.0) / 10.0): double]\n",
       "percentages2: org.apache.spark.sql.DataFrame = [Name: string, percentage: double]\n",
       "grouped: org.apache.spark.sql.RelationalGroupedDataset = RelationalGroupedDataset: [grouping expressions: [Name: string], value: [Name: string, Quiz: string ... 1 more field], type: ]\n",
       "a_grades: org.apache.spar...\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = Array((\"John\",\"Q1\",10),\n",
    "              (\"Jill\",\"Q1\",8),\n",
    "              (\"John\",\"Q2\",3),\n",
    "              (\"Jill\",\"Q2\",9))\n",
    "val rdd = sc.parallelize(x)\n",
    "val df = rdd.toDF(\"Name\",\"Quiz\",\"Score\")\n",
    "val names = df.select(\"Name\")\n",
    "names.show\n",
    "val scores = df.select(\"Name\",\"Score\")\n",
    "scores.show\n",
    "val percentages = df.select($\"Name\",$\"Score\"*100.0/10.0)//$ -> trade \"Score\" as column. when you need one, you need to do every one. \n",
    "percentages.show\n",
    "val percentages2 = df.select($\"Name\",$\"Score\"*100.0/10.0 as \"percentage\")\n",
    "percentages2.show\n",
    "val grouped = df.groupBy(\"Name\")\n",
    "grouped.mean(\"Score\").show\n",
    "val a_grades = df.filter($\"Score\">=9.0)\n",
    "a_grades.show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:large\">Working with columns</span>\n",
    "<li>Adding a $ in front of a column name enables vector operations on a column</li>\n",
    "<li>The <span style=\"color:blue\">col</span> function can also be used to specify a column</li>\n",
    "<li>two functions <span style=\"color:blue\">withColumn</span> and <span style=\"color:blue\">withColumnRenamed</span> add flexibility to columnar operations</li>\n",
    "<li>withColumn takes two arguments, the name for the new column being created and an operation on a column, and returns a new dataframe</li>\n",
    "<li>withColumnRenamed takes two arguments, the name of an existing column and the new name for that column and returns a new data frame</li>\n",
    "<li>conditional expressions on columns use <span style=\"color:blue\">when</span> and <span style=\"color:blue\">otherwise</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Quiz: string (nullable = true)\n",
      " |-- Score: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+\n",
      "|Name|((Score * 100.0) / 10.0)|\n",
      "+----+------------------------+\n",
      "|John|                   100.0|\n",
      "|Jill|                    80.0|\n",
      "|John|                    30.0|\n",
      "|Jill|                    90.0|\n",
      "+----+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "percentage: org.apache.spark.sql.DataFrame = [Name: string, ((Score * 100.0) / 10.0): double]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val percentage = df.select($\"Name\",$\"Score\"*100.0/10.0)\n",
    "percentage.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+\n",
      "|Name|((Score * 100.0) / 10.0)|\n",
      "+----+------------------------+\n",
      "|John|                   100.0|\n",
      "|Jill|                    80.0|\n",
      "|John|                    30.0|\n",
      "|Jill|                    90.0|\n",
      "+----+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "percentage: org.apache.spark.sql.DataFrame = [Name: string, ((Score * 100.0) / 10.0): double]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val percentage = df.select($\"Name\",col(\"Score\")*100.0/10.0)\n",
    "percentage.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+----------+\n",
      "|Name|Quiz|Score|percentage|\n",
      "+----+----+-----+----------+\n",
      "|John|  Q1|   10|     100.0|\n",
      "|Jill|  Q1|    8|      80.0|\n",
      "|John|  Q2|    3|      30.0|\n",
      "|Jill|  Q2|    9|      90.0|\n",
      "+----+----+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "percentage: org.apache.spark.sql.DataFrame = [Name: string, Quiz: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//percentage is a new column in the df\n",
    "val percentage = df.withColumn(\"percentage\",col(\"Score\")*100.0/10)\n",
    "percentage.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+-----------+\n",
      "|Name|Quiz|Score|final score|\n",
      "+----+----+-----+-----------+\n",
      "|John|  Q1|   10|      100.0|\n",
      "|Jill|  Q1|    8|       80.0|\n",
      "|John|  Q2|    3|       30.0|\n",
      "|Jill|  Q2|    9|       90.0|\n",
      "+----+----+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "finals: org.apache.spark.sql.DataFrame = [Name: string, Quiz: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//finals is a df with column percentage renamed as final score\n",
    "val finals = percentage.withColumnRenamed(\"percentage\",\"final score\")\n",
    "finals.show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:large\">Conditional column values</span>\n",
    "<li><span style=\"color:red\">when</span> generates new column values conditionally</li>\n",
    "<li>combine with <span style=\"color:red\">withColumn</span> to create a new column</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+----------+-----+\n",
      "|Name|Quiz|Score|percentage|grade|\n",
      "+----+----+-----+----------+-----+\n",
      "|John|  Q1|   10|     100.0|    A|\n",
      "|Jill|  Q1|    8|      80.0|    B|\n",
      "|John|  Q2|    3|      30.0|    F|\n",
      "|Jill|  Q2|    9|      90.0|    A|\n",
      "+----+----+-----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "grades: org.apache.spark.sql.DataFrame = [Name: string, Quiz: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val grades = percentage.withColumn(\"grade\",\n",
    "                      when(col(\"percentage\")>=90.0,\"A\")\n",
    "                      .when(col(\"percentage\")>=80.0,\"B\")\n",
    "                        .otherwise( \"F\"))\n",
    "grades.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:large\">Accessing nested columns</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|Name|Quiz|Score|\n",
      "+----+----+-----+\n",
      "|John|  Q1|   10|\n",
      "|Jill|  Q1|    8|\n",
      "|John|  Q2|    3|\n",
      "|Jill|  Q2|    9|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- positions: struct (nullable = true)\n",
      " |    |-- NYC: long (nullable = true)\n",
      " |    |-- Palo Alto: long (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [name: string, positions: struct<NYC: bigint, Palo Alto: bigint> ... 1 more field]\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"employees_singleline.json\")\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+\n",
      "|   name|positions|salary|\n",
      "+-------+---------+------+\n",
      "|Michael|   {1, 2}|  3000|\n",
      "|   Andy|{1, null}|  4500|\n",
      "| Justin|   {1, 2}|  3500|\n",
      "|  Berta|   {1, 2}|  4000|\n",
      "+-------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|positions|\n",
      "+---------+\n",
      "|   {1, 2}|\n",
      "|{1, null}|\n",
      "|   {1, 2}|\n",
      "|   {1, 2}|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"positions\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|NYC|\n",
      "+-------+---+\n",
      "|Michael|  1|\n",
      "|   Andy|  1|\n",
      "| Justin|  1|\n",
      "|  Berta|  1|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\",\"positions.NYC\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">User-defined functions</span>\n",
    "<p>\n",
    "        <li>It is possible to define a function that can be applied to each element in a column (or columns)</li>\n",
    "        <li>Define a function that works on one element</li>\n",
    "    <li>add it to the <span style=\"color:red\">udf</span> library</li>\n",
    "        <li>apply it to a column (or columns)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|Name|Quiz|Score|\n",
      "+----+----+-----+\n",
      "|John|  Q1|   10|\n",
      "|Jill|  Q1|    8|\n",
      "|John|  Q2|    3|\n",
      "|Jill|  Q2|    9|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x: Array[(String, String, Int)] = Array((John,Q1,10), (Jill,Q1,8), (John,Q2,3), (Jill,Q2,9))\n",
       "rdd: org.apache.spark.rdd.RDD[(String, String, Int)] = ParallelCollectionRDD[92] at parallelize at <console>:36\n",
       "df: org.apache.spark.sql.DataFrame = [Name: string, Quiz: string ... 1 more field]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = Array((\"John\",\"Q1\",10),\n",
    "              (\"Jill\",\"Q1\",8),\n",
    "              (\"John\",\"Q2\",3),\n",
    "              (\"Jill\",\"Q2\",9))\n",
    "val rdd = sc.parallelize(x)\n",
    "val df = rdd.toDF(\"Name\",\"Quiz\",\"Score\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "F\n",
      "B\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "grader: (x: Int)String\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader(x: Int): String = \n",
    "    if (x >= 9) \"A\"\n",
    "    else if (x >= 8) \"B\"\n",
    "    else \"F\"\n",
    "\n",
    "println(grader(9))\n",
    "println(grader(3))\n",
    "println(grader(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+-----+\n",
      "|Name|Quiz|Score|grade|\n",
      "+----+----+-----+-----+\n",
      "|John|  Q1|   10|    A|\n",
      "|Jill|  Q1|    8|    B|\n",
      "|John|  Q2|    3|    F|\n",
      "|Jill|  Q2|    9|    A|\n",
      "+----+----+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.udf\n",
       "grader_udf: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4966/0x0000000801a29040@2400752b,StringType,List(Some(class[value[0]: int])),Some(class[value[0]: string]),None,true,true)\n",
       "grades_df: org.apache.spark.sql.DataFrame = [Name: string, Quiz: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "val grader_udf = udf(grader _) //defines a udf as \"Call grader with arguments\" for cols\n",
    "val grades_df = df.withColumn(\"grade\",grader_udf($\"Score\"))\n",
    "grades_df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Aggregate functions</span>\n",
    "<p>\n",
    "<li><a href=\"https://spark.apache.org/docs/2.4.7/api/scala/index.html#org.apache.spark.sql.functions$\">documentation</a></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|max(Score)|stddev_samp(Score)|\n",
      "+----------+------------------+\n",
      "|        10| 2.701851217221259|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(max(\"Score\"),stddev(\"Score\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(Score)|\n",
      "+----------+\n",
      "|        10|\n",
      "+----------+\n",
      "\n",
      "+------------------+\n",
      "|stddev_samp(Score)|\n",
      "+------------------+\n",
      "|3.1091263510296048|\n",
      "+------------------+\n",
      "\n",
      "+----------+\n",
      "|avg(Score)|\n",
      "+----------+\n",
      "|       7.5|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(max(\"Score\")).show\n",
    "df.agg(stddev(\"Score\")).show\n",
    "df.agg(avg(\"Score\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">User defined aggregate functions</span>\n",
    "        <li>Example: compute the mean processing time for cases that take more than one day</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Created_Date: timestamp, Closed_Date: timestamp ... 9 more fields]\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "   .csv(\"nyc_311_2022_clean.csv\")\n",
    "    .withColumnRenamed(\"Created Date\",\"Created_Date\")\n",
    "    .withColumnRenamed(\"Closed Date\",\"Closed_Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Created_Date: timestamp (nullable = true)\n",
      " |-- Closed_Date: timestamp (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Agency Name: string (nullable = true)\n",
      " |-- Complaint Type: string (nullable = true)\n",
      " |-- Incident Zip: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- processing_time: string (nullable = true)\n",
      " |-- processing_days: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/06 15:33:04 WARN SimpleFunctionRegistry: The function conditionalaverage replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{Row, SparkSession}\n",
       "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
       "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
       "import org.apache.spark.sql.types._\n",
       "defined object ConditionalAverage\n",
       "res44: org.apache.spark.sql.expressions.UserDefinedAggregateFunction = ConditionalAverage$@313096c2\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Necessary imports\n",
    "import org.apache.spark.sql.{Row, SparkSession}\n",
    "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
    "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "//We'll define a scala program that calculates this average\n",
    "//Scala programs are contained in an object\n",
    "//object can only have one instance\n",
    "//object, class, data, function are the same level\n",
    "object ConditionalAverage extends UserDefinedAggregateFunction {\n",
    "    //Start with a schema for the input to the function\n",
    "    //In this case, a column and the type of the column\n",
    "    def inputSchema: StructType = StructType(StructField(\"inputColumn\", DoubleType) ::\n",
    "                                             //StructField(\"inputColumn\", StringType)\n",
    "                                             Nil)\n",
    "\n",
    "    // Set up an aggregation buffer. This specified the schema for the data that will be updated by each case\n",
    "    // Since we're calculating an average, we need to count elements and sum elements\n",
    "    def bufferSchema: StructType = {\n",
    "        StructType(StructField(\"sum\", DoubleType) :: StructField(\"count\", LongType) :: Nil)\n",
    "    }\n",
    "    \n",
    "    //Define the type of the value returned by the function\n",
    "    def dataType: DataType = DoubleType\n",
    "    \n",
    "    // Whether this function always returns the same output on the identical input\n",
    "    def deterministic: Boolean = true\n",
    "    \n",
    "    //Initialize the aggregation buffer\n",
    "    def initialize(buffer: MutableAggregationBuffer): Unit = {\n",
    "        buffer(0) = 0.0\n",
    "        buffer(1) = 0L //0 long \n",
    "    }\n",
    "    \n",
    "    // Update procedure. What to do with each case\n",
    "    //the buffer argument is the buffer above\n",
    "    //the input argument is a single row. For each row, it extracts the data from the column or columns\n",
    "    def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n",
    "        if (!input.isNullAt(0)) { //0 is the location\n",
    "            if (input.getDouble(0) > 1.0) {\n",
    "                buffer(0) = buffer.getDouble(0) + input.getDouble(0) //The sum of the values\n",
    "                buffer(1) = buffer.getLong(1) + 1 //The count of the values\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Merges two aggregation buffers and stores the updated buffer values back to `buffer1`\n",
    "    // Because - partitions!\n",
    "    // buffer1 is the final buffer\n",
    "    def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n",
    "        buffer1(0) = buffer1.getDouble(0) + buffer2.getDouble(0)\n",
    "        buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)\n",
    "    }\n",
    "    // Calculates the final result (the average)\n",
    "    //the evaluate function converts the buffer into the output\n",
    "    def evaluate(buffer: Row): Double = buffer.getDouble(0) / buffer.getLong(1)\n",
    "}\n",
    "\n",
    "// Register the function to access it\n",
    "spark.udf.register(\"ConditionalAverage\", ConditionalAverage) //first argument is name, second is function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>object ConditionalAverage extends UserDefinedAggregateFunction</b>\n",
    "\n",
    "* Since the function has several components, collect it inside an object (if unparameterized) or a class (if parameterized)\n",
    "* And we extend the UserDefinedAggregateFunction class. Note that the buffers etc. defined below are attributes of this class\n",
    "\n",
    "<b>def inputSchema: StructType = StructType(StructField(\"inputColumn\", DoubleType) :: Nil)</b>\n",
    "\n",
    "The input schema sets up the columns that the aggregate function will use. Multiple columns are permitted (constants use \"lit\") and the schema is organized as a sequence of StructFields. In the example above, the sequence has only one element and we're constructing the sequence by cons-ing that element with Nil. \n",
    "\n",
    "Types in StructField are obvious: DoubleType, LongType, IntType, BooleanType, StringType, etc. See <a href=\"https://spark.apache.org/docs/latest/sql-ref-datatypes.html\">the documentation</a> for all types\n",
    "\n",
    "<b>def bufferSchema: StructType = {\n",
    "        StructType(StructField(\"sum\", DoubleType) :: StructField(\"count\", LongType) :: Nil)\n",
    "    }</b>\n",
    "    \n",
    "An aggregator takes each row and does something with the values in that row. The result of that \"something\" is maintained in the buffer. The buffer is also a sequence of StructField like the input schema. In our example, we need to maintain the sum and the count as the function iterates through values \n",
    "\n",
    "<b>def dataType: DataType = DoubleType</b>\n",
    "\n",
    "The data type of the \"aggregated\" value returned by the function\n",
    "\n",
    "<b>def deterministic: Boolean = true</b>\n",
    "\n",
    "If true, the function always return the same value for the same input. deterministic may be false if, for example, there is a lot of data and the aggregate function randomly samples some of the data rather than all of it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|conditionalaverage$(processing_days)|\n",
      "+------------------------------------+\n",
      "|                  19.811219247322303|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(ConditionalAverage(df(\"processing_days\"))).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "// df.agg(avg(\"processing_days\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example 2</b>\n",
    "\n",
    "<li>Suppose we want to be able to specify the threshold value (i.e., the 1 day in our example above)</li>\n",
    "<li>We will add a second argument (using Cons) to the inputSchema</li>\n",
    "<li>And use this value in the update procedure instead of 1.0</li>\n",
    "<li>Since sparkSQL requires that all arguments to inputSchema be Column data types, we need to convert the threshold value into a column when calling the aggregate function. The <b>lit</b> function does this</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/26 10:31:49 WARN SimpleFunctionRegistry: The function conditionalaverage replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{Row, SparkSession}\n",
       "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
       "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
       "import org.apache.spark.sql.types._\n",
       "defined object ConditionalAverage\n",
       "res45: org.apache.spark.sql.expressions.UserDefinedAggregateFunction = ConditionalAverage$@1d8929ab\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Necessary imports\n",
    "import org.apache.spark.sql.{Row, SparkSession}\n",
    "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
    "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "//We'll define a scala program that calculates this average\n",
    "//Scala programs are contained in an object\n",
    "object ConditionalAverage extends UserDefinedAggregateFunction {\n",
    "    //Start with a schema for the input to the function\n",
    "    //In this case, a column and the type of the column\n",
    "    def inputSchema: StructType = StructType(StructField(\"inputColumn\", DoubleType) :: \n",
    "                                             StructField(\"threshold\",DoubleType) :: Nil)\n",
    "\n",
    "    // Set up an aggregation buffer. This specified the schema for the data that will be updated by each case\n",
    "    // Since we're calculating an average, we need to count elements and sum elements\n",
    "    def bufferSchema: StructType = {\n",
    "        StructType(StructField(\"sum\", DoubleType) :: StructField(\"count\", LongType) :: Nil)\n",
    "    }\n",
    "    \n",
    "    //Define the type of the value returned by the function\n",
    "    def dataType: DataType = DoubleType\n",
    "    \n",
    "    // Whether this function always returns the same output on the identical input\n",
    "    def deterministic: Boolean = true\n",
    "    \n",
    "    //Initialize the aggregation buffer\n",
    "    def initialize(buffer: MutableAggregationBuffer): Unit = {\n",
    "        buffer(0) = 0.0\n",
    "        buffer(1) = 0L\n",
    "    }\n",
    "    \n",
    "    // Update procedure. What to do with each case\n",
    "    def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n",
    "        if (!input.isNullAt(0)) {\n",
    "            if (input.getDouble(0) > input.getDouble(1)) {\n",
    "                buffer(0) = buffer.getDouble(0) + input.getDouble(0)\n",
    "                buffer(1) = buffer.getLong(1) + 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Merges two aggregation buffers and stores the updated buffer values back to `buffer1`\n",
    "    def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n",
    "        buffer1(0) = buffer1.getDouble(0) + buffer2.getDouble(0)\n",
    "        buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)\n",
    "    }\n",
    "  // Calculates the final result\n",
    "    def evaluate(buffer: Row): Double = buffer.getDouble(0) / buffer.getLong(1)\n",
    "}\n",
    "\n",
    "// Register the function to access it\n",
    "spark.udf.register(\"ConditionalAverage\", ConditionalAverage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|conditionalaverage$(processing_days, 1.0)|\n",
      "+-----------------------------------------+\n",
      "|                       19.811219247322303|\n",
      "+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(ConditionalAverage(df(\"processing_days\"),lit(1.0))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Question</h4>\n",
    "What modifications would we need to do to ConditionalAverage if we wanted to:\n",
    "<li>use an interval (e.g., average of times between 1 and 5 days)</li>\n",
    "<li>include something that lets us flip between greater and lesser</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res47: org.apache.spark.sql.Column = 1.0\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res48: org.apache.spark.sql.Column = processing_days\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df(\"processing_days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res51: org.apache.spark.sql.DataFrame = [processing_days: double]\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(col(\"processing_days\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Running SQL queries</span>\n",
    "<p>\n",
    "<li>Since a dataframe is like an SQL table (like-ish)</li>\n",
    "<li> We can run SQL queries on a df</li>\n",
    "<li>Set up a temporary view or a global view</li>\n",
    "<li>Send SQL queries to the dataframe</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Date,Closed Date,Agency,Agency Name,Complaint Type,Incident Zip,Borough,Latitude,Longitude,processing_time,processing_days\r\n",
      "2020-01-07 14:09:00,2020-01-13 11:20:00,DSNY,Department of Sanitation,Electronics Waste Appointment,11692,QUEENS,40.58993519447414,-73.78942049765358,5 days 21:11:00,5.882638888888889\r\n",
      "2020-01-04 11:37:00,2020-01-08 13:19:00,DSNY,Department of Sanitation,Electronics Waste Appointment,10310,STATEN ISLAND,40.62719924888892,-74.11245623591475,4 days 01:42:00,4.070833333333334\r\n",
      "2020-01-03 16:33:00,2020-01-05 00:00:00,DSNY,Department of Sanitation,Request Large Bulky Item Collection,11213,BROOKLYN,40.6672052181697,-73.93463635283278,1 days 07:27:00,1.3104166666666668\r\n",
      "2020-01-06 17:27:00,2020-01-11 00:00:00,DSNY,Department of Sanitation,Request Large Bulky Item Collection,11379,QUEENS,40.72687041685842,-73.8769198480706,4 days 06:33:00,4.272916666666666\r\n",
      "2020-01-06 09:46:00,2020-01-08 00:00:00,DSNY,Department of Sanitation,Request Large Bulky Item Collection,11221,BROOKLYN,40.68911879550976,-73.93833612355152,1 days 14:14:00,1.5930555555555554\r\n",
      "2020-01-07 17:28:00,2020-01-10 00:00:00,DSNY,Department of Sanitation,Request Large Bulky Item Collection,10312,STATEN ISLAND,40.548966463293326,-74.1996266740312,2 days 06:32:00,2.272222222222222\r\n",
      "2020-01-05 19:19:00,2020-01-11 00:00:00,DSNY,Department of Sanitation,Request Large Bulky Item Collection,11209,BROOKLYN,40.63391227749497,-74.02866091480665,5 days 04:41:00,5.195138888888889\r\n",
      "2020-01-04 19:31:00,2020-01-07 00:00:00,DSNY,Department of Sanitation,Request Large Bulky Item Collection,11230,BROOKLYN,40.62365164235015,-73.96898036482935,2 days 04:29:00,2.1868055555555554\r\n",
      "2020-01-07 12:28:00,2020-01-10 00:00:00,DSNY,Department of Sanitation,Request Large Bulky Item Collection,11237,BROOKLYN,40.70478445851195,-73.9223006179736,2 days 11:32:00,2.4805555555555556\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head nyc_311_2022_clean.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Created_Date: string, Closed_Date: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    //.option(\"inferSchema\",\"true\")\n",
    "   .csv(\"nyc_311_2022_clean.csv\")\n",
    "    .withColumnRenamed(\"Created Date\",\"Created_Date\")\n",
    "    .withColumnRenamed(\"Closed Date\",\"Closed_Date\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Created_Date: string (nullable = true)\n",
      " |-- Closed_Date: string (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Agency Name: string (nullable = true)\n",
      " |-- Complaint Type: string (nullable = true)\n",
      " |-- Incident Zip: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- processing_time: string (nullable = true)\n",
      " |-- processing_days: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-01-07 14:09:00,2020-01-13 11:20:00,DSNY,Department of Sanitation,Electronics Waste Appointment,11692,QUEENS,40.58993519447414,-73.78942049765358,5 days 21:11:00,5.882638888888889]\n",
      "[2020-01-04 11:37:00,2020-01-08 13:19:00,DSNY,Department of Sanitation,Electronics Waste Appointment,10310,STATEN ISLAND,40.62719924888892,-74.11245623591475,4 days 01:42:00,4.070833333333334]\n",
      "[2020-01-03 16:33:00,2020-01-05 00:00:00,DSNY,Department of Sanitation,Request Large Bulky Item Collection,11213,BROOKLYN,40.6672052181697,-73.93463635283278,1 days 07:27:00,1.3104166666666668]\n",
      "[2020-01-06 17:27:00,2020-01-11 00:00:00,DSNY,Department of Sanitation,Request Large Bulky Item Collection,11379,QUEENS,40.72687041685842,-73.8769198480706,4 days 06:33:00,4.272916666666666]\n",
      "[2020-01-06 09:46:00,2020-01-08 00:00:00,DSNY,Department of Sanitation,Request Large Bulky Item Collection,11221,BROOKLYN,40.68911879550976,-73.93833612355152,1 days 14:14:00,1.5930555555555554]\n"
     ]
    }
   ],
   "source": [
    "df.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">SQL table and queries</span>\n",
    "        <li>Create an SQL table</li>\n",
    "        <li>And then query it using SQL syntax</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Created_Date: string (nullable = true)\n",
      " |-- Closed_Date: string (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Agency Name: string (nullable = true)\n",
      " |-- Complaint Type: string (nullable = true)\n",
      " |-- Incident Zip: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- processing_time: string (nullable = true)\n",
      " |-- processing_days: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_df: org.apache.spark.sql.DataFrame = [agency: string, processing_time: string]\n",
       "nypd_high: org.apache.spark.sql.DataFrame = [Created_Date: string, Closed_Date: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"complaints\") //Creates a temporary SQL-like table\n",
    "val time_df = spark.sql(\"select agency, processing_time from complaints\")\n",
    "val nypd_high = spark.sql(\"select * FROM complaints WHERE agency='NYPD' AND processing_days>0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|agency|processing_time|\n",
      "+------+---------------+\n",
      "|  DSNY|5 days 21:11:00|\n",
      "|  DSNY|4 days 01:42:00|\n",
      "|  DSNY|1 days 07:27:00|\n",
      "|  DSNY|4 days 06:33:00|\n",
      "+------+---------------+\n",
      "\n",
      "+-------------------+-------------------+------+--------------------+-------------------+------------+-------+------------------+------------------+---------------+------------------+\n",
      "|       Created_Date|        Closed_Date|Agency|         Agency Name|     Complaint Type|Incident Zip|Borough|          Latitude|         Longitude|processing_time|   processing_days|\n",
      "+-------------------+-------------------+------+--------------------+-------------------+------------+-------+------------------+------------------+---------------+------------------+\n",
      "|2019-07-18 01:27:44|2019-07-18 16:18:24|  NYPD|New York City Pol...|Noise - Residential|       10466|  BRONX| 40.88797508393539|-73.85580773230332|0 days 14:50:40|0.6185185185185185|\n",
      "|2020-03-15 19:51:35|2020-03-17 02:09:05|  NYPD|New York City Pol...|   Blocked Driveway|       11434| QUEENS| 40.68454981919937| -73.7902755152974|1 days 06:17:30|1.2621527777777777|\n",
      "|2020-03-15 11:33:19|2020-03-18 05:54:32|  NYPD|New York City Pol...|Noise - Residential|       10469|  BRONX|  40.8818219228322|-73.84969503141993|2 days 18:21:13| 2.764733796296296|\n",
      "|2020-03-15 02:20:15|2020-03-16 01:49:58|  NYPD|New York City Pol...|   Blocked Driveway|       10460|  BRONX|40.840918812343375|-73.86865664223183|0 days 23:29:43|0.9789699074074075|\n",
      "+-------------------+-------------------+------+--------------------+-------------------+------------+-------+------------------+------------------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_df.limit(4).show\n",
    "nypd_high.limit(4).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">Using variables inside an SQL query</span>\n",
    "<li>Because SQL query values need to be quoted inside the query string, variables require special care</li>\n",
    "<li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Column 'ag' does not exist. Did you mean one of the following? [complaints.Agency, complaints.Borough, complaints.Latitude, complaints.Longitude, complaints.Agency Name, complaints.Closed_Date, complaints.Created_Date, complaints.Incident Zip, complaints.Complaint Type, complaints.processing_days, complaints.processing_time]; line 1 pos 38;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Column 'ag' does not exist. Did you mean one of the following? [complaints.Agency, complaints.Borough, complaints.Latitude, complaints.Longitude, complaints.Agency Name, complaints.Closed_Date, complaints.Created_Date, complaints.Incident Zip, complaints.Complaint Type, complaints.processing_days, complaints.processing_time]; line 1 pos 38;",
      "'Project [*]",
      "+- 'Filter ((agency#634 = 'ag) AND (cast(processing_time#641 as double) > cast(1.0 as double)))",
      "   +- SubqueryAlias complaints",
      "      +- View (`complaints`, [Created_Date#654,Closed_Date#667,Agency#634,Agency Name#635,Complaint Type#636,Incident Zip#637,Borough#638,Latitude#639,Longitude#640,processing_time#641,processing_days#642])",
      "         +- Project [Created_Date#654, Closed Date#633 AS Closed_Date#667, Agency#634, Agency Name#635, Complaint Type#636, Incident Zip#637, Borough#638, Latitude#639, Longitude#640, processing_time#641, processing_days#642]",
      "            +- Project [Created Date#632 AS Created_Date#654, Closed Date#633, Agency#634, Agency Name#635, Complaint Type#636, Incident Zip#637, Borough#638, Latitude#639, Longitude#640, processing_time#641, processing_days#642]",
      "               +- Relation [Created Date#632,Closed Date#633,Agency#634,Agency Name#635,Complaint Type#636,Incident Zip#637,Borough#638,Latitude#639,Longitude#640,processing_time#641,processing_days#642] csv",
      "",
      "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:199)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:192)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:192)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:192)",
      "  at scala.collection.immutable.Stream.foreach(Stream.scala:533)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:192)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:101)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:101)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:187)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:210)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)",
      "  ... 48 elided",
      ""
     ]
    }
   ],
   "source": [
    "//This won't work because ag will be replaced by the unquoted string NYPD\n",
    "//Spark will look for a column named NYPD\n",
    "val ag=\"NYPD\"\n",
    "spark.sql(s\"select * from complaints where agency=$ag and processing_time>1.0\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: String = NYPD\n",
       "res64: String = NYPD\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = \"\"\"NYPD\"\"\"\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ag: String = \"NYPD\"\n",
       "res65: String = \"NYPD\"\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ag=\"\"\"\"NYPD\"\"\"\"\n",
    "ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------+--------------------+------------------+------------+---------+------------------+------------------+----------------+------------------+\n",
      "|       Created_Date|        Closed_Date|Agency|         Agency Name|    Complaint Type|Incident Zip|  Borough|          Latitude|         Longitude| processing_time|   processing_days|\n",
      "+-------------------+-------------------+------+--------------------+------------------+------------+---------+------------------+------------------+----------------+------------------+\n",
      "|2020-07-23 13:08:44|2020-08-25 14:56:03|   DOE|Department of Edu...|School Maintenance|       10467|    BRONX| 40.88562974190851|-73.86605482246202|33 days 01:47:19| 33.07452546296296|\n",
      "|2020-03-13 15:43:56|2020-06-19 15:39:18|   DOE|Department of Edu...|School Maintenance|       10031|MANHATTAN| 40.83050367593391|-73.94449624244284|97 days 23:55:22| 97.99678240740741|\n",
      "|2020-03-10 17:27:21|2020-06-03 08:52:47|   DOE|Department of Edu...|School Maintenance|       10030|MANHATTAN|40.815448691325535|-73.94406082243243|84 days 15:25:26| 84.64266203703704|\n",
      "|2022-07-21 11:28:48|2022-07-25 08:37:48|   DOE|Department of Edu...|School Maintenance|       11104|   QUEENS|  40.7425717563312|-73.92260299038753| 3 days 21:09:00|           3.88125|\n",
      "|2022-07-22 10:40:18|2022-08-01 09:01:34|   DOE|Department of Edu...|School Maintenance|       11368|   QUEENS| 40.75024565043563|-73.86782484715943| 9 days 22:21:16| 9.931435185185185|\n",
      "|2022-07-11 14:15:27|2022-07-21 11:13:00|   DOE|Department of Edu...|School Maintenance|       11226| BROOKLYN| 40.64501536896206|-73.96320846783166| 9 days 20:57:33|  9.87329861111111|\n",
      "|2022-07-18 18:29:41|2022-07-20 08:14:07|   DOE|Department of Edu...|School Maintenance|       11224| BROOKLYN| 40.57732006934196| -73.9713129720379| 1 days 13:44:26| 1.572523148148148|\n",
      "|2022-07-18 18:30:00|2022-07-19 11:22:52|   DOE|Department of Edu...|School Maintenance|       11357|   QUEENS|40.783607908123344| -73.7946541802031| 0 days 16:52:52|0.7033796296296296|\n",
      "|2019-08-05 19:04:13|2019-08-06 11:07:48|   DOE|Department of Edu...|School Maintenance|       10472|    BRONX| 40.83141566963466|-73.87778523448054| 0 days 16:03:35|0.6691550925925926|\n",
      "|2019-09-06 13:47:11|2019-09-09 08:46:45|   DOE|Department of Edu...|School Maintenance|       11214| BROOKLYN|  40.6132612484812|-74.00340366879479| 2 days 18:59:34|2.7913657407407406|\n",
      "|2022-07-19 11:19:39|2022-07-20 08:19:01|   DOE|Department of Edu...|School Maintenance|       10456|    BRONX|40.821220322559874| -73.9088326110657| 0 days 20:59:22|0.8745601851851851|\n",
      "|2022-07-19 11:01:52|2022-07-20 08:18:17|   DOE|Department of Edu...|School Maintenance|       10456|    BRONX|40.821220322559874| -73.9088326110657| 0 days 21:16:25| 0.886400462962963|\n",
      "|2022-07-18 19:02:03|2022-07-20 08:11:35|   DOE|Department of Edu...|School Maintenance|       11235| BROOKLYN|40.586844568736325| -73.9619803959889| 1 days 13:09:32|1.5482870370370372|\n",
      "|2022-07-20 15:40:57|2022-08-15 10:11:30|   DOE|Department of Edu...|School Maintenance|       11220| BROOKLYN|40.642121192272114|  -74.018996722857|25 days 18:30:33|25.771215277777777|\n",
      "|2022-07-20 15:43:25|2022-08-15 10:06:15|   DOE|Department of Edu...|School Maintenance|       11220| BROOKLYN|40.642121192272114|  -74.018996722857|25 days 18:22:50| 25.76585648148148|\n",
      "|2022-07-20 13:49:06|2022-07-21 14:25:54|   DOE|Department of Edu...|School Maintenance|       11101|   QUEENS| 40.74796186800034|-73.94004648281121| 1 days 00:36:48|1.0255555555555556|\n",
      "|2022-07-21 15:21:32|2022-08-10 09:20:56|   DOE|Department of Edu...|School Maintenance|       10473|    BRONX| 40.82140824935939|-73.86297571531716|19 days 17:59:24|19.749583333333334|\n",
      "|2022-07-21 15:07:58|2022-07-22 08:51:03|   DOE|Department of Edu...|School Maintenance|       10023|MANHATTAN| 40.77423860296157|-73.98852230784257| 0 days 17:43:05|0.7382523148148148|\n",
      "|2019-09-28 14:26:13|2019-10-02 12:53:23|   DOE|Department of Edu...|School Maintenance|       11373|   QUEENS| 40.74112156698627|-73.87472109532551| 3 days 22:27:10|3.9355324074074076|\n",
      "|2022-07-25 13:41:04|2022-08-03 14:22:30|   DOE|Department of Edu...|School Maintenance|       10472|    BRONX|40.828517515707325|-73.85656893067177| 9 days 00:41:26| 9.028773148148149|\n",
      "+-------------------+-------------------+------+--------------------+------------------+------------+---------+------------------+------------------+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ag: String = \"DOE\"\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//This will work. The three quotes will set the value of ag to NYPD with two quotes (six characters)\n",
    "val ag=\"\"\"\"DOE\"\"\"\"\n",
    "spark.sql(s\"select * from complaints where agency=$ag and processing_days>0.5\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">Calculating averages by agency</span>\n",
    "<li>We used combineByKey when calculating this on RDD datasets</li>\n",
    "<li>Life is a lot simpler with SQL!</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Created_Date: string (nullable = true)\n",
      " |-- Closed_Date: string (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Agency Name: string (nullable = true)\n",
      " |-- Complaint Type: string (nullable = true)\n",
      " |-- Incident Zip: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- processing_time: string (nullable = true)\n",
      " |-- processing_days: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              Agency|avg(processing_days)|\n",
      "+--------------------+--------------------+\n",
      "|MAYORS OFFICE ...|  10.993121710572932|\n",
      "|                 DPR|   65.99335925229708|\n",
      "|                 TLC|   53.68203580195775|\n",
      "|                 DOE|   43.44743418663761|\n",
      "|                 DOB|  39.106442592865434|\n",
      "|               DOHMH|  15.397117888616267|\n",
      "|                 DEP|   5.006676030990062|\n",
      "|                 DOT|  14.494687185115172|\n",
      "|                 DOF|   19.76244362870433|\n",
      "|                DSNY|   6.984900241530156|\n",
      "|                 DCA|  3.1134426905912487|\n",
      "|                 EDC|   56.00096076391825|\n",
      "|                NYPD|  0.3395727580073328|\n",
      "|               DOITT|  28.392176800309258|\n",
      "|                 HPD|  13.242378653308977|\n",
      "|                 DHS|   1.257410746037668|\n",
      "|                DFTA|  13.390725308641976|\n",
      "|OFFICE OF TECHNOL...|  0.7780439814814815|\n",
      "|                 OSE| 0.12648533950617283|\n",
      "|                FDNY|   402.1443981481481|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "agency_averages: org.apache.spark.sql.DataFrame = [Agency: string, avg(processing_days): double]\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val agency_averages = spark.sql(\"select Agency, AVG(processing_days) from complaints GROUP BY Agency\")\n",
    "agency_averages.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">SQL joins</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "defined class Student\n",
       "defined class EnrolledIn\n",
       "student_df: org.apache.spark.sql.DataFrame = [id_no: string, name: string ... 1 more field]\n",
       "enrolled_df: org.apache.spark.sql.DataFrame = [id_no: string, course_no: string ... 1 more field]\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._ //This will let us convert a Scala Seq of arbitrary objects to a df\n",
    "case class Student(id_no: String,name: String, age: Int)\n",
    "case class EnrolledIn(id_no: String,course_no: String,course_name: String)\n",
    "\n",
    "    \n",
    "val student_df = Seq(Student(\"S1\",\"Jack\",30),Student(\"S2\",\"Jill\",20)).toDF\n",
    "val enrolled_df = Seq(EnrolledIn(\"S1\",\"C1\",\"Cloud\"),EnrolledIn(\"S2\",\"C1\",\"Cloud\"),EnrolledIn(\"S1\",\"C2\",\"ML\")).toDF\n",
    "\n",
    "student_df.createOrReplaceTempView(\"students\")\n",
    "enrolled_df.createOrReplaceTempView(\"enrolled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+\n",
      "|id_no|name|age|\n",
      "+-----+----+---+\n",
      "|   S1|Jack| 30|\n",
      "|   S2|Jill| 20|\n",
      "+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+\n",
      "|id_no|course_no|course_name|\n",
      "+-----+---------+-----------+\n",
      "|   S1|       C1|      Cloud|\n",
      "|   S2|       C1|      Cloud|\n",
      "|   S1|       C2|         ML|\n",
      "+-----+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enrolled_df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">writing an inner join</span>\n",
    "<li>get all the courses that Jack is currently registered in</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+\n",
      "|id_no|name|age|\n",
      "+-----+----+---+\n",
      "|   S1|Jack| 30|\n",
      "|   S2|Jill| 20|\n",
      "+-----+----+---+\n",
      "\n",
      "+-----+---------+-----------+\n",
      "|id_no|course_no|course_name|\n",
      "+-----+---------+-----------+\n",
      "|   S1|       C1|      Cloud|\n",
      "|   S2|       C1|      Cloud|\n",
      "|   S1|       C2|         ML|\n",
      "+-----+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_df.show\n",
    "enrolled_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|name|course_name|\n",
      "+----+-----------+\n",
      "|Jack|      Cloud|\n",
      "|Jack|         ML|\n",
      "+----+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "c: String = \"Jack\"\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val c = \"\"\"\"Jack\"\"\"\"\n",
    "spark.sql(s\"select students.name, enrolled.course_name from students inner join enrolled on students.id_no = enrolled.id_no where students.name=$c\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DataFrames are functional objects</h1>\n",
    "<li>Which means we can chain function calls</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|              Agency|    mean_proc_time|\n",
      "+--------------------+------------------+\n",
      "|MAYORS OFFICE ...|2.4033759376747463|\n",
      "|                 DPR|2.7649726904896768|\n",
      "|                 TLC|2.3661278584455667|\n",
      "|                 DOB|2.5542114746715168|\n",
      "|               DOHMH|  2.10285441492303|\n",
      "|                 DEP| 2.385515127231277|\n",
      "|                 DOT|2.5067696788362825|\n",
      "|                 DOF|2.7180876825601366|\n",
      "|                DSNY| 2.451787242140279|\n",
      "|                 DCA|2.1624431795090624|\n",
      "|                NYPD| 2.033820700596487|\n",
      "|                 HPD|2.7037918624513955|\n",
      "|                 DHS|2.1804076945162225|\n",
      "|                 DOE| 2.574423699906918|\n",
      "|               DOITT|2.4433302696078436|\n",
      "|                 EDC| 2.939592983254035|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "categorizer: (x: Double)Int\n",
       "categorizer_udf: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$5948/0x0000000801d6e040@6f29f505,IntegerType,List(Some(class[value[0]: double])),Some(class[value[0]: int]),None,false,true)\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorizer(x: Double): Int = \n",
    "    if (x < 1.0) 0\n",
    "    else if (x < 5.0) 1\n",
    "    else 2\n",
    "val categorizer_udf = udf(categorizer _) //defines a udf as \"Call categorizer with an argument\"\n",
    "\n",
    "\n",
    "df.withColumn(\"proc_time_category\",categorizer_udf($\"processing_days\"))\n",
    "    .filter($\"proc_time_category\"===\"1\")\n",
    "    .groupBy(\"Agency\")\n",
    "    .agg(ConditionalAverage(col(\"processing_days\"),lit(1.0)))\n",
    "    .withColumnRenamed(\"conditionalaverage$(processing_days, 1.0)\",\"mean_proc_time\")\n",
    "    .select(\"Agency\",\"mean_proc_time\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dataframes and datasets can handle missing values</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+--------+\n",
      "|       name| age|    city|\n",
      "+-----------+----+--------+\n",
      "|       John|  56|New York|\n",
      "|       Jill|  80|  Boston|\n",
      "|     Jacoby|null| Phoenix|\n",
      "|Jabberwocky|  17|New York|\n",
      "+-----------+----+--------+\n",
      "\n",
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|    51.0|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y: org.apache.spark.rdd.RDD[(String, String, String)] = ParallelCollectionRDD[149] at parallelize at <console>:52\n",
       "y2: org.apache.spark.rdd.RDD[(String, Option[Int], String)] = MapPartitionsRDD[150] at map at <console>:54\n",
       "ds: org.apache.spark.sql.DataFrame = [name: string, age: int ... 1 more field]\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val y = sc.parallelize(Array((\"John\",\"56\",\"New York\"),(\"Jill\",\"80\",\"Boston\"),\n",
    "              (\"Jacoby\",\"\",\"Phoenix\"),(\"Jabberwocky\",\"17\",\"New York\")))\n",
    "val y2 = y.map(t => {\n",
    "    try {\n",
    "        (t._1,Some(t._2.toInt),t._3)\n",
    "    } catch {\n",
    "        case e: Exception => (t._1,None,t._3)\n",
    "    }\n",
    "})\n",
    "val ds = y2.toDF(\"name\",\"age\",\"city\")\n",
    "ds.show\n",
    "ds.select(avg(\"age\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: Int = 51\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(56+80+17)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|    51.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.agg(avg(\"age\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Dataset</span>\n",
    "<br>\n",
    "<li>Sort of intermediate between an RDD and a DF</li>\n",
    "<li>They are type safe unlike dataframes</li>\n",
    "<li>Can have a schema (like dataframes)</li>\n",
    "<li>Are performance optimized (like dataframes)</li>\n",
    "<li>Python and R Spark APIs do not support datasets</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "x: Array[(String, String, Int)] = Array((John,Q1,10), (Jill,Q1,8), (John,Q2,3), (Jill,Q2,9), (Bill,Q2,7))\n",
       "rdd: org.apache.spark.rdd.RDD[(String, String, Int)] = ParallelCollectionRDD[160] at parallelize at <console>:59\n",
       "df: org.apache.spark.sql.DataFrame = [Name: string, Quiz: string ... 1 more field]\n",
       "defined class grades\n",
       "ds_from_df: org.apache.spark.sql.Dataset[grades] = [Name: string, Quiz: string ... 1 more field]\n",
       "ds_from_rdd: org.apache.spark.sql.Dataset[(String, String, Int)] = [_1: string, _2: string ... 1 more field]\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "val x = Array((\"John\",\"Q1\",10),\n",
    "              (\"Jill\",\"Q1\",8),\n",
    "              (\"John\",\"Q2\",3),\n",
    "              (\"Jill\",\"Q2\",9),\n",
    "            (\"Bill\",\"Q2\",7))\n",
    "val rdd = sc.parallelize(x)\n",
    "val df = rdd.toDF(\"Name\",\"Quiz\",\"Score\")\n",
    "\n",
    "case class grades(name: String,quiz: String, score: Int)\n",
    "val ds_from_df = df.as[grades] //Creates a dataset from a DataFrame and uses df schema\n",
    "val ds_from_rdd = rdd.toDS //Creates a dataset from an RDD with a default schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res78: Array[(String, String, Int)] = Array((John,Q1,10), (Jill,Q1,8), (John,Q2,3), (Jill,Q2,9), (Bill,Q2,7))\n"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Quiz: string (nullable = true)\n",
      " |-- Score: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Quiz: string (nullable = true)\n",
      " |-- Score: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_from_df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_from_rdd.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res82: Array[grades] = Array(grades(John,Q1,10), grades(Jill,Q1,8), grades(John,Q2,3), grades(Jill,Q2,9), grades(Bill,Q2,7))\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_from_df.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "|John|\n",
      "|Jill|\n",
      "|John|\n",
      "|Jill|\n",
      "|Bill|\n",
      "+----+\n",
      "\n",
      "+----+\n",
      "|  _1|\n",
      "+----+\n",
      "|John|\n",
      "|Jill|\n",
      "|John|\n",
      "|Jill|\n",
      "|Bill|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_from_df.select(\"Name\").show\n",
    "ds_from_rdd.select(\"_1\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Dataframes, RDDs, Datasets</span>\n",
    "<li>RDD</li>\n",
    "<ul>\n",
    "    <li>Data is unstructured (no schema)</li>\n",
    "    <li>Useful when you want to do low level transformations</li>\n",
    "    <li>More control over the data (you can impose any schema-in-the-mind on the data)</li>\n",
    "    <li>Schema does not matter to you (because you're good at functional programming!)</li>\n",
    "    <li>RDDs are type safe</li>\n",
    "</ul>\n",
    "<li>Dataframes</li>\n",
    "<ul>\n",
    "    <li>Structured (RDBMS) or semi-structured (JSON/CSV) data</li>\n",
    "    <li>Uses an SQL optimized engine that converts operations into low level RDD transformations (efficient and fast)</li>\n",
    "    <li>Better for PySpark or other non-functional programming environment</li>\n",
    "    <li>Better if you like SQL over functional languages</li>\n",
    "    <li>Dataframes are not type safe (a non-existent column will not be detected until it is referenced</li>\n",
    "</ul>\n",
    "<li>Datasets</li>\n",
    "<ul>\n",
    "    <li>Not available for pyspark</li>\n",
    "    <li>Sort of intermediate between an RDD and a Dataframe</li>\n",
    "    <li>More efficient than an RDD but less than a dataframe</li>\n",
    "    <li>Datasets are type safe</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
