{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">Spark Streaming</span>\n",
    "<br><br>\n",
    "<li>Enables \"the internet of things\"</li>\n",
    "<li>New data is constantly \"streaming\" into an application</li>\n",
    "<li>The application processes this data in real time or \"near\" real time</li>\n",
    "<li>Streaming applications are <span style=\"color:darkred\">Producer-Consumer</span> applications</li>\n",
    "<li>Other systems: Kafka, (Twitter) Storm, Gearpump, Apex, ... </li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Examples of Streaming Applications</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Uber uses a streaming application to monitor customer requests and move drivers from one part of a city to another</li>\n",
    "<li>Real time A/B testing</li>\n",
    "<li>Figuring out fake news in real time (facebook/twitter)</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Characteristics of Streaming Applications</span>\n",
    "<li>Low latency and high throughput of incoming streams</li>\n",
    "<li>Fault tolerance, usually by maintaining the current state in memory (low latency) and backing it up on a disk (fault tolerance)</li>\n",
    "<ul>\n",
    "    <li>Example: A system reporting the top 10 \"most traded\" stocks in real time</li>\n",
    "    <li>A running count of each stock needs to be kept in memory</li>\n",
    "    <li>The count needs to be updated with each tick</li>\n",
    "    <li>The count must have a backup in case of failures (it can't start from scratch!)</li>\n",
    "</ul>\n",
    "<li>Interoperability with batch processing systems</li>\n",
    "<ul>\n",
    "    <li>Example: A real time inventory tracker</li>\n",
    "    <li>A static file may contain item (number, price, location) information</li>\n",
    "    <li>The streaming application contains transaction information (item, number sold, number bought)</li>\n",
    "    <li>The streaming data needs to be joined with the static file to get updated inventory information</li>\n",
    "</ul>\n",
    "<li><span style=\"color:red\">Exactly once</span> data guarantees</li>\n",
    "<ul>\n",
    "    <li>Some stream processing systems may guarantee \"at least once\" (data could be duplicated or read twice), others may guarantee \"at most once\" (data could be lost). Ideally, they should guarantee \"exactly once\".\n",
    "</ul>\n",
    "<li>Dealing with uneven data arrival rates</li>\n",
    "<ul>\n",
    "    <li>If data is arriving from multiple sources, they may not arrive in the correct temporal sequence (see next point!)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Time</span>\n",
    "<br>\n",
    "<li><span style=\"color:red\">Event time</span>: the time the event occurs (e.g., the time  a trade actually takes place). Note that the event time is usually independent of the streaming application</li>\n",
    "<li><span style=\"color:red\">Processing time</span>: the time that the system processes the event. This may be very close to the event time (nanoseconds) or not so close (milliseconds, seconds, ....). <b>Ideally, a streaming app should report the correct status at each event time</b></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"event time vs processing time.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Stream processing models</span>\n",
    "<br><br>\n",
    "<li><span style=\"color:red\">Record-at-a-time</span> processes each piece of arriving data as soon as it arrives</li>\n",
    "<ul>\n",
    "    <li>low latency (immediate processing of data)</li>\n",
    "    <li>difficult to deal with out of sequence data</li>\n",
    "    <li>(lower throughput) relatively inefficient since the processing algorithm has to run for each data arrival</li>\n",
    "</ul>\n",
    "<li><span style=\"color:red\">micro-batching</span> accumulates arriving data into small batches and processes a batch at a time</li>\n",
    "<ul>\n",
    "    <li>high latency (data is processed only when the batch is formed)</li>\n",
    "    <li>easier to deal with out of sequence data (e.g., it is less important within a batch)</li>\n",
    "    <li>(higher throughput) relatively efficient since the processing algorithm runs a batch at a time</li>\n",
    "</ul>\n",
    "<li>Generally, if a stream is very busy, micro batching is preferred. If latency is an issue (e.g., detecting hacker attacks on a machine), then record-at-a-time is preferred</li>\n",
    "<li>Spark uses the micro-batching model (with latencies as low as a nanosecond)</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Windowing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li><span style=\"color:red\">Windowing</span> is a stream processing pattern where incoming data stream is divided into chunks based on temporal boundaries</li>\n",
    "<li>For example, a high frequency trader may gather data every nanosecond but may process the data in one second chunks. The one second chunk is a window on the stream</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Types of windows</span>\n",
    "<li><span style=\"color:red\">fixed windows</span>: Fixed windows are defined by a window length The incoming stream is divided into chunks, each of the window length size and each data point is processed in exactly one window. For example, a traffic control system may divide the day into 5 minute chunks and process traffic in each 5 minute chunk to get a sense for the differences in commuting patterns at different times</li>\n",
    "<li><span style=\"color:red\">sliding windows</span>: Sliding windows sit on top of the data stream and are defined by a \"slide factor\" and a \"window length\". For example, a stock trading application may look at 5 minute moving average that slide every minute. In this type of windowing system, each data point will be processed in multiple windows. A sliding window with a slide factor = window length is the same as a fixed window</li>\n",
    "<li><span style=\"color:red\">session windows</span>: Session windows coincide with the start and end of a session. For example, an analytics app on a web page may look at the sessions of each user for analysis (time spent/pages visited/ads clicked/page sequences). In this case, each session window will have its own (temporal) length</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<img src=\"fixed windows.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sliding windows.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"session windowing.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Spark Streaming</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Spark doesn't work with continuous streams</li>\n",
    "<li>Rather, it monitors the stream port, collects data into small batches, and processes each batch</li>\n",
    "<li>Because the smallest batch size is <span style=\"color:blue\">500ms</span> for RDD streaming and 1 nanosecond for Structured Streaming, think of Spark Streaming as \"Almost Streaming\"</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:large\">Streaming abstractions in Spark</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<li><span style=\"color:red\">RDD based streaming abstraction</span>: DStream (Discretized Stream) abstraction</li>\n",
    "<li><span style=\"color:red\">Dataframe based stream abstraction</span>: Structured streaming</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">A Spark Streaming Application</span>\n",
    "<img src=\"streaming.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "<span style=\"color:red;font-size:50px\">RDD Based Streaming</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Streaming context</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>The entry point for spark streaming</li>\n",
    "<li>Sets up batch size</li>\n",
    "<li>Arguments: the spark context and the time interval for a micro batch</li>\n",
    "<li>We will use 10 seconds to give us enough time to see stuff happenning</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@56e89e6a\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc,Seconds(10.toLong))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@4a2c2521\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ssc = new StreamingContext(sc,Seconds(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Batch processing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>The basic unit of a streaming application is a batch processing program</li>\n",
    "<li>When a batch is ready (e.g., every 10 seconds), the batch processing program processes it to generate data</li>\n",
    "<li>Example, a twitter stream application may compute the sentiment on the data</li>\n",
    "<li>A stock price stream application may calculate or update moving averages or other technical indicators</li>\n",
    "<li>We'll start with a \"socket listener\"</li>\n",
    "<li><span style=\"color:blue\">ssc.socketTextStream</span> listens at the specified socket (localhost for us)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Create a listener</span>\n",
    "<li>We'll use a local socket listener</li>\n",
    "<li>Open a terminal (OSX/Linux) or cmd (Windows) window</li>\n",
    "<li>OSX or linux (at the terminal prompt)</li>\n",
    "<ul><li> nc -lk 4444</li></ul>\n",
    "<li>Windows</li>\n",
    "<ul><li>Follow instructions <a href=\"https://joncraton.org/blog/46/netcat-for-windows/\">here</a> (make sure you read the comments if you run into trouble!) and download netcat\n",
    "    </ul>\n",
    "    <li>Once the listener is created, start the stream</li>\n",
    "    <li>And start typing!</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">The program</span>\n",
    "<li>Reads data coming in through the socket</li>\n",
    "<li>Does a word count on each batch</li>\n",
    "<li>Each batch is a <span style=\"color:blue\">DStream</span> object</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">The Spark Streaming Process</span>\n",
    "<li>Create a StreamingContext from the SparkContext</li>\n",
    "<li>Create a DStream from the StreamingContext</li>\n",
    "<li>Create a set of transformations and actions that can be applied to each RDD in the DStream</li>\n",
    "<li>Start the streaming context</li>\n",
    "<ul>\n",
    "    <li>Once started, you cannot change the transformations and actions</li>\n",
    "    <li>There must be at least one action, otherwise the StreamingContext will not start</li>\n",
    "</ul>\n",
    "<li>When done, stop the streaming context</li>\n",
    "<ul>\n",
    "    <li>Once stopped, you can't restart the streaming context</li>\n",
    "    <li>You must create a new streaming context if you need to keep it running</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@258ccff1\n",
       "lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@797b9aab\n",
       "words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@53aaa0ed\n",
       "pairs: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.MappedDStream@f047741\n",
       "wordCounts: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.ShuffledDStream@4c34c217\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc,Seconds(10.toLong))\n",
    "\n",
    "//NOTE: lines, words, pairs, wordCounts are all DStream objects, not RDDs!\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "val words = lines.flatMap(line => line.split(\" \"))\n",
    "val pairs = words.map(word => (word, 1))\n",
    "val wordCounts = pairs.reduceByKey((x, y) => x + y)\n",
    "wordCounts.print()\n",
    "// wordCounts.foreachRDD((rdd,time) => print(time.toString.takeRight(10),\": Total  \",rdd.collect()(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Start the stream</span>\n",
    "<li>So far, we've created a program that can be applied to each microbatch</li>\n",
    "<li>Once we start listening at the stream, the program will be active</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "//!nc -lk 4444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">start</span> is a method that starts listening on the stream and processing batches when they arrive\n",
    "<li>Any unprocessed data in the stream buffer will be picked up by the application</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1671473640000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "22/12/19 13:14:01 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/12/19 13:14:01 WARN BlockManager: Block input-0-1671473641200 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/12/19 13:14:01 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/12/19 13:14:01 WARN BlockManager: Block input-0-1671473641600 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1671473650000 ms\n",
      "-------------------------------------------\n",
      "(Hi,1)\n",
      "(is,1)\n",
      "(name,1)\n",
      "(Wei,1)\n",
      "(my,1)\n",
      "(Zhou,1)\n",
      "\n",
      "22/12/19 13:14:13 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/12/19 13:14:13 WARN BlockManager: Block input-0-1671473653400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/12/19 13:14:13 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/12/19 13:14:13 WARN BlockManager: Block input-0-1671473653600 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1671473660000 ms\n",
      "-------------------------------------------\n",
      "(Hi,1)\n",
      "(is,1)\n",
      "(name,1)\n",
      "(Wei,1)\n",
      "(my,1)\n",
      "(Zhou,1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">When done, stop the stream</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/19 13:14:25 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "22/12/19 13:14:25 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/12/19 13:14:25 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/12/19 13:14:25 WARN ReceiverSupervisorImpl: Receiver has been stopped\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">DStream: The Streaming Structured Object</span>\n",
    "<li>DStream: Discretized Stream</li>\n",
    "<li>A DStream object is a sequence of RDDs, one at each time interval</li>\n",
    "<li>Each micro-batch is processed independently of other batches</li>\n",
    "<li>micro-batch RDDs are \"state indepedent\"</li>\n",
    "<li>Roughly:\n",
    "    <ul>\n",
    "        <li>inputs are collected from the stream</li>\n",
    "        <li>at the time interval mark, they are put into a DStream</li>\n",
    "        <li>the DStream is handed over to the process app for processing</li>\n",
    "        <li>so, each interval corresponds to one DStream in the stream</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Streaming sources</span>\n",
    "<li><b>socketTextStream</b>: collects data from a socket listener</li>\n",
    "<li><b>fileStream</b>: collects data from new files in a \"hadoop compatible\" file system</li>\n",
    "<li><b>textFileStream</b>: collects data from new files as text</li>\n",
    "<li><b>Other:</b> rawSocketStream, queueStream, binaryRecordsStream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">File streaming example</span>\n",
    "<li>Create a folder where the files will be added</li>\n",
    "<li>A file stream looks for <span style=\"color:red\">new</span> files</li>\n",
    "<li>Thanks to the vagaries of file systems, this may not be very clear</li>\n",
    "<li>For example: </li>\n",
    "<ul><li>On a mac, the file must have a new id</li>\n",
    "    <li>Use the cp command in terminal to make a new file</li>\n",
    "    </ul>\n",
    "    <li>Of course, in practice, the file will always be new (for ex, an internet log file that gets created every few minutes</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc,Seconds(10.toLong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lines = ssc.textFileStream(\"stream\")\n",
    "val words = lines.flatMap(line => line.split(\" \"))\n",
    "val pairs = words.map(word => (word, 1))\n",
    "val wordCounts = pairs.reduceByKey( (x, y) => x + y)\n",
    "wordCounts.print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//cat file1\n",
    "//cp file1 file7 ->copy file 1 to file 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
