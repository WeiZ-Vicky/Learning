{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd24bf1",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Set up PySpark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3300c811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 09:16:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/07 09:16:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark ML Basics\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1343243",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Vectors</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10f098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "x = np.array([3.2,0,0,0,4.7,1.6,0,0,0,0,10.2,0,0,11.1])\n",
    "data_dense = Vectors.dense(x) \n",
    "data_sparse = Vectors.sparse(14,np.array([0,4,5,10,13]),np.array([3.2,4.7,1.6,10.2,11.1])) #using numpy array to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf21e7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([3.2, 0.0, 0.0, 0.0, 4.7, 1.6, 0.0, 0.0, 0.0, 0.0, 10.2, 0.0, 0.0, 11.1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c3b9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(14, {0: 3.2, 4: 4.7, 5: 1.6, 10: 10.2, 13: 11.1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse #dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca6531a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.799999999999997\n",
      "16.190738093119784\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(Vectors.norm(data_dense,1)) #returns the p=1 norm (taxicab)\n",
    "print(Vectors.norm(data_dense,2)) #returns the euclidean norm\n",
    "print(Vectors.squared_distance(data_dense,data_sparse)) #distance between two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad8565",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Matrices</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ea26ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.linalg import Matrices\n",
    "\n",
    "data = np.array([1.0, 0.0, 4.0, 0.0, 3.0, 5.0, 2.0, 0.0, 6.0])\n",
    "dense_m = Matrices.dense(3,3,data)\n",
    "sparse_m = dense_m.toSparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d34c01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(3, 3, [1.0, 0.0, 4.0, 0.0, 3.0, 5.0, 2.0, 0.0, 6.0], False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "890f7c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseMatrix(3, 3, [0, 2, 4, 6], [0, 2, 1, 2, 0, 2], [1.0, 4.0, 3.0, 5.0, 2.0, 6.0], False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93dcd18",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<h2 style=\"color:red;font-size:50px\">feature transformers</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca5862",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Vector Assembler</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f6091",
   "metadata": {},
   "source": [
    "<li>Combines a set of columns into a single <b>sparse</b> vector</li>\n",
    "<li>In supervised learning, the independent features are combined into a single vector</li>\n",
    "<li>As a result, each case is represented by a pair (dv,iv-vector)</li>\n",
    "<li><a href=\"https://spark.apache.org/docs/latest/ml-features#vectorassembler\">https://spark.apache.org/docs/latest/ml-features#vectorassembler</a></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "376df370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|label|   features|\n",
      "+-----+-----------+\n",
      "|    3|[22.0,23.1]|\n",
      "|    2|[12.2,13.0]|\n",
      "|    4|[43.7,16.2]|\n",
      "|    3|[36.4,34.8]|\n",
      "|    3| [6.1,71.0]|\n",
      "|    7|[28.2,22.1]|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "  (22.0, 23.1,3),\n",
    "  (12.2, 13.0,2),\n",
    "  (43.7, 16.2,4),\n",
    "  (36.4, 34.8,3),\n",
    "  (6.1, 71.0,3),\n",
    "  (28.2, 22.1,7)\n",
    "]).toDF(\"feature1\", \"feature2\",\"dv\")\n",
    "\n",
    "#Create an assembler object identifying the columns that need to be vectorized\n",
    "assembler = VectorAssembler()\\\n",
    "  .setInputCols([\"feature1\",\"feature2\"])\\\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "#Call transform on the dataframe. This creates a new dataframe using the specifications\n",
    "#(specs = which columns to keep)\n",
    "#by default, spark ml models assume the dv is in a column called label and the iv in a column called features\n",
    "df_lr = assembler.transform(df)\\\n",
    "    .select(\"dv\",\"features\")\\\n",
    "    .withColumnRenamed(\"dv\",\"label\")\n",
    "\n",
    "df_lr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019911f5",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:green;font-size:xx-large\">StringIndexer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49537b08",
   "metadata": {},
   "source": [
    "<li>ML algorithms need numbers!</li>\n",
    "<li>Any string variables need to be converted into numbers before they can be used</li>\n",
    "<li><a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/ml/feature/StringIndexer.html\">StringIndexer</a> is a spark feature transofrmer that does this</li>\n",
    "<li>The most frequent category is given the value 1, second most 2, etc.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4491a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---+-------------+\n",
      "|feature1|feature2| dv|feature1Index|\n",
      "+--------+--------+---+-------------+\n",
      "|     MIA|    17.2|  2|          4.0|\n",
      "|     NYC|    23.1|  3|          1.0|\n",
      "|     SFO|    13.0|  2|          0.0|\n",
      "|     NYC|    16.2|  4|          1.0|\n",
      "|     CHI|    34.8|  3|          2.0|\n",
      "|     SFO|    71.0|  3|          0.0|\n",
      "|     SFO|   22.12|  6|          0.0|\n",
      "|     LAX|    22.1|  7|          3.0|\n",
      "+--------+--------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "  (\"MIA\", 17.2,2),  \n",
    "  (\"NYC\", 23.1,3),\n",
    "  (\"SFO\", 13.0,2),\n",
    "  (\"NYC\", 16.2,4),\n",
    "  (\"CHI\", 34.8,3),\n",
    "  (\"SFO\", 71.0,3),\n",
    "  (\"SFO\", 22.12,6),\n",
    "  (\"LAX\", 22.1,7)\n",
    "]).toDF(\"feature1\", \"feature2\",\"dv\")\n",
    "\n",
    "#Create a StringIndexer object with the column specifications\n",
    "indexer = StringIndexer()\\\n",
    "  .setInputCol(\"feature1\")\\\n",
    "  .setOutputCol(\"feature1Index\")\n",
    "\n",
    "#The \"fit\" operation determines the category to number relationship\n",
    "#The \"transform\" operation does the actual assigning of values\n",
    "indexed = indexer\\\n",
    "                .fit(df)\\\n",
    "                .transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072318d",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">One Hot Encoding</span>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d577795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (\"Jack\",\"A\",\"IEOR\"),\n",
    "    (\"Jill\",\"B\",\"IEOR\"),\n",
    "    (\"Jiahuo\",\"A\",\"CS\"),\n",
    "    (\"Pierre\",\"C\",\"APAM\"),\n",
    "    (\"Clemence\",\"B\",\"APAM\"),\n",
    "    (\"Savitri\",\"A\",\"CS\"),\n",
    "    (\"Bjorn\",\"A\",\"QMSS\")]).toDF(\"student\",\"grade\",\"department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc8f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "indexer = StringIndexer()\\\n",
    "  .setInputCols((\"grade\",\"department\"))\\\n",
    "  .setOutputCols((\"gradeIndex\",\"departmentIndex\"))\n",
    "\n",
    "#The \"fit\" operation determines the category to number relationship\n",
    "#The \"transform\" operation does the actual assigning of values\n",
    "indexedDf = indexer\\\n",
    "                .fit(df)\\\n",
    "                .transform(df)\n",
    "indexedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\\\n",
    "  .setInputCols((\"gradeIndex\", \"departmentIndex\"))\\\n",
    "  .setOutputCols((\"gradeVec\", \"departmentVec\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoder.fit(indexedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f006cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.transform(indexedDf)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d1d8a4",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">toPandas</span>\n",
    "\n",
    "<li>Handy function that converts a Spark DF into a Pandas DF</li>\n",
    "<li>But, beware, a pandas dataframe is not lazy. Only use this when you're sure that the dataframe is small enough</li>\n",
    "<li>toArray() converts a vector into a numpy array</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295cf433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame((\n",
    "  (\"MIA\", 17.2,2,4),  \n",
    "  (\"NYC\", 23.1,3,4),\n",
    "  (\"SFO\", 13.0,2,2),\n",
    "  (\"NYC\", 16.2,1,1),\n",
    "  (\"CHI\", 34.8,3,7),\n",
    "  (\"SFO\", 71.0,3,6),\n",
    "  (\"SFO\", 22.12,3,3),  \n",
    "  (\"LAX\", 22.1,2,4)\n",
    ")).toDF(\"feature1\", \"feature2\",\"feature3\",\"dv\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc017f7",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<h2 style=\"color:red;font-size:50px\">Example: California home values</h2>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2acae5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\",\"false\")\\\n",
    "        .option(\"inferschema\",\"true\")\\\n",
    "        .load(\"cal_housing.data\")\\\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\\\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\\\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "586c69c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- MedianAge: double (nullable = true)\n",
      " |-- TotalRooms: double (nullable = true)\n",
      " |-- TotalBedrooms: double (nullable = true)\n",
      " |-- Population: double (nullable = true)\n",
      " |-- Households: double (nullable = true)\n",
      " |-- MedianIncome: double (nullable = true)\n",
      " |-- MedianHomeValue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a543d",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Setting up the dependent variable</span>\n",
    "<li>We'll simplify the median home value by dividing it by 100,000\n",
    "<li>With PySpark, we can't use the dollar sign to represent a column and need to explicitly use col</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab7e8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\",\"false\")\\\n",
    "        .option(\"inferschema\",\"true\")\\\n",
    "        .load(\"cal_housing.data\")\\\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\\\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\\\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\\\n",
    "        .withColumn(\"MedianHomeValue\",col(\"MedianHomeValue\")/100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0841e5e9",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Setting up independent variables</span>\n",
    "<li>We'll divide total rooms, total bedrooms, and population by the number of households to get per household data</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f60aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\",\"false\")\\\n",
    "        .option(\"inferschema\",\"true\")\\\n",
    "        .load(\"cal_housing.data\")\\\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\\\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\\\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\\\n",
    "        .withColumn(\"MedianHomeValue\",col(\"MedianHomeValue\")/100000)    \\\n",
    "    .withColumn(\"RoomsPerHouse\", col(\"TotalRooms\")/col(\"Households\")) \\\n",
    "    .withColumn(\"PeoplePerHouse\", col(\"Population\")/col(\"Households\")) \\\n",
    "    .withColumn(\"BedroomsPerHouse\", col(\"TotalBedrooms\")/col(\"Households\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2638354a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Select the features we need</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16545b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\",\"false\")\\\n",
    "        .option(\"inferschema\",\"true\")\\\n",
    "        .load(\"cal_housing.data\")\\\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\\\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\\\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\\\n",
    "        .withColumn(\"MedianHomeValue\",col(\"MedianHomeValue\")/100000)    \\\n",
    "    .withColumn(\"RoomsPerHouse\", col(\"TotalRooms\")/col(\"Households\")) \\\n",
    "    .withColumn(\"PeoplePerHouse\", col(\"Population\")/col(\"Households\")) \\\n",
    "    .withColumn(\"BedroomsPerHouse\", col(\"TotalBedrooms\")/col(\"Households\"))\\\n",
    "    .select(\"MedianHomeValue\", \n",
    "              \"MedianAge\", \n",
    "              \"Population\", \n",
    "              \"Households\", \n",
    "              \"MedianIncome\", \n",
    "              \"RoomsPerHouse\", \n",
    "              \"PeoplePerHouse\", \n",
    "              \"BedroomsPerHouse\",\n",
    "               \"Latitude\",\n",
    "               \"Longitude\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2af5c",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;font-size:xx-large\">Machine Learning Pipelines</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee9d94f",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Read the data from a file and split into train and test</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b72f3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def readData(): \n",
    "    df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\",\"false\")\\\n",
    "        .option(\"inferschema\",\"true\")\\\n",
    "        .load(\"cal_housing.data\")\\\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\\\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\\\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\n",
    "    train,test = df.randomSplit((0.8,0.2),seed=1234)\n",
    "    return train,test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fcb437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = readData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c7c80e",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Do the preprocessing steps</span>\n",
    "<li>train and test can be separately passed through this function</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f95ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "def prepareData(df):\n",
    "    return df.withColumn(\"MedianHomeValue\",col(\"MedianHomeValue\")/100000)\\\n",
    "        .withColumn(\"RoomsPerHouse\", col(\"TotalRooms\")/col(\"Households\"))\\\n",
    "        .withColumn(\"PeoplePerHouse\", col(\"Population\")/col(\"Households\"))\\\n",
    "        .withColumn(\"BedroomsPerHouse\", col(\"TotalBedrooms\")/col(\"Households\"))\\\n",
    "        .select(\"MedianHomeValue\", \\\n",
    "                  \"MedianAge\", \\\n",
    "                  \"Population\", \\\n",
    "                  \"Households\", \\\n",
    "                  \"MedianIncome\", \\\n",
    "                  \"RoomsPerHouse\", \\\n",
    "                  \"PeoplePerHouse\", \\\n",
    "                  \"BedroomsPerHouse\",\\\n",
    "                   \"Latitude\",\\\n",
    "                   \"Longitude\")\\\n",
    "        .withColumnRenamed(\"MedianHomeValue\",\"label\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cab653f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, MedianAge: double, Population: double, Households: double, MedianIncome: double, RoomsPerHouse: double, PeoplePerHouse: double, BedroomsPerHouse: double, Latitude: double, Longitude: double]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepareData(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68110a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Vector Assembler</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73ff0fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedianAge',\n",
       " 'Population',\n",
       " 'Households',\n",
       " 'MedianIncome',\n",
       " 'RoomsPerHouse',\n",
       " 'PeoplePerHouse',\n",
       " 'BedroomsPerHouse',\n",
       " 'Latitude',\n",
       " 'Longitude']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(filter(lambda l : l if (l != \"label\")  else None,prepareData(train).columns))\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e0847df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|0.946|[52.0,806.0,270.0...|\n",
      "|1.036|[17.0,1244.0,456....|\n",
      "| 0.79|[36.0,1194.0,465....|\n",
      "|0.761|[32.0,434.0,187.0...|\n",
      "|1.067|[52.0,1152.0,435....|\n",
      "|0.508|[52.0,544.0,172.0...|\n",
      "|0.732|[11.0,1343.0,479....|\n",
      "|0.783|[28.0,1530.0,653....|\n",
      "|0.581|[32.0,620.0,268.0...|\n",
      "|0.669|[20.0,1993.0,721....|\n",
      "|0.684|[17.0,1947.0,647....|\n",
      "|0.901|[21.0,2907.0,972....|\n",
      "| 0.69|[30.0,1367.0,583....|\n",
      "|  0.7|[37.0,640.0,260.0...|\n",
      "|0.746|[15.0,1645.0,640....|\n",
      "| 1.07|[35.0,480.0,179.0...|\n",
      "|0.722|[33.0,656.0,236.0...|\n",
      "| 0.67|[34.0,950.0,317.0...|\n",
      "|0.702|[37.0,867.0,310.0...|\n",
      "|0.646|[40.0,788.0,279.0...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Matrix\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "#Get the names of all columns except MedianHomeValue (label)\n",
    "cols = list(filter(lambda l : l if (l != \"label\")  else None,prepareData(train).columns))\n",
    "\n",
    "\n",
    "\n",
    "#Create a vectorassembler from the list of columns and specify the name of the column of vectors\n",
    "assembler = VectorAssembler()\\\n",
    "  .setInputCols(cols)\\\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "#Apply the transform function on the data frame, select the dv and features column\n",
    "#And rename the dv column to label\n",
    "\n",
    "vector_df = assembler.transform(prepareData(train))\\\n",
    "    .select(\"label\",\"features\")\n",
    "\n",
    "vector_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f203b7",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Scaling</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "add3e8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|      scaledFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|0.946|[52.0,806.0,270.0...|[1.85293575512378...|\n",
      "|1.036|[17.0,1244.0,456....|[-0.9237482035396...|\n",
      "| 0.79|[36.0,1194.0,465....|[0.58359451687762...|\n",
      "|0.761|[32.0,434.0,187.0...|[0.26625920731607...|\n",
      "|1.067|[52.0,1152.0,435....|[1.85293575512378...|\n",
      "|0.508|[52.0,544.0,172.0...|[1.85293575512378...|\n",
      "|0.732|[11.0,1343.0,479....|[-1.3997511678820...|\n",
      "|0.783|[28.0,1530.0,653....|[-0.0510761022454...|\n",
      "|0.581|[32.0,620.0,268.0...|[0.26625920731607...|\n",
      "|0.669|[20.0,1993.0,721....|[-0.6857467213685...|\n",
      "|0.684|[17.0,1947.0,647....|[-0.9237482035396...|\n",
      "|0.901|[21.0,2907.0,972....|[-0.6064128939781...|\n",
      "| 0.69|[30.0,1367.0,583....|[0.10759155253530...|\n",
      "|  0.7|[37.0,640.0,260.0...|[0.66292834426800...|\n",
      "|0.746|[15.0,1645.0,640....|[-1.0824158583204...|\n",
      "| 1.07|[35.0,480.0,179.0...|[0.50426068948723...|\n",
      "|0.722|[33.0,656.0,236.0...|[0.34559303470646...|\n",
      "| 0.67|[34.0,950.0,317.0...|[0.42492686209685...|\n",
      "|0.702|[37.0,867.0,310.0...|[0.66292834426800...|\n",
      "|0.646|[40.0,788.0,279.0...|[0.90092982643916...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler()\\\n",
    "      .setInputCol(\"features\")\\\n",
    "      .setOutputCol(\"scaledFeatures\")\\\n",
    "      .setWithStd(True)\\\n",
    "      .setWithMean(True)\n",
    "\n",
    "#Generate the parameters (fit the scaling object to the data)\n",
    "fitted_scaler = scaler.fit(vector_df)\n",
    "\n",
    "#scale the data\n",
    "scaled_df = fitted_scaler.transform(vector_df)\n",
    "scaled_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b28c0d",
   "metadata": {},
   "source": [
    "<h3>Regression model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17b762e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 10:02:06 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/12/07 10:02:06 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\\\n",
    "    .setMaxIter(10)\\\n",
    "    .setRegParam(0.3) \\\n",
    "    .setElasticNetParam(0.8) \\\n",
    "    .setFeaturesCol(\"scaledFeatures\") \\\n",
    "    .setLabelCol(\"label\") \n",
    "\n",
    "lrModel = lr.fit(scaled_df) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d868e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel: uid=LinearRegression_a4d23de06dcc, numFeatures=9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf11a4f",
   "metadata": {},
   "source": [
    "<h2>The pipeline</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30dd7bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "pipeline = Pipeline().setStages((assembler,scaler,lr))\n",
    "model = pipeline.fit(prepareData(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084af51",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Model evaluation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9adc2931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator()\\\n",
    "  .setLabelCol(\"label\")\\\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffab79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(prepareData(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2880957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  RMSE:  0.8771558134520177  Test  r2:  0.41378266806495867\n"
     ]
    }
   ],
   "source": [
    "rmse_test = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "r2_test = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "print(\"Test  RMSE: \",rmse_test,\" Test  r2: \",r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4459e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:  [0.0,0.0,0.0,0.5287039916961691,0.0,0.0,0.0,0.0,0.0]\n",
      "Intercept:  2.074400715885009\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "lrModel = model.stages[2]\n",
    "print(\"Coefficients: \",lrModel.coefficients) #untyped\n",
    "print(\"Intercept: \",lrModel.intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46348d8",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Hyperparameter tuning</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbfa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, (0.1, 0.01))\\\n",
    "    .addGrid(lr.elasticNetParam,(0.7,0.8, 0.9))\\\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a33342",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator()\\\n",
    "    .setEstimator(pipeline)\\\n",
    "    .setEvaluator(RegressionEvaluator())\\\n",
    "    .setEstimatorParamMaps(paramGrid)\\\n",
    "    .setNumFolds(3)\\\n",
    "    .setParallelism(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316387da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(prepareData(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33435d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r = cvModel.transform(prepareData(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5046a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator()\\\n",
    "  .setLabelCol(\"label\")\\\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  \n",
    "\n",
    "rmse = evaluator.setMetricName(\"rmse\").evaluate(test_r)\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(test_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvModel.bestModel\\\n",
    "    .stages[2]\\\n",
    "    .coefficients)\n",
    "\n",
    "print(cvModel.bestModel\n",
    "    .stages[2]\n",
    "    .intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13811543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
