{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">Windowing with DStreams</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.log4j.Logger\n",
    "import org.apache.log4j.Level\n",
    "\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "Logger.getLogger(\"akka\").setLevel(Level.OFF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Data distributions may change over time (non-stationary)</li>\n",
    "<li>We might want to overweight recent data and underweight older data</li>\n",
    "<li>Examples:\n",
    "<ul><li>Monitoring frequency of errors in log files\n",
    "<li>A high frequency market making app that uses data from short time windows\n",
    "<li>Monitoring passenger flows into a railway station \n",
    "</ul>\n",
    "    <li>Spark Streaming allows the creation of sliding windows (and, therefore, also fixed windows)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Windows in Spark</span>\n",
    "<br><br>\n",
    "<li>Spark DStream implements sliding windows</li>\n",
    "<li>Each window has a length (for example, 20 seconds)</li>\n",
    "<li>Each window has a \"sliding interval\" (for example, the window is created every 10 seconds)</li>\n",
    "<li>Window length is a multiple of microbatch size</li>\n",
    "<li>sliding interval is a multiple of microbatch size</li>\n",
    "<li>Both can be, at a minimum, the batch size</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">windows, slide intervals, and microbatches</span>\n",
    "<img src=\"streaming_windows.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Window transformations</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li><b>Window(window_length,slide_interval)</b>: Creates a window</li>\n",
    "<li><b>countByWindow(window_length,slide_interval)</b>: returns the number of elements in a window</li>\n",
    "<li><b>countByValueAndWindow(window_length,slide_interval)</b>: Counts the number for each key</li>\n",
    "<li><b>reduceByWindow(function,window_length,slide_interval)</b>: Applies reduce, using the specified function, on the data in the DStreams in the window</li>\n",
    "<li><b>reduceByKeyAndWindow</b>: The key version of reduceByWindow.  </li>\n",
    "<li>Windowed operations require a checkpoint</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>reduceByKeyAndWindow(reduce_function,inverse_function,window_duration,slide_duration)</b>\n",
    "<ul>\n",
    "<li><b>reduce_function</b>: The ordinary reduce function (in our example, add)\n",
    "<li><b>inverse_function</b>: What to do with data that slides out of the window\n",
    "<li><b>window_duration</b>: window size (multiple of batch interval\n",
    "<li><b>slide_duration</b>: the amount the window will slide by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Example: countByWindow</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Microbatch size: 10 seconds</li>\n",
    "<li>Window length: 30 seconds</li>\n",
    "<li>Slide interval: 20 seconds</li>\n",
    "<li>Report the total number of words in each microbatch and in each window</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "val words = lines.flatMap(l => l.split(\" \"))\n",
    "val pairs = words.map(word => (word, 1))\n",
    "val wordCounts = pairs.reduceByKey((x, y) => x + y)\n",
    "wordCounts.print()\n",
    "//Total words in the microbatch (10 seconds)\n",
    "//This will print every 10 seconds\n",
    "//words.count.print\n",
    "\n",
    "//Total number of words in every 30 second window sliding every 20 seconds\n",
    "//The print is at each point that the window slides (20, 40, 60 ,80,....)\n",
    "//This will print at 20 (total words in 20 seconds since this is the first slide)\n",
    "//at 40 seconds (total words in the last 3 10 second intervals)\n",
    "//at 60 seconds, etc.\n",
    "val wcount = words.countByWindow(Seconds(40),Seconds(20))\n",
    "wcount.print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>To pretty print, we can add  <span style=\"color:blue\">foreachRDD</span></li>\n",
    "<li><span style=\"color:blue\">foreachRDD</span> is an iterator through a collection of DStream RDDs</li>\n",
    "<li>In this example, we use it for printing</li>\n",
    "<li>Since wcount is an RDD, we need to extract the number of elements from it</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Example: foreachRDD</span>\n",
    "<li>DStream objects consist of RDDs</li>\n",
    "<li>foreachRDD applies a function to each RDD in a DStream object</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "val words = lines.flatMap(l => l.split(\" \"))\n",
    "\n",
    "//Total words in the microbatch (10 seconds)\n",
    "//This will print every 10 seconds\n",
    "words.count.print\n",
    "\n",
    "//Total number of words in every 30 second window sliding every 20 seconds\n",
    "//The print is at each point that the window slides (20, 40, 60 ,80,....)\n",
    "//This will print at 20 (total words in 20 seconds since this is the first slide)\n",
    "//at 40 seconds (total words in the last 3 10 second intervals)\n",
    "//at 60 seconds, etc.\n",
    "val wcount = words.countByWindow(Seconds(30),Seconds(20))\n",
    "wcount.foreachRDD((rdd,time) => print(time.toString.takeRight(6),\": Total  \",rdd.collect()(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Example: countByValueAndWindow</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Microbatch size: 10 seconds</li>\n",
    "<li>Window length: 30 seconds</li>\n",
    "<li>Slide interval: 20 seconds</li>\n",
    "<li>Calculate the total instances of each word in a window</li>\n",
    "<li>We'll use countByValueAndWindow for this</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@3f907db8\n",
       "lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@3fe4a956\n",
       "words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@2438db2e\n",
       "pairs: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.MappedDStream@3c5f8492\n",
       "windowedWordCounts1: org.apache.spark.streaming.dstream.DStream[((String, Int), Long)] = org.apache.spark.streaming.dstream.ReducedWindowedDStream@9e531c\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "val words = lines.flatMap(l => l.split(\" \"))\n",
    "\n",
    "//Construct the (word,1) pairs\n",
    "val pairs = words.map(word => (word, 1))\n",
    "pairs.print()\n",
    "\n",
    "val windowedWordCounts1 = pairs.countByValueAndWindow(Seconds(30),Seconds(20))\n",
    "\n",
    "\n",
    "windowedWordCounts1.print\n",
    "\n",
    "windowedWordCounts1.foreachRDD((rdd,time) => {\n",
    "    println(time.toString.takeRight(15));\n",
    "    rdd.foreach(r => println(r._1._1,r._2))\n",
    "})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1668290350000 ms\n",
      "-------------------------------------------\n",
      "(John,1)\n",
      "(John,1)\n",
      "(Jim,1)\n",
      "(Jim,1)\n",
      "(Jim,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1668290360000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1668290360000 ms\n",
      "-------------------------------------------\n",
      "((Jim,1),3)\n",
      "((John,1),2)\n",
      "\n",
      "668290360000 ms\n",
      "(Jim,3)\n",
      "(John,2)\n",
      "-------------------------------------------\n",
      "Time: 1668290370000 ms\n",
      "-------------------------------------------\n",
      "(james,1)\n",
      "(james,1)\n",
      "(Jim,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1668290380000 ms\n",
      "-------------------------------------------\n",
      "(John,1)\n",
      "(Jim,1)\n",
      "(Jim,1)\n",
      "(Jim,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1668290380000 ms\n",
      "-------------------------------------------\n",
      "((Jim,1),4)\n",
      "((james,1),2)\n",
      "((John,1),1)\n",
      "\n",
      "668290380000 ms\n",
      "(Jim,4)\n",
      "(james,2)\n",
      "(John,1)\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/12 16:59:44 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Example: reduceByKeyAndWindow</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Microbatch size: 10 seconds</li>\n",
    "<li>Window length: 30 seconds</li>\n",
    "<li>Slide interval: 20 seconds</li>\n",
    "<li>Every time the window slides, some data goes out of the window and some data enters</li>\n",
    "<li>We'll score each word as follows:</li>\n",
    "<ul>\n",
    "    <li>Add the instances of the words that enter to the current total</li>\n",
    "    <li>Subtract half the instances of the words that leave to the current total</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "val words = lines.flatMap(l => l.split(\" \"))\n",
    "val pairs = words.map(word => (word, 1.0)) //Make this double so that the division by 2 works properly\n",
    "\n",
    "//Total words in the microbatch (10 seconds)\n",
    "pairs.count.print\n",
    "\n",
    "//All the words in a 20 second window\n",
    "val window_data = words.window(Seconds(20))\n",
    "\n",
    "//Each word count, in a 60 second window, sliding every 20 seconds. The word counts are incremented by\n",
    "// the new words that enter every 20 seconds and decreased by half the count of the departing words (i.e., \n",
    "//of the words that go out of the window\n",
    "val windowedWordCounts = pairs.reduceByKeyAndWindow((x, y)=> x + y,(x,y)=> x - y/2, Seconds(60), Seconds(20))\n",
    "\n",
    "\n",
    "window_data.print\n",
    "windowedWordCounts.print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
