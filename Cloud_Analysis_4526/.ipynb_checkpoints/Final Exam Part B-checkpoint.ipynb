{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc50616",
   "metadata": {},
   "source": [
    "<h1>Load GraphFrames packages</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.packages= [\"graphframes:graphframes:0.8.2-spark3.2-s_2.12\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2676ba",
   "metadata": {},
   "source": [
    "<h1>Problem 1 Solution</h1>\n",
    "<li>GraphX and Pregel Problem</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "//WORK ON THE PROBLEM BELOW AND COPY YOUR FINAL SOLUTION INTO THIS CELL\n",
    "//YOUR ENTIRE SOLUTION MUST BE IN THIS SINGLE CELL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191642e7",
   "metadata": {},
   "source": [
    "<h1>Problem 2 Solution</h1>\n",
    "<li>Machine learning and pipelines problem</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eebc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "//WORK ON THE PROBLEM BELOW AND COPY YOUR FINAL SOLUTION INTO THIS CELL\n",
    "//YOUR ENTIRE SOLUTION MUST BE IN THIS SINGLE CELL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b9b53",
   "metadata": {},
   "source": [
    "<h1>Problem 3 Solution</h1>\n",
    "<li>GraphFrames motifs and DataFrames problem</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26abf702",
   "metadata": {},
   "outputs": [],
   "source": [
    "//WORK ON THE PROBLEM BELOW AND COPY YOUR FINAL SOLUTION INTO THIS CELL\n",
    "//YOUR ENTIRE SOLUTION MUST BE IN THIS SINGLE CELL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb7880",
   "metadata": {},
   "source": [
    "<h1>Problem 1</h1>\n",
    "<img src=\"sample graph.png\">\n",
    "<br>\n",
    "<p>\n",
    "In a graph, the flow from vertex v1 to vertex v2 along a path p, is defined as the minimum weight on that path. In the following graph, the flow from vertex A to vertex D along the path A-->C-->D is 6 because at most 6 units can complete the journey. The maximal flow from vertex v1 to vertex v2 is the path with the highest flow. In the graph, the maximal flow from A to F is 6, since the path A --> C -->D -->F has a flow of 6 while other paths have lower flows (A --> F: 5; A-->B-->F; 3; A-->C-->F: 2; A-->C-->D-->E-->F: 7)\n",
    "</p>\n",
    "<p>\n",
    "Using the GraphX pregel algorithm, compute the maximal flow from a given node to every other node in the graph. You need to compute both the maximal flow (e.g., 6) as well as a path with the maximal flow (any one path if more than one path are a maximum). For the example graph, starting from node 1, your code should return:\n",
    "</p>\n",
    "<pre>\n",
    "(1,(List(),Infinity))\n",
    "(2,(List(1),3.0))\n",
    "(3,(List(1),7.0))\n",
    "(4,(List(1, 3),6.0))\n",
    "(5,(List(1, 3, 4),5.0))\n",
    "(6,(List(1, 3, 4),6.0))\n",
    "(7,(List(1, 3, 4, 6),6.0))\n",
    "</pre>\n",
    "<p>\n",
    "Note the following:\n",
    "<ol>\n",
    "<li>The flow from a node to itself is infinity\n",
    "<li>The maximal flow path should be returned as a list\n",
    "<li>The general idea is the following:\n",
    "    <ul>\n",
    "<li>initialize the graph so that each vertex contains the maximal path from vertex 1 and the maximal flow. Initially, we don't know either so the path will be empty and the flow will be - what?\n",
    "<li>in each pass, send a message from vertex i to j. A message will be sent if the minimum of the flow at i and the edge weight from i to j is greater than the flow at j (otherwise don't send a message)\n",
    "<li>merge messages to pick the the message with the highest flow\n",
    "<li>in the vertex program, update the path and the flow in the vertex using the result of merge message\n",
    "    </ul>\n",
    "<li>Use the template in the problem submission cell to construct your program. The key in any pregel program is to get the structure of the message right - so make sure you figure that out first!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Solution template\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val vertexArray = Array(\n",
    "  (1, \"A\"),\n",
    "  (2, \"B\"),\n",
    "  (3, \"C\"),\n",
    "  (4, \"D\"),\n",
    "  (5, \"E\"),\n",
    "  (6, \"F\"),\n",
    "  (7, \"G\"),\n",
    ")\n",
    "\n",
    "val edgeArray = Array(\n",
    "    (1,2,3),\n",
    "    (1,6,5),\n",
    "    (1,3,7),\n",
    "    (2,6,10),\n",
    "    (2,7,1),\n",
    "    (3,6,2),\n",
    "    (3,4,6),\n",
    "    (4,6,8),\n",
    "    (4,5,5),\n",
    "    (5,6,7),\n",
    "    (5,7,3),\n",
    "    (6,7,6)\n",
    ")\n",
    "\n",
    "val vertexArrayX = vertexArray.map(r => (r._1.toLong,r._2))\n",
    "val edgeArrayX = edgeArray.map(r => Edge(r._1.toLong,r._2.toLong,r._3))\n",
    "\n",
    "//val vertexRDD = sc.parallelize(vertexArrayX)\n",
    "val vertexRDD: RDD[(Long, String)] = sc.parallelize(vertexArrayX)\n",
    "val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArrayX)\n",
    "\n",
    "val graph: Graph[String, Int] = Graph(vertexRDD, edgeRDD)\n",
    "\n",
    "\n",
    "//THE PROGRAM\n",
    "val sourceId: VertexId = 1 //Set the source for the maximal flows\n",
    "val initialGraph =  //Create the initial graph\n",
    "\n",
    "\n",
    "val vertex_program = //Create the vertex program\n",
    "\n",
    "val sendMsg = //Send message \n",
    "\n",
    "val mrgMsg = //merge messages\n",
    "\n",
    "val initial_message = //what should the initial message be!\n",
    "\n",
    "val sssp = initialGraph.pregel(initial_message)(\n",
    "  vertex_program,\n",
    "    sendMsg,\n",
    "    mrgMsg)\n",
    "\n",
    "println(sssp.vertices.collect.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5cb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbf430e8",
   "metadata": {},
   "source": [
    "<h1>Problem 2</h1>\n",
    "<p>\n",
    "A hedge fund is looking into building a trading model using sentiment and analyst ratings to predict the future one day return. Sentiments can be positve, negative or neutral. Analyst ratings are the most recent rating issued by an equity analyst and are integers ranging from 1 to 5 (1=strong sell, 5=strong buy) An example of the data that they plan to use is:\n",
    "</p>\n",
    "<pre>\n",
    "val dates = Array(\"20221201\",\"20221202\",\"20221203\",\"20221204\",\"20221205\",\"20221206\",\n",
    "\"20221207\",\"20221208\",\"20221209\",\"20221210\")\n",
    "val sentiment=Array(\"+\",\"+\",\"-\",\"-\",\" \",\"+\",\"+\",\"+\",\" \",\"-\")\n",
    "val price=Array(212.1,212.7,212.9,211.5,214.1,215.7,212.2,213.8,215.1,216.10)\n",
    "val new_analyst_rating = Array(1,4,2,2,2,1,3,2,2,4,4,5,5)\n",
    "</pre>\n",
    "Prepare a machine learning pipeline that will organize a ML model for the hedge fund. You will need to do the following:\n",
    "<ol>\n",
    "    <li>Organize the data into a dataframe</li>\n",
    "    <li>Calculate the one-day future return (for example, the one day future return of 20221201 is (212.7/212.1) - 1</li>\n",
    "    <li>Drop any rows that have null future returns</li>\n",
    "    <li>Bucketize the one-day future returns into 5 buckets (use <a href=\"https://spark.apache.org/docs/latest/ml-features#quantilediscretizer\">QuantileDiscretizer</a> to bucket the data</li>\n",
    "    <li>Use <a href=\"https://spark.apache.org/docs/latest/ml-features#stringindexer\">StringIndexer</a> to convert sentiments into integers</li>\n",
    "    <li>Use <a href=\"https://spark.apache.org/docs/latest/ml-features#onehotencoder\">OneHotEncoding</a> to one hot encode sentiments</li>\n",
    "    <li>Use <a href=\"https://spark.apache.org/docs/latest/ml-features#vectorassembler\">VectorAssembler</a> to collect the independent variables that will be used in the model into a vector</li>\n",
    "    <li>Use <a href=\"https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-regression\">RandomForestRegressor</a> as your model choice</li>\n",
    "</ol>    \n",
    "<h3>Notes for Problem 2</h3>\n",
    "<p>\n",
    "    <h4>Creating the dataframe</h4>\n",
    "    <li>Use whatever method makes sense to you to combine the data into an appropriate array for creating the dataframe but, perhaps, <a href=\"https://www.geeksforgeeks.org/scala-iterator-zip-method-with-example/\">zip</a> with map may be the easiest</li>\n",
    "    <h4>Computing one day future returns</h4>\n",
    "    <li>To compute the one day future return you need to create an additional \n",
    "column in your dataframe that contains the price for the next day</li>\n",
    "    <li>Then, a simple operation on the price today and the price next day column\n",
    "will give you the one day future return</li>\n",
    "    <li>See this stack overflow post for how to create the additional column (lag over window) <a href=\"https://stackoverflow.com/questions/41158115/spark-sql-window-function-lag\">https://stackoverflow.com/questions/41158115/spark-sql-window-function-lag</a></li>\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7677e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "\n",
    "val dates = Array(\"20221201\",\"20221202\",\"20221203\",\"20221204\",\"20221205\",\"20221206\",\n",
    "\"20221207\",\"20221208\",\"20221209\",\"20221210\")\n",
    "val sentiment=Array(\"+\",\"+\",\"-\",\"-\",\" \",\"+\",\"+\",\"+\",\" \",\"-\")\n",
    "val price=Array(212.1,212.7,212.9,211.5,214.1,215.7,212.2,213.8,215.1,216.10)\n",
    "val new_analyst_rating = Array(1,4,2,2,2,1,3,2,2,4,4,5,5)\n",
    "\n",
    "val df = //Create the dataframe\n",
    "\n",
    "def prepareData //Write the prepareData function\n",
    "\n",
    "//Construct each transformer and the pipeline (no evaluation necessary!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c446bd5a",
   "metadata": {},
   "source": [
    "<h1>Problem 3</h1>\n",
    "Given a graphframes graph, g, find the value of the lowest weighted edge in all paths of length 3 (i.e., 3 edges in the path) such that the nodes on the paths are distinct (no duplicates). For the example graph, your code should return:\n",
    "\n",
    "<pre>\n",
    "+-------+-------+-----+-------------+\n",
    "|   from|    via|   to|lowest_weight|\n",
    "+-------+-------+-----+-------------+\n",
    "|Charlie|    Bob|David|            1|\n",
    "|Charlie|    Bob|Alice|            4|\n",
    "|  David|  Alice|  Bob|            1|\n",
    "|     Ed|Charlie|  Bob|            2|\n",
    "|     Ed|Charlie|  Bob|            4|\n",
    "|     Ed|    Bob|David|            1|\n",
    "|     Ed|    Bob|Alice|            2|\n",
    "+-------+-------+-----+-------------+\n",
    "</pre>\n",
    "\n",
    "<h3>Notes:</h3>\n",
    "<li>The spark sql function <a href=\"https://spark.apache.org/docs/2.3.0/api/sql/index.html#least\">least</a> will be useful</li>\n",
    "<li>Note that the result must be in the specified format</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.graphframes._\n",
    "val vertexArray = Array(\n",
    "  (1, \"Alice\", 28),\n",
    "  (2, \"Bob\", 27),\n",
    "  (3, \"Charlie\", 65),\n",
    "  (4, \"David\", 42),\n",
    "  (5, \"Ed\", 55),\n",
    "  (6, \"Fran\", 50),\n",
    "    (7, \"Qing\",27),\n",
    "    (8, \"Sarika\",78),\n",
    "    (9, \"Olafson\",17),\n",
    "    (10, \"Birgit\",33)\n",
    ")\n",
    "\n",
    "val edgeArray = Array(\n",
    "  (2, 1, 7),\n",
    "  (1, 2, 13),\n",
    "  (2, 4, 2),\n",
    "  (3, 2, 4),\n",
    "  (3, 6, 3),\n",
    "  (4, 1, 1),\n",
    "  (5, 2, 2),\n",
    "  (5, 3, 8),\n",
    "  (5, 6, 3),\n",
    "    (7, 8, 14),\n",
    "    (7, 9, 2),\n",
    "    (8, 10, 8),\n",
    "    (9, 10, 6)\n",
    ")\n",
    "\n",
    "val vertex_df = spark.createDataFrame(vertexArray).toDF(\"id\",\"name\",\"age\")\n",
    "val edge_df = spark.createDataFrame(edgeArray).toDF(\"src\",\"dst\",\"attr\")\n",
    "\n",
    "val g = GraphFrame(vertex_df, edge_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f329457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
