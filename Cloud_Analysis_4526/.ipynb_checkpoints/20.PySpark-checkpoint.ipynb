{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d865e7b8",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">PySpark</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aedf52a",
   "metadata": {},
   "source": [
    "<li>The PySpark package should already be installed</li>\n",
    "<li>If it isn't, use pip or conda to install the correct version of pyspark</li>\n",
    "<li>Check the spark version <span style=\"color:blue\">spark.version</span> (mine is 3.3.0) and install the same version of pyspark (e.g., <span style=\"color:blue\">pip install pyspark==3.3.0</span>)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2aba0a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Start a spark session</span>\n",
    "<li>Unlike Scala Spark, you need to explicitly start a spark session</li>\n",
    "<li>And you need to explicitly extract the spark context</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark ML Basics\").getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3ae83",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "<span style=\"color:green;font-size:xx-large\">Working with RDDs in pyspark</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38943639",
   "metadata": {},
   "source": [
    "<li>The spark API for Python is similar to the API for Scala</li>\n",
    "<li>With the difference that actual data objects are Python objects (untyped) and not Scala objects</li>\n",
    "<li>Compare the data type of the RDD below with the equivalent RDD in Scala</li>\n",
    "<pre>\n",
    "x: Array[(String, Int, Int)] = Array((John,20,32), (Jill,15,45))\n",
    "res6: org.apache.spark.rdd.RDD[(String, Int, Int)] = ParallelCollectionRDD[7] at parallelize at <console>:33\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2055d7",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Creating an RDD</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca311412",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [(\"John\",20,32),(\"Jill\",15,45)]\n",
    "sc.parallelize(x) #No type information!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af8242",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Reading a text file</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb56267",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(\"../../DataAnalytics/DataVisualization/nyc_311_2022_clean.csv\")\n",
    "data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a87ed2",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Using map</span>\n",
    "<li>Note the use of lambda functions (Python's anonymous function)</li>\n",
    "<li>Actions like <span style=\"color:blue\">take</span>, <span style=\"color:blue\">first</span>, <span style=\"color:blue\">collect</span>, etc., are the same as in the scala API</li>\n",
    "<li>But, you need to always use parentheses to indicate that these are functions</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade60a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = data.map(lambda l: l.split(\",\")).map(lambda l: (l[2],l[10]))\n",
    "time_data.first()\n",
    "#time_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db13b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e872b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f91a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c8cc5",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Drop the header row</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41bdc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data.mapPartitionsWithIndex(lambda i,it: iter(list(it)[1:] if i==0 else it)).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f495bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data.mapPartitionsWithIndex(lambda i,it: iter(list(it)[1:] if i==0 else it)).take(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103cfe59",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Convert processing time to float</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646996bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data.mapPartitionsWithIndex(lambda i,it: iter(list(it)[1:] if i==0 else it)).\\\n",
    "    map(lambda l: (l[0],float(l[1]))).\\\n",
    "    take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cff8464",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = time_data.mapPartitionsWithIndex(lambda i,it: iter(list(it)[1:] if i==0 else it)).\\\n",
    "    map(lambda l: (l[0],float(l[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca18cd3",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Calculate averages using combineByKey</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bafcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combiner(x):\n",
    "    return (1,x)\n",
    "\n",
    "def merger(x,y):\n",
    "    return ((x[0]+1,x[1]+y))\n",
    "\n",
    "def merge_and_combiner(x,y):\n",
    "    return ((x[0]+y[0],x[1]+y[1]))\n",
    "\n",
    "processed_data.combineByKey(combiner,merger,merge_and_combiner)\\\n",
    "    .map(lambda x: (x[0], x[1][1]/x[1][0]))\\\n",
    "    .collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99b5b0",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">PySpark Dataframes</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514aba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('header',True).csv(\"../../DataAnalytics/DataVisualization/nyc_311_2022_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91476852",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Reassigning values is possible in PySpark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b83abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"Incident Zip\",\"Zip Code\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f746d",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">df.select</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Agency\",\"processing_days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae26dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.select(\"Agency\",col(\"processing_days\")*24).toDF(\"Agency\",\"processing_hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Agency\",df[\"processing_days\"]*24).toDF(\"Agency\",\"processing_hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922df3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"processing_hours\",df[\"processing_days\"]*24).take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071a17c",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">SQL</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb22848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dataDB\")\n",
    "spark.sql(\"select Agency, AVG(processing_days) from dataDB GROUP BY Agency\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052482ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b96118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
