{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">GraphFrames Assignment</span>\n",
    "<br><br>\n",
    "In this assignment, you need to do the following:\n",
    "<li>Read the file 201710-citibike-tripdata.csv</li>\n",
    "<li>Construct a graph with stations as vertices and trips between stations as edges</li>\n",
    "<li>Vertex Ids are station numbers and Vertex attributes are station names</li>\n",
    "<li>Edge attributes are trip duration (durations are in seconds)</li>\n",
    "<li>Then answer the questions below</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>NOTE</h2>\n",
    "<li>There is a good chance that this won't run on your local Jupyter notebook. If that happens, create a subset of the data (you can use python to do that), run it locally, and then run it on the entire dataset on GCP</li>\n",
    "<li>If you reboot your machine, make sure no other applications are open, and then work on the assignment, you have a good shot (depending on your machine) of running it locally</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.packages= [\"graphframes:graphframes:0.8.2-spark3.2-s_2.12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://cluster-be60-m:8088/proxy/application_1668271071267_0002\n",
       "SparkContext available as 'sc' (version = 3.1.3, master = yarn, app id = application_1668271071267_0002)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.graphframes._\n",
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//GraphFrame imports\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.graphframes._\n",
    "\n",
    "\n",
    "//GraphX imports\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Step 1: Construct the graph</span>\n",
    "<br><br>\n",
    "<li>read the data file and drop the header line</li>\n",
    "<li>create a vertex dataframe (the union of start stations and end stations)</li>\n",
    "<li>create an edge dataframe (the trips - start station id, end station id, duration)</li>\n",
    "<li>create a GraphFrame</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[String] = gs://wz2547-ieor4526-bucket/data/201710-citibike-tripdata.csv MapPartitionsRDD[1] at textFile at <console>:37\n",
       "text_nohead: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[3] at map at <console>:38\n",
       "rdd_vertex_start: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[4] at map at <console>:41\n",
       "rdd_vertex_end: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[5] at map at <console>:42\n",
       "rdd_vertex: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[9] at distinct at <console>:43\n",
       "vertices: org.apache.spark.sql.DataFrame = [id: string, attr: string]\n",
       "rdd_edge: org.apache.spark.rdd.RDD[(String, String, Float)] = MapPartitionsRDD[10] at map at <console>:46\n",
       "edges: org.apache.spark.sql.DataFrame = [src: string, dst...\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = sc.textFile(\"gs://wz2547-ieor4526-bucket/data/201710-citibike-tripdata.csv\")\n",
    "val text_nohead = text.mapPartitionsWithIndex{ (idx,iter) => if (idx==0) iter.drop(1) else iter}.map(x=>x.split(','))\n",
    "\n",
    "//Construct vertices and edges here\n",
    "val rdd_vertex_start = text_nohead.map(x=>(x(3),x(4)))\n",
    "val rdd_vertex_end = text_nohead.map(x=>(x(7),x(8)))\n",
    "val rdd_vertex = rdd_vertex_start.union(rdd_vertex_end).distinct\n",
    "val vertices = spark.createDataFrame(rdd_vertex).toDF(\"id\",\"attr\")\n",
    "\n",
    "val rdd_edge = text_nohead.map(x=>(x(3),x(7),x(0).toFloat))\n",
    "val edges = spark.createDataFrame(rdd_edge).toDF(\"src\",\"dst\",\"duration_secs\")\n",
    "\n",
    "\n",
    "val g = GraphFrame(vertices,edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// g.edges.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Step 2: Basic questions</span>\n",
    "<br><br>\n",
    "\n",
    "<li>How many citibike stations are there in the network?</li>\n",
    "<li>How many trips were made in the month in question?</li>\n",
    "<li>How many trips started and ended at the same station?</li>\n",
    "<li>How many station to station connections are there (at least one edge exists between station i and station j and i is not equal to j)?</li>\n",
    "<li>Your code should print:</li>\n",
    "<pre>\n",
    "Total number of stations: 785\n",
    "Total number of trips.  : 1897592\n",
    "Trips that started and ended at the same station: 33245\n",
    "Number of station to station connections: 107524\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_undirected_graph: (g: org.graphframes.GraphFrame)org.graphframes.GraphFrame\n",
       "total_stations: Long = 785\n",
       "total_trips: Long = 1897592\n",
       "round_trips: Long = 33245\n",
       "u_g: org.graphframes.GraphFrame = GraphFrame(v:[id: string, attr: string], e:[src: string, dst: string])\n",
       "station_to_station: Long = 107524\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//You might need this\n",
    "def make_undirected_graph(g: GraphFrame) = {\n",
    "    val u_edge_df = g.find(\"(a)-[]->(b)\")\n",
    "        .select($\"a.id\".as(\"src\"),$\"b.id\".as(\"dst\"))\n",
    "        .withColumn(\"swap\",when(col(\"src\")<col(\"dst\"),col(\"dst\")))\n",
    "        .withColumn(\"dst\",\n",
    "                    when(col(\"swap\").isNotNull,col(\"src\"))\n",
    "                    .otherwise(col(\"dst\")))\n",
    "        .withColumn(\"src\",\n",
    "                    when(col(\"swap\").isNotNull,col(\"swap\"))\n",
    "                   .otherwise(col(\"src\")))\n",
    "        .drop(col(\"swap\"))\n",
    "        .distinct\n",
    "    val u_vertices_df = g.vertices\n",
    "    val u_g = GraphFrame(u_vertices_df,u_edge_df)    \n",
    "    u_g\n",
    "}\n",
    "val total_stations = g.vertices.count()\n",
    "val total_trips = g.edges.count()\n",
    "val round_trips = g.find(\"(a)-[]->(a)\").count()\n",
    "val u_g = make_undirected_graph(g)\n",
    "val station_to_station = u_g.find(\"(a)-[e]->(b)\").filter(\"a.id != b.id\").select(\"e.src\",\"e.dst\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stations: 785\n",
      "Total number of trips: 1897592\n",
      "Trips that started and ended at the same station: 33245\n",
      "Number of station to station connections: 107524\n"
     ]
    }
   ],
   "source": [
    "println(s\"Total number of stations: $total_stations\")\n",
    "println(s\"Total number of trips: $total_trips\")\n",
    "println(s\"Trips that started and ended at the same station: $round_trips\")\n",
    "println(s\"Number of station to station connections: $station_to_station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Step 3: Find the Station from which most trips originate</span>\n",
    "<br><br>\n",
    "<li>Note that the graph has one edge for each trip (i.e., there are many edges between two vertices)</li>\n",
    "<li>The function <span style=\"color:blue\">outDegrees</span> returns the number of outgoing edges from every vertex</li>\n",
    "<li>Print the name of the station with most originating trips</li>\n",
    "<li>Your code should print:</li>\n",
    "<pre>  \n",
    "The station from which most trips originate is: \"Pershing Square North\"\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outDegree: org.apache.spark.sql.DataFrame = [id: string, outDegree: int]\n",
       "most_station_id: String = 519\n",
       "most_trips: String = \"Pershing Square North\"\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outDegree = g.outDegrees\n",
    "outDegree.createOrReplaceTempView(\"outDegree_v\")\n",
    "val most_station_id = spark.sql(\"select id from outDegree_v order by outDegree desc limit 1\").collect()(0)(0).toString\n",
    "val most_trips = g.vertices.select(\"attr\").filter($\"id\" === most_station_id).collect()(0)(0).toString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The station from which most trips originate is: \"Pershing Square North\"\n"
     ]
    }
   ],
   "source": [
    "println(s\"The station from which most trips originate is: $most_trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">STEP 4: Proportion of trips for each station that start and end at that same station</span>\n",
    "<br><br>\n",
    "<li>Create a GraphX graph from the GraphFrames graph (use the method that retains datatypes)</li>\n",
    "<li>Use aggregateMessages to calculate the number of trips that start and end at the same vertex (for each vertex)</li>\n",
    "<li>Convert the resulting (VertexRDD) to a DataFrame</li>\n",
    "<li>Using join add the location of the station column to the result df from the previous step and then use select to create a dataframe with the schema (vertex, location, trips)</li>\n",
    "<li>Join this df to the out degrees df created earlier</li>\n",
    "<li>Divide the \"same trips\" column by the \"out degrees column\" </li>\n",
    "<li>Sort the resulting df by this proportion in descending order</li>\n",
    "<li>Your output should be the following dataframe\n",
    "<pre>\n",
    "\n",
    "+----+--------------------+-----+---------+-------------------+\n",
    "|  id|            location|trips|outDegree|               prop|\n",
    "+----+--------------------+-----+---------+-------------------+\n",
    "|3488|  \"8D QC Station 01\"|    1|        1|                1.0|\n",
    "|3245|\"NYCBS DEPOT - DE...|    1|        2|                0.5|\n",
    "|3182|\"Yankee Ferry Ter...|  309|      900| 0.3433333333333333|\n",
    "|3254|  \"Soissons Landing\"|  358|     1100|0.32545454545454544|\n",
    "|3342|\"Pioneer St & Ric...|   59|      299|0.19732441471571907|\n",
    "|3477|\"39 St & 2 Ave - ...|   45|      245| 0.1836734693877551|\n",
    "|3532|\"Ditmars Blvd & 1...|   70|      407|  0.171990171990172|\n",
    "|3180|\"Brooklyn Bridge ...|  232|     1354|0.17134416543574593|\n",
    "|3423|\"West Drive & Pro...|  367|     2463|0.14900527811611855|\n",
    "|3636|\"Expansion Wareho...|    1|        8|              0.125|\n",
    "|3302|\"Columbus Ave & W...|   74|      598|0.12374581939799331|\n",
    "|3120|\"Center Blvd & Bo...|   74|      622| 0.1189710610932476|\n",
    "|3514|\"Astoria Park S &...|   34|      299|0.11371237458193979|\n",
    "|3479|      \"Picnic Point\"|   77|      712|0.10814606741573034|\n",
    "|3594|\"Montgomery St & ...|    9|       87|0.10344827586206896|\n",
    "|3524|    \"19 St & 24 Ave\"|   24|      249| 0.0963855421686747|\n",
    "|3349|\"Grand Army Plaza...|  237|     2570|0.09221789883268483|\n",
    "|3333|\"Columbia St & Lo...|    5|       55|0.09090909090909091|\n",
    "|3354|\"3 St & Prospect ...|  142|     1572|0.09033078880407125|\n",
    "|3607|    \"31 Ave & 14 St\"|   10|      113|0.08849557522123894|\n",
    "+----+--------------------+-----+---------+-------------------+\n",
    "only showing top 20 rows\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v: org.apache.spark.rdd.RDD[(Long, String)] = MapPartitionsRDD[69] at map at <console>:37\n",
       "e: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] = MapPartitionsRDD[75] at map at <console>:38\n",
       "gx: org.apache.spark.graphx.Graph[String,Double] = org.apache.spark.graphx.impl.GraphImpl@2927a853\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val v = g.vertices.rdd.map(r => (r(0).toString.toLong,r(1).toString))\n",
    "val e = g.edges.rdd.map(r => Edge(r(0).toString.toLong,r(1).toString.toLong,r(2).toString.toDouble))\n",
    "val gx: Graph[String, Double] = Graph(v, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+---------+-------------------+\n",
      "|  id|            location|trips|outDegree|               prop|\n",
      "+----+--------------------+-----+---------+-------------------+\n",
      "|3488|  \"8D QC Station 01\"|    1|        1|                1.0|\n",
      "|3245|\"NYCBS DEPOT - DE...|    1|        2|                0.5|\n",
      "|3182|\"Yankee Ferry Ter...|  309|      900| 0.3433333333333333|\n",
      "|3254|  \"Soissons Landing\"|  358|     1100|0.32545454545454544|\n",
      "|3342|\"Pioneer St & Ric...|   59|      299|0.19732441471571907|\n",
      "|3477|\"39 St & 2 Ave - ...|   45|      245| 0.1836734693877551|\n",
      "|3532|\"Ditmars Blvd & 1...|   70|      407|  0.171990171990172|\n",
      "|3180|\"Brooklyn Bridge ...|  232|     1354|0.17134416543574593|\n",
      "|3423|\"West Drive & Pro...|  367|     2463|0.14900527811611855|\n",
      "|3636|\"Expansion Wareho...|    1|        8|              0.125|\n",
      "|3302|\"Columbus Ave & W...|   74|      598|0.12374581939799331|\n",
      "|3120|\"Center Blvd & Bo...|   74|      622| 0.1189710610932476|\n",
      "|3514|\"Astoria Park S &...|   34|      299|0.11371237458193979|\n",
      "|3479|      \"Picnic Point\"|   77|      712|0.10814606741573034|\n",
      "|3594|\"Montgomery St & ...|    9|       87|0.10344827586206896|\n",
      "|3524|    \"19 St & 24 Ave\"|   24|      249| 0.0963855421686747|\n",
      "|3349|\"Grand Army Plaza...|  237|     2570|0.09221789883268483|\n",
      "|3333|\"Columbia St & Lo...|    5|       55|0.09090909090909091|\n",
      "|3354|\"3 St & Prospect ...|  142|     1572|0.09033078880407125|\n",
      "|3607|    \"31 Ave & 14 St\"|   10|      113|0.08849557522123894|\n",
      "+----+--------------------+-----+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trip_count: org.apache.spark.sql.DataFrame = [id: bigint, trips: int]\n",
       "trip_location: org.apache.spark.sql.DataFrame = [id: string, location: string ... 1 more field]\n",
       "trip_outDegree: org.apache.spark.sql.DataFrame = [id: string, location: string ... 2 more fields]\n",
       "trip_prop: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, location: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trip_count = gx.aggregateMessages[Int](ec => if (ec.srcId == ec.dstId) ec.sendToDst(1)\n",
    "                                           ,(a,b) => a+b)\n",
    "                   .toDF(\"id\",\"trips\")\n",
    "val trip_location = vertices.join(trip_count,g.vertices(\"id\") === trip_count(\"id\"),\"left\")\n",
    "                            .select(g.vertices(\"id\"),g.vertices(\"attr\") as \"location\",trip_count(\"trips\"))\n",
    "val trip_outDegree = trip_location.join(g.outDegrees,trip_location(\"id\") === g.outDegrees(\"id\"),\"left\")\n",
    "                                  .drop(g.outDegrees(\"id\"))\n",
    "\n",
    "val trip_prop = trip_outDegree.withColumn(\"prop\",$\"trips\"/$\"outDegree\").orderBy(col(\"prop\").desc)\n",
    "trip_prop.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// round_trip_count_df.createOrReplaceTempView(\"trips\")\n",
    "// g.vertices.createOrReplaceTempView(\"vertices\")\n",
    "// spark.sql(\"select a.id, a.attr, b.trips from vertices as a left join trips as b on a.id = b.id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">STEP 5: Create a new graph that contains all edges except for those between the same station</span>\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "except_same_station: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [a: struct<id: string, attr: string>, e: struct<src: string, dst: string ... 1 more field> ... 1 more field]\n",
       "except_same_station_vertices: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, attr: string]\n",
       "except_same_station_edges: org.apache.spark.sql.DataFrame = [src: string, dst: string ... 1 more field]\n",
       "new_g: org.graphframes.GraphFrame = GraphFrame(v:[id: string, attr: string], e:[src: string, dst: string ... 1 more field])\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val except_same_station = g.find(\"(a)-[e]->(b)\").filter(\"a.id != b.id\")\n",
    "val except_same_station_vertices = except_same_station.select(\"a.*\").union(except_same_station.select(\"b.*\"))\n",
    "val except_same_station_edges = except_same_station.select(\"e.*\")\n",
    "val new_g = GraphFrame(except_same_station_vertices,except_same_station_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- duration_secs: float (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_g.edges.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- attr: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_g.vertices.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">STEP 6: Calculate the average duration between every pair of stations</span>\n",
    "<br><br>\n",
    "<li>use the new graph from step 5 for this</li>\n",
    "<li>I'll let you figure this out but this should be really easy (think SQL)</li>\n",
    "<pre>\n",
    "+----+----+------------------+\n",
    "| src| dst|                 m|\n",
    "+----+----+------------------+\n",
    "| 504| 350| 772.7647058823529|\n",
    "| 433| 527| 532.9677419354839|\n",
    "| 434| 470|316.52272727272725|\n",
    "| 438| 151|  546.195652173913|\n",
    "| 445| 507| 553.3947368421053|\n",
    "|2021| 446| 827.6904761904761|\n",
    "| 116| 518| 1115.857142857143|\n",
    "|3435| 358| 895.6666666666666|\n",
    "|3402|3414| 634.1666666666666|\n",
    "| 498| 495| 801.2272727272727|\n",
    "|3637| 418|             889.7|\n",
    "| 380|3260|419.42105263157896|\n",
    "|3360| 507|            1442.0|\n",
    "| 326| 247|             713.0|\n",
    "|3358| 467| 500.6666666666667|\n",
    "|3164| 457|502.97241379310344|\n",
    "| 498| 528| 434.3207547169811|\n",
    "| 405|3256| 843.2168674698795|\n",
    "| 477|2000|2672.3333333333335|\n",
    "|3226|3163| 686.4444444444445|\n",
    "+----+----+------------------+\n",
    "only showing top 20 rows\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------------------+\n",
      "| src| dst|                 m|\n",
      "+----+----+------------------+\n",
      "| 467| 330|            1388.5|\n",
      "| 467| 144| 791.6666666666666|\n",
      "| 296|2021| 5554.333333333333|\n",
      "|3312| 519|1330.1739130434783|\n",
      "| 447|3289|            1035.4|\n",
      "| 307| 454|            1307.8|\n",
      "| 307| 417| 713.7407407407408|\n",
      "| 307| 249|             764.5|\n",
      "| 307| 498|            1260.0|\n",
      "|3167|3258|            1017.0|\n",
      "|3167|3577|            2074.0|\n",
      "| 334| 439| 912.7391304347826|\n",
      "|3553|3542|            1702.8|\n",
      "|3553| 285|            2477.0|\n",
      "| 334| 443|            2291.0|\n",
      "|3408| 217| 968.7826086956521|\n",
      "|3365|3324|             580.6|\n",
      "| 442|3307|            1569.8|\n",
      "| 470| 383| 518.5862068965517|\n",
      "| 470|2003|          677.5625|\n",
      "+----+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_g.edges.groupBy(\"src\",\"dst\").mean(\"duration_secs\")\n",
    ".withColumnRenamed(\"avg(duration_secs)\",\"m\")\n",
    "// .filter($\"src\"===\"504\").filter(\"dst==350\")\n",
    ".show\n",
    "// spark.sql(\"select src, dst, avg(duration_secs) from new_g.edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">STEP 7: Important stations</span><br><br>\n",
    "Citibike wants to figure out how best to deploy its workers in checking whether a station is over-full (too many bikes) or needs more bikes. It figures that the best way to do this is to find out which stations are the most important in terms of flows:\n",
    "<li>A station that has high bike returns and is connected to other stations with high bike returns is more likely to have too many bikes in its station and therefore should be monitored more often</li>\n",
    "<li>A station that has high bike pickups and is connected to other stations with high bike pickups is more likely to be short of bikes and therefore should be monitored more often</li>\n",
    "<li>Calculate the propensities for over-fullness and emptiness for every station</li>\n",
    "<li>Report the 5 most important stations for over-fullness (use pageRank on the graph)</li>\n",
    "<li>Report the 5 most important stations for emptiness (reverse all the edges on the graph and use pageRank)</li>\n",
    "<li>Your results (Don't worry about the meaning of location names!):</li>\n",
    "<li>Note: Assume a reset_probability of 0.15 and a tolerance of .0001 if you want the same results as mine</li>\n",
    "<pre>\n",
    "+---+--------------------+------------------+\n",
    "| id|            location|          pagerank|\n",
    "+---+--------------------+------------------+\n",
    "|519|\"Pershing Square ...| 4.930887390071603|\n",
    "|426|\"West St & Chambe...|3.7410934274030576|\n",
    "|402|\"Broadway & E 22 St\"|  3.58520147183096|\n",
    "|497|\"E 17 St & Broadway\"| 3.537658018512581|\n",
    "|435|   \"W 21 St & 6 Ave\"| 3.438585855241344|\n",
    "+---+--------------------+------------------+\n",
    "\n",
    "+----+--------------------+------------------+\n",
    "|  id|            location|          pagerank|\n",
    "+----+--------------------+------------------+\n",
    "|3197|      \"Hs Don't Use\"| 5.710640869520747|\n",
    "| 519|\"Pershing Square ...| 5.012823444592195|\n",
    "|3480|      \"WS Don't Use\"| 4.272284643284593|\n",
    "| 402|\"Broadway & E 22 St\"|3.4515211069038183|\n",
    "| 497|\"E 17 St & Broadway\"|3.3347259745457443|\n",
    "+----+--------------------+------------------+\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------------+\n",
      "| id|                attr|          pagerank|\n",
      "+---+--------------------+------------------+\n",
      "|519|\"Pershing Square ...| 4.930887390071584|\n",
      "|426|\"West St & Chambe...|3.7410934274030416|\n",
      "|402|\"Broadway & E 22 St\"| 3.585201471830956|\n",
      "|497|\"E 17 St & Broadway\"|3.5376580185125737|\n",
      "|435|   \"W 21 St & 6 Ave\"| 3.438585855241341|\n",
      "+---+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ranks: org.graphframes.GraphFrame = GraphFrame(v:[id: string, attr: string ... 1 more field], e:[src: string, dst: string ... 2 more fields])\n",
       "top_fullness: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, attr: string ... 1 more field]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ranks = g.pageRank.resetProbability(0.15).tol(0.0001).run()\n",
    "val top_fullness = ranks.vertices.orderBy(col(\"pagerank\").desc)\n",
    "top_fullness.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+\n",
      "|  id|                attr|          pagerank|\n",
      "+----+--------------------+------------------+\n",
      "|3197|      \"Hs Don't Use\"| 5.710640869520743|\n",
      "| 519|\"Pershing Square ...| 5.012823444592204|\n",
      "|3480|      \"WS Don't Use\"|  4.27228464328459|\n",
      "| 402|\"Broadway & E 22 St\"| 3.451521106903815|\n",
      "| 497|\"E 17 St & Broadway\"|3.3347259745457425|\n",
      "+----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reverse_edges: org.apache.spark.sql.DataFrame = [dst: string, src: string ... 1 more field]\n",
       "reverse_g: org.graphframes.GraphFrame = GraphFrame(v:[id: string, attr: string], e:[src: string, dst: string ... 1 more field])\n",
       "reverse_ranks: org.graphframes.GraphFrame = GraphFrame(v:[id: string, attr: string ... 1 more field], e:[src: string, dst: string ... 2 more fields])\n",
       "top_emptiness: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, attr: string ... 1 more field]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reverse_edges = g.edges.select($\"src\" as \"dst\",$\"dst\" as \"src\",$\"duration_secs\")\n",
    "val reverse_g = GraphFrame(g.vertices,reverse_edges)\n",
    "val reverse_ranks = reverse_g.pageRank.resetProbability(0.15).tol(0.0001).run()\n",
    "val top_emptiness = reverse_ranks.vertices.orderBy(col(\"pagerank\").desc)\n",
    "top_emptiness.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">STEP 8: Calculate the clustering coefficient of every station</span><br><br>\n",
    "\n",
    "<li>And report the top 20 stations by clustering coefficient</li>\n",
    "<li>Find the number of triangles that each vertex belongs to in the undirected graph</li>\n",
    "<li>Get the number of adjacent vertices (degrees of the undirected graph)</li>\n",
    "<li>Calculate the number of possible triangles a vertex can belong to (for every vertex)</li>\n",
    "<li>Divide actual triangles by possible triangles for each vertex\n",
    "\n",
    "<li>And report the top 20 stations by clustering coefficient</li>\n",
    "<pre>\n",
    "+----+--------------------+------------------+\n",
    "|  id|            location|             coeff|\n",
    "+----+--------------------+------------------+\n",
    "|3040|     \"GOW Tech Shop\"|               1.0|\n",
    "|3639|        \"Harborside\"|               1.0|\n",
    "|3192|\"Liberty Light Rail\"|               1.0|\n",
    "|3485| \"NYCBS Depot - RIS\"|               1.0|\n",
    "|3647|    \"48 Ave & 30 Pl\"|               1.0|\n",
    "|3279|       \"Dixon Mills\"|               1.0|\n",
    "|3186|     \"Grove St PATH\"|               1.0|\n",
    "| 153|   \"E 40 St & 5 Ave\"|               1.0|\n",
    "| 339|\"Avenue D & E 12 St\"| 0.877201420748853|\n",
    "|3464|\"W 37 St & Broadway\"|0.8679573382796197|\n",
    "| 247|\"Perry St & Bleec...|0.8602079768329604|\n",
    "|3175|\"W 70 St & Amster...|0.8592469808193227|\n",
    "|3176|\"W 64 St & West E...|0.8568452539928423|\n",
    "|3623|\"W 120 St & Clare...|0.8549019607843137|\n",
    "|3491|  \"E 118 St & 1 Ave\"| 0.854122621564482|\n",
    "| 266| \"Avenue D & E 8 St\"| 0.849218980253463|\n",
    "|3441|   \"10 Hudson Yards\"|0.8482701509017299|\n",
    "|3646|    \"35 Ave & 10 St\"|0.8333333333333334|\n",
    "|3642|\"E 98 St & Lexing...|             0.832|\n",
    "| 444|\"Broadway & W 24 St\"|0.8283229697508064|\n",
    "+----+--------------------+------------------+\n",
    "only showing top 20 rows\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_undirected_graph: (g: org.graphframes.GraphFrame)org.graphframes.GraphFrame\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_undirected_graph(g: GraphFrame) = {\n",
    "    val u_edge_df = g.find(\"(a)-[]->(b)\")\n",
    "        .select($\"a.id\".as(\"src\"),$\"b.id\".as(\"dst\"))\n",
    "        .withColumn(\"swap\",when(col(\"src\")<col(\"dst\"),col(\"dst\")))\n",
    "        .withColumn(\"dst\",\n",
    "                    when(col(\"swap\").isNotNull,col(\"src\"))\n",
    "                    .otherwise(col(\"dst\")))\n",
    "        .withColumn(\"src\",\n",
    "                    when(col(\"swap\").isNotNull,col(\"swap\"))\n",
    "                   .otherwise(col(\"src\")))\n",
    "        .drop(col(\"swap\"))\n",
    "        .distinct\n",
    "    val u_vertices_df = g.vertices\n",
    "    val u_g = GraphFrame(u_vertices_df,u_edge_df)    \n",
    "    u_g\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+\n",
      "|  id|            location|             coeff|\n",
      "+----+--------------------+------------------+\n",
      "| 153|   \"E 40 St & 5 Ave\"|               1.0|\n",
      "|3485| \"NYCBS Depot - RIS\"|               1.0|\n",
      "|3040|     \"GOW Tech Shop\"|               1.0|\n",
      "|3639|        \"Harborside\"|               1.0|\n",
      "|3192|\"Liberty Light Rail\"|               1.0|\n",
      "|3647|    \"48 Ave & 30 Pl\"|               1.0|\n",
      "|3279|       \"Dixon Mills\"|               1.0|\n",
      "|3186|     \"Grove St PATH\"|               1.0|\n",
      "| 339|\"Avenue D & E 12 St\"| 0.877201420748853|\n",
      "|3464|\"W 37 St & Broadway\"|0.8679573382796197|\n",
      "| 247|\"Perry St & Bleec...|0.8602079768329604|\n",
      "|3175|\"W 70 St & Amster...|0.8592469808193227|\n",
      "|3176|\"W 64 St & West E...|0.8568452539928423|\n",
      "|3623|\"W 120 St & Clare...|0.8549019607843137|\n",
      "|3491|  \"E 118 St & 1 Ave\"| 0.854122621564482|\n",
      "| 266| \"Avenue D & E 8 St\"| 0.849218980253463|\n",
      "|3441|   \"10 Hudson Yards\"|0.8482701509017299|\n",
      "|3646|    \"35 Ave & 10 St\"|0.8333333333333334|\n",
      "|3642|\"E 98 St & Lexing...|             0.832|\n",
      "| 444|\"Broadway & W 24 St\"|0.8283229697508064|\n",
      "+----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "triangles: org.apache.spark.sql.DataFrame = [count: bigint, t_id: string ... 1 more field]\n",
       "degrees: org.apache.spark.sql.DataFrame = [id: string, degree: int]\n",
       "possible: org.apache.spark.sql.DataFrame = [id: string, degree: int ... 1 more field]\n",
       "joined: org.apache.spark.sql.DataFrame = [t_id: string, count: bigint ... 4 more fields]\n",
       "coeff: org.apache.spark.sql.DataFrame = [t_id: string, count: bigint ... 5 more fields]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val triangles = g.triangleCount.run().withColumnRenamed(\"id\",\"t_id\") //Get the number of triangles each vertex belongs to\n",
    "val degrees = make_undirected_graph(g).degrees //Get the number of adjacent vertices for each vertex\n",
    "val possible = degrees.withColumn(\"possible\",col(\"degree\")*(col(\"degree\")-1)/lit(2)) //Calculate possible triangles\n",
    "val joined = triangles.select($\"t_id\",$\"count\",$\"attr\" as \"location\").join(possible,triangles(\"t_id\")===possible(\"id\"))\n",
    "val coeff = joined.withColumn(\"coeff\",col(\"count\")/col(\"possible\"))\n",
    "coeff.orderBy(col(\"coeff\").desc).select(\"id\",\"location\",\"coeff\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
