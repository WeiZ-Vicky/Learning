{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082730be",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">GraphX</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d32da7",
   "metadata": {},
   "source": [
    "<li>GraphX provides an RDD level implementation of graphs</li>\n",
    "<li>GraphFrames graph algorithm implementations are done in GraphX</li>\n",
    "<li>Building custom algorithms is easier using two GraphX building blocks but graphframes, since the work at the dataframe level, provide a higher level interface</li>\n",
    "<ul>\n",
    "    <li><span style=\"color:red\">aggregateMessages</span>: An implementation of an asynchronous message passing algorithm on a graph</li>\n",
    "    <li><span style=\"color:red\">pregel</span>: Google's parallel graph algorithm building block (parallel, graph, google)</li> \n",
    "</ul>\n",
    "<li>A GraphFrames graph is convertible to a GraphX graph, implement an algorithm, and convert the result back into a dataframe or a GraphFrame graph</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e6c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.packages= [\"graphframes:graphframes:0.8.2-spark3.2-s_2.12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d390f2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://dyn-160-39-133-139.dyn.columbia.edu:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1666042981864)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.graphframes._\n",
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//GraphFrame imports\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.graphframes._\n",
    "\n",
    "\n",
    "//GraphX imports\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572f6d0",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:large\">Creating a GraphFrame graph</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72fa7d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertexArray: Array[(Int, String, Int)] = Array((1,Alice,28), (2,Bob,27), (3,Charlie,65), (4,David,42), (5,Ed,55), (6,Fran,50), (7,Qing,27), (8,Sarika,78), (9,Olafson,17), (10,Birgit,33))\n",
       "edgeArray: Array[(Int, Int, Int)] = Array((2,1,7), (1,2,13), (2,4,2), (3,2,4), (3,6,3), (4,1,1), (5,2,2), (5,3,8), (5,6,3), (7,8,14), (7,9,2), (8,10,8), (9,10,6))\n",
       "vertex_df: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]\n",
       "edge_df: org.apache.spark.sql.DataFrame = [src: int, dst: int ... 1 more field]\n",
       "g: org.graphframes.GraphFrame = GraphFrame(v:[id: int, name: string ... 1 more field], e:[src: int, dst: int ... 1 more field])\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vertexArray = Array(\n",
    "  (1, \"Alice\", 28),\n",
    "  (2, \"Bob\", 27),\n",
    "  (3, \"Charlie\", 65),\n",
    "  (4, \"David\", 42),\n",
    "  (5, \"Ed\", 55),\n",
    "  (6, \"Fran\", 50),\n",
    "    (7, \"Qing\",27),\n",
    "    (8, \"Sarika\",78),\n",
    "    (9, \"Olafson\",17),\n",
    "    (10, \"Birgit\",33)\n",
    ")\n",
    "\n",
    "val edgeArray = Array(\n",
    "  (2, 1, 7),\n",
    "  (1, 2, 13),\n",
    "  (2, 4, 2),\n",
    "  (3, 2, 4),\n",
    "  (3, 6, 3),\n",
    "  (4, 1, 1),\n",
    "  (5, 2, 2),\n",
    "  (5, 3, 8),\n",
    "  (5, 6, 3),\n",
    "    (7, 8, 14),\n",
    "    (7, 9, 2),\n",
    "    (8, 10, 8),\n",
    "    (9, 10, 6)\n",
    ")\n",
    "\n",
    "val vertex_df = spark.createDataFrame(vertexArray).toDF(\"id\",\"name\",\"age\")\n",
    "val edge_df = spark.createDataFrame(edgeArray).toDF(\"src\",\"dst\",\"attr\")\n",
    "\n",
    "val g = GraphFrame(vertex_df, edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84054a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|src|dst|attr|\n",
      "+---+---+----+\n",
      "|  2|  1|   7|\n",
      "|  1|  2|  13|\n",
      "|  9| 10|   6|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.filterEdges(\"attr>2\").filterVertices(\"age < 50\").edges.show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace4db8",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Creating a GraphX graph</span>\n",
    "<br><br>\n",
    "<li>Vertex ids in GraphX must be of type Long (convert ids to Long)</li>\n",
    "<li>Vertex attributes must be a single object (convert attributes to a tuple)</li>\n",
    "<li>Edge objects must be of type GraphX.Edge (convert the edgeArray tuples into Edge objects while also converting vertex ids to Long</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a9ab5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertexArray: Array[(Int, String, Int)] = Array((1,Alice,28), (2,Bob,27), (3,Charlie,65), (4,David,42), (5,Ed,55), (6,Fran,50), (7,Qing,27), (8,Sarika,78), (9,Olafson,17), (10,Birgit,33))\n",
       "edgeArray: Array[(Int, Int, Int)] = Array((2,1,7), (1,2,13), (2,4,2), (3,2,4), (3,6,3), (4,1,1), (5,2,2), (5,3,8), (5,6,3), (7,8,14), (7,9,2), (8,10,8), (9,10,6))\n",
       "vertexArrayX: Array[(Long, (String, Int))] = Array((1,(Alice,28)), (2,(Bob,27)), (3,(Charlie,65)), (4,(David,42)), (5,(Ed,55)), (6,(Fran,50)), (7,(Qing,27)), (8,(Sarika,78)), (9,(Olafson,17)), (10,(Birgit,33)))\n",
       "edgeArrayX: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(2,1,7), Edge(1,2,13), Edge(2,4,2), Edge(3,2,4), Edge(3,6,3), Edge(4,1,1), Edge(5,2,2), Edge(5,3,8), Edge(5,6,3), Edge(7,8,14), Edge(7,9,2), Edge(8,10,8), Edge(9,10,6))\n",
       "ve...\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vertexArray = Array(\n",
    "  (1, \"Alice\", 28),\n",
    "  (2, \"Bob\", 27),\n",
    "  (3, \"Charlie\", 65),\n",
    "  (4, \"David\", 42),\n",
    "  (5, \"Ed\", 55),\n",
    "  (6, \"Fran\", 50),\n",
    "    (7, \"Qing\",27),\n",
    "    (8, \"Sarika\",78),\n",
    "    (9, \"Olafson\",17),\n",
    "    (10, \"Birgit\",33)\n",
    ")\n",
    "\n",
    "val edgeArray = Array(\n",
    "  (2, 1, 7),\n",
    "  (1, 2, 13),\n",
    "  (2, 4, 2),\n",
    "  (3, 2, 4),\n",
    "  (3, 6, 3),\n",
    "  (4, 1, 1),\n",
    "  (5, 2, 2),\n",
    "  (5, 3, 8),\n",
    "  (5, 6, 3),\n",
    "    (7, 8, 14),\n",
    "    (7, 9, 2),\n",
    "    (8, 10, 8),\n",
    "    (9, 10, 6)\n",
    ")\n",
    "val vertexArrayX = vertexArray.map(r => (r._1.toLong,(r._2,r._3)))\n",
    "val edgeArrayX = edgeArray.map(r => Edge(r._1.toLong,r._2.toLong,r._3))\n",
    "\n",
    "val vertexRDD: RDD[(Long, (String, Int))] = sc.parallelize(vertexArrayX)\n",
    "val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArrayX)\n",
    "\n",
    "val graph: Graph[(String, Int), Int] = Graph(vertexRDD, edgeRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a194134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.graphx.Graph[(String, Int),Int] = org.apache.spark.graphx.impl.GraphImpl@61c0ca35\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12061ec",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Convert from GraphFrame to GraphX</span>\n",
    "<br><br>\n",
    "<li>Method 1: Call the function toGraphX</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ef2515d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gx: org.apache.spark.graphx.Graph[org.apache.spark.sql.Row,org.apache.spark.sql.Row] = org.apache.spark.graphx.impl.GraphImpl@1c11965b\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gx: Graph[Row, Row] = g.toGraphX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c653835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.sql.Row)] = Array((10,[10,Birgit,33]), (1,[1,Alice,28]), (2,[2,Bob,27]), (3,[3,Charlie,65]), (4,[4,David,42]), (5,[5,Ed,55]), (6,[6,Fran,50]), (7,[7,Qing,27]), (8,[8,Sarika,78]), (9,[9,Olafson,17]))\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gx.vertices.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782ae18d",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Convert from GraphFrame to GraphX</span>\n",
    "<br><br>\n",
    "<li>Method 2: By unpacking row objects and then creating a GraphX object</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b574f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: org.apache.spark.rdd.RDD[(Long, (String, Int))] = MapPartitionsRDD[79] at map at <console>:39\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.vertices.rdd.map(r => (r(0).toString.toLong,(r(1).toString,r(2).toString.toInt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c104d15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Long = 1\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.vertices.rdd.first()(0).toString.toLong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af66054d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: (String, Int) = (Alice,28)\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(g.vertices.rdd.first()(1).toString,g.vertices.rdd.first()(2).toString.toInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be805abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v: org.apache.spark.rdd.RDD[(Long, (String, Int))] = MapPartitionsRDD[80] at map at <console>:37\n",
       "e: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[86] at map at <console>:38\n",
       "gx: org.apache.spark.graphx.Graph[(String, Int),Int] = org.apache.spark.graphx.impl.GraphImpl@539facf8\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val v = g.vertices.rdd.map(r => (r(0).toString.toLong,(r(1).toString,r(2).toString.toInt)))\n",
    "val e = g.edges.rdd.map(r => Edge(r(0).toString.toLong,r(1).toString.toLong,r(2).toString.toInt))\n",
    "val gx: Graph[(String, Int), Int] = Graph(v, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bbc2ad",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Convert from GraphX to GraphFrame</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58d799d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.graphx.Graph\n",
       "import org.apache.spark.sql.Row\n",
       "g2: org.graphframes.GraphFrame = GraphFrame(v:[id: bigint, attr: struct<_1: string, _2: int>], e:[src: bigint, dst: bigint ... 1 more field])\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx.Graph\n",
    "import org.apache.spark.sql.Row\n",
    "val g2: GraphFrame = GraphFrame.fromGraphX(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28f0c3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- attr: struct (nullable = true)\n",
      " |    |-- _1: string (nullable = true)\n",
      " |    |-- _2: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g2.vertices.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db39913",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Convert from GraphX to GraphFrame</span>\n",
    "<br><br>\n",
    "<li>Method 2: With schema</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d4be7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Array[(org.apache.spark.graphx.VertexId, (String, Int))] = Array((10,(Birgit,33)), (1,(Alice,28)), (2,(Bob,27)), (3,(Charlie,65)), (4,(David,42)), (5,(Ed,55)), (6,(Fran,50)), (7,(Qing,27)), (8,(Sarika,78)), (9,(Olafson,17)))\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gx.vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af5d350c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: org.apache.spark.rdd.RDD[(Int, String, Int)] = MapPartitionsRDD[99] at map at <console>:41\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gx.vertices.map(a => (a._1.toInt,a._2._1,a._2._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c0d2b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v1: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]\n",
       "e1: org.apache.spark.sql.DataFrame = [src: int, dst: int ... 1 more field]\n",
       "g: org.graphframes.GraphFrame = GraphFrame(v:[id: int, name: string ... 1 more field], e:[src: int, dst: int ... 1 more field])\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val v1 = gx.vertices.map(a => (a._1.toInt,a._2._1,a._2._2)).toDF(\"id\",\"name\",\"age\")\n",
    "val e1 = gx.edges.map(e => (e.srcId.toInt,e.dstId.toInt,e.attr)).toDF(\"src\",\"dst\",\"attr\")\n",
    "val g = GraphFrame(vertex_df, edge_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5169d650",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Algorithm building blocks</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9f52a",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">aggregateMessages</span>\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd7499",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Calculate the total incoming “likes” on each vertex</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adb2e0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_incoming_likes: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[109] at RDD at VertexRDD.scala:57\n",
       "res13: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((10,14), (1,8), (2,19), (3,8), (4,2), (6,6), (8,14), (9,2))\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val total_incoming_likes = gx.aggregateMessages[Int](ec => ec.sendToDst(ec.attr),(x,y) => x+y)\n",
    "total_incoming_likes.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de76344e",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">try this: total outgoing likes for each person</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65e34609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_outgoing_likes: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[113] at RDD at VertexRDD.scala:57\n",
       "res14: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((1,13), (2,9), (3,7), (4,1), (5,13), (7,16), (8,8), (9,6))\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val total_outgoing_likes = gx.aggregateMessages[Int](ec => ec.sendToSrc(ec.attr),(x,y) => x+y)\n",
    "total_outgoing_likes.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7870758",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">for each person, who likes them the most and how much?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30ea0b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,(Qing,14))\n",
      "(9,(Qing,2))\n",
      "(1,(Bob,7))\n",
      "(2,(Alice,13))\n",
      "(4,(Bob,2))\n",
      "(3,(Ed,8))\n",
      "(10,(Sarika,8))\n",
      "(6,(Ed,3))\n"
     ]
    }
   ],
   "source": [
    "gx.aggregateMessages[(String,Int)](ec => ec.sendToDst((ec.srcAttr._1,ec.attr)),(x,y) => if (x._2 > y._2) x else y).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1206423",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68850b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02185fc1",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">try this: return the age of the oldest person who likes each user</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f6c185a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((10,78), (1,42), (2,65), (3,55), (4,27), (6,65), (8,27), (9,27))\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gx.aggregateMessages[Int](ec => ec.sendToDst((ec.srcAttr._2)),(x,y)=>Math.max(x,y)).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062fd60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b84108d",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Pregel</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b391200",
   "metadata": {},
   "source": [
    "<li>Pregel works by sending messages along the edges of the graph in parallel</li>\n",
    "<li>the messages are then used to compute \"the state\" of a node</li>\n",
    "<li>Roughly:</li>\n",
    "<ul>\n",
    "    <li>pregel is applied in multiple iterations known as supersteps</li>\n",
    "    <li>at each iteration, vertices send messages to adjacent vertices</li>\n",
    "    <li>at each iteration, vertices update their state by processing messages received in the previous superstep</li>\n",
    "    <li>the algorithm terminates when it converges or after a fixed number of steps\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0534a661",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Calculate the shortest path from a given vertex to every other vertex</span>\n",
    "\n",
    "<li>pregel is run on a copy of the graph</li>\n",
    "<li>Since pregel processes messages in vertices, the vertex attribute of the copy contains the statistic being calculated</li>\n",
    "<li>Edge attributes are copied from the original graph</li>\n",
    "<li>pregel takes three function arguments:\n",
    "    <ul>\n",
    "        <li>a vertex program: </li>\n",
    "        <li>a send message program: </li>\n",
    "        <li>a merge message program: </li>\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "170082c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sourceId: org.apache.spark.graphx.VertexId = 3\n",
       "initialGraph: org.apache.spark.graphx.Graph[Double,Int] = org.apache.spark.graphx.impl.GraphImpl@45e5edef\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sourceId: VertexId = 3\n",
    "val initialGraph = gx.mapVertices((id, _) =>\n",
    "    if (id == sourceId) 0.0 else Double.PositiveInfinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f0348de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(2,1,7), Edge(1,2,13), Edge(2,4,2), Edge(3,2,4), Edge(3,6,3), Edge(4,1,1), Edge(5,2,2), Edge(5,3,8), Edge(5,6,3), Edge(7,8,14), Edge(7,9,2), Edge(8,10,8), Edge(9,10,6))\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialGraph.edges.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d311f46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: Array[(org.apache.spark.graphx.VertexId, Double)] = Array((10,Infinity), (1,Infinity), (2,Infinity), (3,0.0), (4,Infinity), (5,Infinity), (6,Infinity), (7,Infinity), (8,Infinity), (9,Infinity))\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialGraph.vertices.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb4a52",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Vertex program</span>\n",
    "<li>At each vertex, when a new distance arrives, replace the current shortest distance by the lesser of the current distance and the new distance</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eba8f049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertex_program: (org.apache.spark.graphx.VertexId, Double, Double) => Double = $Lambda$5618/0x0000000801e3a440@78ef7ba9\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vertex_program = (id: VertexId, dist: Double, newDist: Double) => math.min(dist, newDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e0312",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">The message</span>\n",
    "\n",
    "\n",
    "<li>add the current shortest path at the source of each triplet to the distance to the destination</li>\n",
    "<li>if this sum is less than the current shortest path at destination, send a message to the destination with this sum\n",
    "<li>otherwise don't send a message\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0cf3a0",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Triplets</span>\n",
    "<li>A triplet is a combination of (source vertex data, destination vertex dat, aedge data)</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44fa4df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: Array[org.apache.spark.graphx.EdgeTriplet[(String, Int),Int]] = Array(((2,(Bob,27)),(1,(Alice,28)),7), ((1,(Alice,28)),(2,(Bob,27)),13), ((2,(Bob,27)),(4,(David,42)),2), ((3,(Charlie,65)),(2,(Bob,27)),4), ((3,(Charlie,65)),(6,(Fran,50)),3), ((4,(David,42)),(1,(Alice,28)),1), ((5,(Ed,55)),(2,(Bob,27)),2), ((5,(Ed,55)),(3,(Charlie,65)),8), ((5,(Ed,55)),(6,(Fran,50)),3), ((7,(Qing,27)),(8,(Sarika,78)),14), ((7,(Qing,27)),(9,(Olafson,17)),2), ((8,(Sarika,78)),(10,(Birgit,33)),8), ((9,(Olafson,17)),(10,(Birgit,33)),6))\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gx.triplets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4eabb56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res25: Array[org.apache.spark.graphx.EdgeTriplet[Double,Int]] = Array(((2,Infinity),(1,Infinity),7), ((1,Infinity),(2,Infinity),13), ((2,Infinity),(4,Infinity),2), ((3,0.0),(2,Infinity),4), ((3,0.0),(6,Infinity),3), ((4,Infinity),(1,Infinity),1), ((5,Infinity),(2,Infinity),2), ((5,Infinity),(3,0.0),8), ((5,Infinity),(6,Infinity),3), ((7,Infinity),(8,Infinity),14), ((7,Infinity),(9,Infinity),2), ((8,Infinity),(10,Infinity),8), ((9,Infinity),(10,Infinity),6))\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialGraph.triplets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ed888a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sendMsg: org.apache.spark.graphx.EdgeTriplet[Double,Int] => Iterator[(org.apache.spark.graphx.VertexId, Double)] = $Lambda$5656/0x0000000801e5e040@2f907156\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sendMsg = (triplet: EdgeTriplet[Double,Int]) => { \n",
    "    if (triplet.srcAttr + triplet.attr < triplet.dstAttr) {\n",
    "      Iterator((triplet.dstId, triplet.srcAttr + triplet.attr))\n",
    "    } else {\n",
    "      Iterator.empty\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07437d35",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">merge messages</span>\n",
    "<li>When multiple messages arrive, choose the one with the lowest shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5092d4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mrgMsg: (Double, Double) => Double = $Lambda$5657/0x0000000801e5f840@3da78d85\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mrgMsg = (a: Double, b: Double) => math.min(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc815cc",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Run pregel</span>\n",
    "<li>pregel(configs)(funcs)</li>\n",
    "<li>configs = initial_msg, maximum number of iterations, and the edge direction in which to send messages)</li>\n",
    "<li>funcs = (vertex_program,sendMsg,mrgMsg)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c07830a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sssp: org.apache.spark.graphx.Graph[Double,Int] = org.apache.spark.graphx.impl.GraphImpl@2a3af5a\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sssp = initialGraph.pregel(Double.PositiveInfinity)(\n",
    "  vertex_program,\n",
    "    sendMsg,\n",
    "    mrgMsg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75a2cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,Infinity)\n",
      "(1,7.0)\n",
      "(2,4.0)\n",
      "(3,0.0)\n",
      "(4,6.0)\n",
      "(5,Infinity)\n",
      "(6,3.0)\n",
      "(7,Infinity)\n",
      "(8,Infinity)\n",
      "(9,Infinity)\n"
     ]
    }
   ],
   "source": [
    "println(sssp.vertices.collect.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092580b2",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Putting it all together</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7454dd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,Infinity)\n",
      "(1,7.0)\n",
      "(2,4.0)\n",
      "(3,0.0)\n",
      "(4,6.0)\n",
      "(5,Infinity)\n",
      "(6,3.0)\n",
      "(7,Infinity)\n",
      "(8,Infinity)\n",
      "(9,Infinity)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sourceId: org.apache.spark.graphx.VertexId = 3\n",
       "initialGraph: org.apache.spark.graphx.Graph[Double,Int] = org.apache.spark.graphx.impl.GraphImpl@490cb996\n",
       "sssp: org.apache.spark.graphx.Graph[Double,Int] = org.apache.spark.graphx.impl.GraphImpl@59d776d1\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 20:33:03 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 969338 ms exceeds timeout 120000 ms\n",
      "22/10/17 20:33:03 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "val sourceId: VertexId = 3\n",
    "val initialGraph = graph.mapVertices((id, _) =>\n",
    "    if (id == sourceId) 0.0 else Double.PositiveInfinity)\n",
    "val sssp = initialGraph.pregel(Double.PositiveInfinity)(\n",
    "  vertex_program,\n",
    "    sendMsg,\n",
    "    mrgMsg)\n",
    "println(sssp.vertices.collect.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d9ad0",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Walk through</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691be39",
   "metadata": {},
   "source": [
    "<img src=\"initialGraph.png\"><br>\n",
    "<img src=\"pass1.png\"><br>\n",
    "<img src=\"pass2.png\"><br>\n",
    "<img src=\"pass3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46a7cf",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:50px\">Partitioning</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e423a",
   "metadata": {},
   "source": [
    "<li>Partitioning graph data is complicated because vertices share edges</li>\n",
    "<li>Graph partitioning can only be done using GraphX. GraphFrames does not provide partitioning support</li>\n",
    "<li>Partitioning strategies:</li>\n",
    "<ul>\n",
    "    <li><span style=\"color:red\">Vertex cut</span>: The graph is partitioned by edges. A vertex can end up in multiple partitions if one of its edges is in one partition and another in a different partition</li>\n",
    "    <li><span style=\"color:red\">Edge cut</span>: The graph is partitioned by vertices. Edges are split into two if their vertices end up in different partitions and a \"ghost\" of the missing vertex is added to the partition</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d6c60",
   "metadata": {},
   "source": [
    "<li>GraphX uses <span style=\"color:blue\">vertex cut</span> partitioning strategies. The graph is partitioned by edges and vertices can span multiple partitions</li>\n",
    "<ul>\n",
    "    <li><span style=\"color:blue\">EdgePartition1D</span>: Edges are partitioned by hashing the srcId of the edge. All edges from a vertex will end up in the same partition </li>\n",
    "    <li><span style=\"color:blue\">EdgePartition2D</span>: An extension of 1D too complicated to explain. The main focus is in constraining the number of partitions a vertex can be in </li>\n",
    "    <li><span style=\"color:blue\">RandomVertexCut</span>:  Edges are randomly distributed across partitions by hashing (srcId, dstId) of each edge. Any vertex could exist in multiple partitions but, because of hashing, multiple same direction edges between two vertices will be in the same partition</li>\n",
    "    <li><span style=\"color:blue\">CanonicalRandomVertexCut</span>:  Same as RandomVertexCut with the exception that the (srcId, dstId) pairs are ordered <span style=\"color:red\">before</span> hashing. All edges between two vertices, regardless of direction, will end up in the same partition</li>\n",
    "    <li>The choice of partitioning strategy depends on what you want to do with the data</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96807b9e",
   "metadata": {},
   "source": [
    "<li>A graph contains multiple edges in both directions between pairs of vertices. If you want to calculate the total outflow from a vertex, which strategy would you use?</li>\n",
    "<li>A graph contains multiple edges in both directions between pairs of vertices. If you want to calculate the total traffic that goes from vertex i to vertex j, which strategy would you use?</li>\n",
    "<li>A graph contains multiple edges in both directions between pairs of vertices. If you want to calculate the total flow (sum of bi-directional flows) between two vertices, which strategy would you use?</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ef45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.partitionBy(PartitionStrategy.RandomVertexCut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4dcbf1",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">GraphFrames partitioning</span>\n",
    "<li>Convert the graph into a GraphX graph, partition, convert back into GraphFrames</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994a33d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
