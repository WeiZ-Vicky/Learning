{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">DStream transformations</h1>\n",
    "<br><br>\n",
    "<li><b>map</b>: map, what else!</li>\n",
    "<li><b>flatMap</b>: flatMap, what else!</li>\n",
    "<li><b>filter</b>: filter</li>\n",
    "<li><b>repartition</b>: changes the number of partitions (increase or decrease) for the DStream</li>\n",
    "<li><b>count</b>: the number of elements in the RDD of the source dstream</li>\n",
    "<li><b>countByValue</b>: computes the frequency of each key and returns a DStream of (key,count) pairs</li>\n",
    "<li><b>union</b>: union of two DStreams</li>\n",
    "<li><b>reduceByKey</b>: reduceByKey</li>\n",
    "<li><b>updateStateByKey</b>: applies a function to a DStream to update values for a given key</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">DStream union</span>\n",
    "<br><br>\n",
    "<li>Returns the union of two DStream objects</li>\n",
    "<li>Use this to combine data arriving from two different streams</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.log4j.Logger\n",
    "import org.apache.log4j.Level\n",
    "\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "Logger.getLogger(\"akka\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "\n",
    "val ssc = new StreamingContext(sc,Seconds(10.toLong))\n",
    "val lines1 = ssc.socketTextStream(\"localhost\", 4444)\n",
    "val lines2 = ssc.socketTextStream(\"localhost\",9999)\n",
    "val lines3 = lines1.union(lines2)\n",
    "val words = lines3.flatMap(line => line.split(\" \"))\n",
    "val pairs = words.map(word => (word, 1))\n",
    "val wordCounts = pairs.reduceByKey((x, y) => x + y)\n",
    "wordCounts.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Checkpointing</span>\n",
    "<li>A stream runs continuously for a very long time and must be resilient to failures</li>\n",
    "<li>checkpointing is a mechanism for recovering from failures</li>\n",
    "<li>After failure, a stream can be initialized from an existing checkpoint</li>\n",
    "<li>A checkpoint must be created if a <span style=\"color:blue\">stateful</span> transformation is being used</li>\n",
    "<li>Checkpointing, in a stateful transaction, computes the value of an RDD and saves it. Recovery then starts from this value</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Stateful transformations and checkpointing</span>\n",
    "<li>The value of stateful results (e.g., averages/sums) at a point in time depends on all the prior RDDs</li>\n",
    "<li>Spark requires that stateful transformations be checkpointed</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Setting the checkpoint</span>\n",
    "<li>Specify the location where checkpoints will be saved</li>\n",
    "<li>On HDFS, the location will be distributed and fault tolerant</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.checkpoint(\"checkpoint\") //checkpoint is the directory where checkpoint data will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">DStream updateStateByKey</span>\n",
    "<li>updateStateByKey maintains state information that is updated by each batch\n",
    "<li>Need to define a state (runningCount in example below)\n",
    "<li>And define a state update function (updateFunction in example below)\n",
    "<li>Note the ByKey part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Set up the streaming context</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc, Seconds(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Create a function that updates the state</span>\n",
    "<li>We need to keep a running total for each key</li>\n",
    "<li>So that's what we will update</li>\n",
    "<li>We need to initialize values for each key's running total so we'll use Option[Int] for that (either it exists or it doesn't)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Define an update function</span>\n",
    "<li> Simple function. Set value to 0 if it doesn't already exist\n",
    "<li> then add in the new value and return the updated value\n",
    "<li> This function will be applied to updateStateByKey\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val updateFunction = (nv: Seq[Int], rc: Option[Int]) => { //nv = new value; rc = running count\n",
    "    val uc = rc.getOrElse(0) + nv.sum  //If rc does not exist, set it to 0, otherwise use the existing value\n",
    "    Some(uc) //return Some(uc) Why Some?   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updateFunction(Seq(5,7),Some(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updateFunction(Seq(5,7),None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Do the transformations (adding updateStateByKey)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc, Seconds(20))\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "\n",
    "val words = lines.flatMap(line => line.split(\" \"))\n",
    "val pairs = words.map(word => (word, 1))\n",
    "\n",
    "//val runningCount = pairs.updateStateByKey[Int]((a: Seq[Int],b: Option[Int]) => Some(b.getOrElse(0) + a.sum))\n",
    "val runningCount = pairs.updateStateByKey[Int](updateFunction)\n",
    "runningCount.print()\n",
    "ssc.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">DStream transform</span>\n",
    "<li>The <b>transform</b> function on a DStream returns a new DStream by applying a function on each rdd in the DStream</li>\n",
    "<li>It is particularly useful for joining a static RDD to the RDDs in a DStream</li>\n",
    "<li>Example: Individual sales of products are coming in on a stream. Calculate revenue on each sale by multiplying by prices in a static RDD</li>\n",
    "<pre>\n",
    "Example Sales\n",
    "A,7\n",
    "B,2\n",
    "A,11\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "\n",
    "val ssc = new StreamingContext(sc,Seconds(10.toLong))\n",
    "val prices = Array((\"A\",10.2),(\"B\",7.4))\n",
    "val pricesRDD = sc.parallelize(prices)\n",
    "\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "val sales = lines.map(line=>line.split(\",\"))\n",
    "        .map(l=>(l(0),l(1).toInt))\n",
    "val salesPrices = sales.transform(rdd => rdd.join(pricesRDD))\n",
    "val revenue = salesPrices.map(t => (t._1,t._2._1*t._2._2))\n",
    "revenue.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
