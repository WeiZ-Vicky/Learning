{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7882caad",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Spark ML</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff8f50",
   "metadata": {},
   "source": [
    "\n",
    "<li>provides a uniform API for building and tuning ML models</li>\n",
    "<li>provides libraries for feature extraction and transformation</li>\n",
    "<li>provides support for <b>ML pipelines</b></li>\n",
    "<li>provides support for linear algebra, statistics, scaling, etc.</li>\n",
    "<li>Mostly works with Spark Dataframes</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d37be",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Why Spark ML?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c4e16",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Machine learning at scale</span> Spark's ML library and its data analytic models can support ML operations with billions of observations</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b0d6b",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Spark ML Data Structures</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f329a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Vectors</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a041ea1e",
   "metadata": {},
   "source": [
    "<li><a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/ml/linalg/Vectors.html\">https://spark.apache.org/docs/latest/api/java/org/apache/spark/ml/linalg/Vectors.html</a></li>\n",
    "<li><span style=\"color:blue\">dense vectors</span>: Ordinary vectors. Each index has a data value associated with it</li>\n",
    "<li><span style=\"color:blue\">sparse vectors</span>: Only actual elements are stored. Specify indices and values. Unspecified locations are 0.0. Arguments: number of elements, (index,value) pairs for non-zero elements</li>\n",
    "<li>Which one should you use? Depends on the data (sparse data, sparse vectors) and the algorithm (some algorithms - e.g., naive bayes, work better with dense vectors than with sparse vectors</li>  \n",
    "<li>Vectors contain two useful functions <span style=\"color:blue\">norm</span> and <span style=\"color:blue\">squared_distance</span></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcdbc007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.Vectors\n",
       "x: Array[Double] = Array(3.2, 0.0, 0.0, 0.0, 4.7, 1.6, 0.0, 0.0, 0.0, 0.0, 10.2, 0.0, 0.0, 11.1)\n",
       "data_dense: org.apache.spark.ml.linalg.Vector = [3.2,0.0,0.0,0.0,4.7,1.6,0.0,0.0,0.0,0.0,10.2,0.0,0.0,11.1]\n",
       "data_sparse: org.apache.spark.ml.linalg.SparseVector = (14,[0,4,5,10,13],[3.2,4.7,1.6,10.2,11.1])\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Vectors\n",
    "val x = Array(3.2,0,0,0,4.7,1.6,0,0,0,0,10.2,0,0,11.1)\n",
    "val data_dense = Vectors.dense(x) \n",
    "val data_sparse = data_dense.toSparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a76faeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.ml.linalg.Vector = [3.2,0.0,0.0,0.0,4.7,1.6,0.0,0.0,0.0,0.0,10.2,0.0,0.0,11.1]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a4faf10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.ml.linalg.SparseVector = (14,[0,4,5,10,13],[3.2,4.7,1.6,10.2,11.1])\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse //14 elements, nonzero location, value of nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6eb2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_dense: org.apache.spark.ml.linalg.Vector = [3.2,0.0,0.0,0.0,4.7,1.6,0.0,0.0,0.0,0.0,10.2,0.0,0.0,11.1]\n",
       "data_sparse: org.apache.spark.ml.linalg.Vector = (14,[0,4,5,10,13],[3.2,4.7,1.6,10.2,11.1])\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data_dense = Vectors.dense(3.2,0,0,0,4.7,1.6,0,0,0,0,10.2,0,0,11.1) \n",
    "val data_sparse = Vectors.sparse(14,Array(0,4,5,10,13),Array(3.2,4.7,1.6,10.2,11.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "857358a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.799999999999997\n",
      "16.190738093119784\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "println(Vectors.norm(data_dense,1)) //returns the p=1 norm (taxicab)\n",
    "println(Vectors.norm(data_dense,2)) //returns the euclidean norm\n",
    "println(Vectors.sqdist(data_dense,data_sparse)) //distance between two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bb61c7",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Matrix</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5198732a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.SparseMatrix\n",
       "import org.apache.spark.ml.linalg.Matrices\n",
       "data: Array[Double] = Array(1.0, 0.0, 4.0, 0.0, 3.0, 5.0, 2.0, 0.0, 6.0)\n",
       "dense_m: org.apache.spark.ml.linalg.Matrix =\n",
       "1.0  0.0  2.0\n",
       "0.0  3.0  0.0\n",
       "4.0  5.0  6.0\n",
       "sparse_m: org.apache.spark.ml.linalg.SparseMatrix =\n",
       "3 x 3 CSCMatrix\n",
       "(0,0) 1.0\n",
       "(2,0) 4.0\n",
       "(1,1) 3.0\n",
       "(2,1) 5.0\n",
       "(0,2) 2.0\n",
       "(2,2) 6.0\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.SparseMatrix\n",
    "import org.apache.spark.ml.linalg.Matrices\n",
    "\n",
    "\n",
    "val data = Array(1.0, 0.0, 4.0, 0.0, 3.0, 5.0, 2.0, 0.0, 6.0)\n",
    "val dense_m = Matrices.dense(3,3,data)\n",
    "val sparse_m = dense_m.toSparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a6af96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.ml.linalg.SparseMatrix =\n",
       "3 x 3 CSCMatrix\n",
       "(0,0) 1.0\n",
       "(2,0) 4.0\n",
       "(1,1) 3.0\n",
       "(2,1) 5.0\n",
       "(0,2) 2.0\n",
       "(2,2) 6.0\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec4b9ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.ml.linalg.Matrix =\n",
       "1.0  0.0  4.0\n",
       "0.0  3.0  5.0\n",
       "2.0  0.0  6.0\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_m.transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36f8061f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.ml.linalg.SparseMatrix =\n",
       "3 x 3 CSCMatrix\n",
       "(0,0) 1.0\n",
       "(2,0) 2.0\n",
       "(1,1) 3.0\n",
       "(0,2) 4.0\n",
       "(1,2) 5.0\n",
       "(2,2) 6.0\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_m.transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10eeff",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<h2 style=\"color:red;font-size:50px\">feature transformers</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0c53c",
   "metadata": {},
   "source": [
    "<li>Spark contains an extensive library of feature transformers</li>\n",
    "<li>We'll take a quick look at a few here</li>\n",
    "<li><a href=\"https://spark.apache.org/docs/latest/ml-features.html\">https://spark.apache.org/docs/latest/ml-features.html</a></li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d27e8",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Vector Assembler</span>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc3766",
   "metadata": {},
   "source": [
    "<li>Combines a set of columns into a single <b>sparse</b> vector</li>\n",
    "<li>In supervised learning, the independent features are combined into a single vector</li>\n",
    "<li>As a result, each case is represented by a pair (dv,iv-vector)</li>\n",
    "<li><a href=\"https://spark.apache.org/docs/latest/ml-features#vectorassembler\">https://spark.apache.org/docs/latest/ml-features#vectorassembler</a></li>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "390f6e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---+\n",
      "|feature1|feature2| dv|\n",
      "+--------+--------+---+\n",
      "|    22.0|    23.1|  3|\n",
      "|    12.2|    13.0|  2|\n",
      "|    43.7|    16.2|  4|\n",
      "|    36.4|    34.8|  3|\n",
      "|     6.1|    71.0|  3|\n",
      "|    28.2|    22.1|  7|\n",
      "+--------+--------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "df: org.apache.spark.sql.DataFrame = [feature1: double, feature2: double ... 1 more field]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val df = spark.createDataFrame(Seq(\n",
    "  (22.0, 23.1,3),\n",
    "  (12.2, 13.0,2),\n",
    "  (43.7, 16.2,4),\n",
    "  (36.4, 34.8,3),\n",
    "  (6.1, 71.0,3),\n",
    "  (28.2, 22.1,7)\n",
    ")).toDF(\"feature1\", \"feature2\",\"dv\")\n",
    "\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b83ba",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Create an assembler object identifying the columns that need to be vectorized</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9a0448d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_037dcb832033, handleInvalid=error, numInputCols=2\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"feature1\",\"feature2\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ac1a2",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Call transform on the dataframe</span>\n",
    "<br>\n",
    "<li>This creates a new dataframe, vectorizing the input columns, and storing the vector in the output column</li>\n",
    "<li>by default, spark ml models assume the dv is in a column called label and the iv in a column called features</li>\n",
    "<li>In the example below, we select the two columns of interest and rename the dv as \"label\"</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85b6e51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|dv |features   |\n",
      "+---+-----------+\n",
      "|3  |[22.0,23.1]|\n",
      "|2  |[12.2,13.0]|\n",
      "|4  |[43.7,16.2]|\n",
      "|3  |[36.4,34.8]|\n",
      "|3  |[6.1,71.0] |\n",
      "|7  |[28.2,22.1]|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler.transform(df)\n",
    ".select(\"dv\",\"features\")\n",
    ".show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c858d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df_lr = assembler.transform(df)\n",
    "    .select(\"dv\",\"features\")\n",
    "    .withColumnRenamed(\"dv\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e19bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b8967",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler.transform(df).show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56235a65",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Putting it all together</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1d9bcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|label|   features|\n",
      "+-----+-----------+\n",
      "|    3|[22.0,23.1]|\n",
      "|    2|[12.2,13.0]|\n",
      "|    4|[43.7,16.2]|\n",
      "|    3|[36.4,34.8]|\n",
      "|    3| [6.1,71.0]|\n",
      "|    7|[28.2,22.1]|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "df: org.apache.spark.sql.DataFrame = [feature1: double, feature2: double ... 1 more field]\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_fc11181aad9a, handleInvalid=error, numInputCols=2\n",
       "df_lr: org.apache.spark.sql.DataFrame = [label: int, features: vector]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val df = spark.createDataFrame(Seq(\n",
    "  (22.0, 23.1,3),\n",
    "  (12.2, 13.0,2),\n",
    "  (43.7, 16.2,4),\n",
    "  (36.4, 34.8,3),\n",
    "  (6.1, 71.0,3),\n",
    "  (28.2, 22.1,7)\n",
    ")).toDF(\"feature1\", \"feature2\",\"dv\")\n",
    "\n",
    "//Create an assembler object identifying the columns that need to be vectorized\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"feature1\",\"feature2\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "//Call transform on the dataframe. This creates a new dataframe using the specifications\n",
    "//(specs = which columns to keep)\n",
    "//by default, spark ml models assume the dv is in a column called label and the iv in a column called features\n",
    "val df_lr = assembler.transform(df)\n",
    "    .select(\"dv\",\"features\")\n",
    "    .withColumnRenamed(\"dv\",\"label\")\n",
    "\n",
    "df_lr.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f0c17",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">String indexer</span>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a1a4f",
   "metadata": {},
   "source": [
    "\n",
    "<li>ML algorithms need numbers!</li>\n",
    "<li>Any string variables need to be converted into numbers before they can be used</li>\n",
    "<li><a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/ml/feature/StringIndexer.html\">StringIndexer</a> is a spark feature transofrmer that does this</li>\n",
    "<li>The most frequent category is given the value 1, second most 2, etc.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d819f80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---+\n",
      "|feature1|feature2| dv|\n",
      "+--------+--------+---+\n",
      "|     MIA|    17.2|  2|\n",
      "|     NYC|    23.1|  3|\n",
      "|     SFO|    13.0|  2|\n",
      "|     NYC|    16.2|  4|\n",
      "|     CHI|    34.8|  3|\n",
      "|     SFO|    71.0|  3|\n",
      "|     LAX|    22.1|  7|\n",
      "+--------+--------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "df: org.apache.spark.sql.DataFrame = [feature1: string, feature2: double ... 1 more field]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val df = spark.createDataFrame(Seq(\n",
    "  (\"MIA\", 17.2,2),  \n",
    "  (\"NYC\", 23.1,3),\n",
    "  (\"SFO\", 13.0,2),\n",
    "  (\"NYC\", 16.2,4),\n",
    "  (\"CHI\", 34.8,3),\n",
    "  (\"SFO\", 71.0,3),\n",
    "  (\"LAX\", 22.1,7)\n",
    ")).toDF(\"feature1\", \"feature2\",\"dv\")\n",
    "\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7effaaa6",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Create a StringIndexer object</span>\n",
    "<br>\n",
    "<li>Note the similarity with VectorAssembler</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c33abf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_823ea4a5c0b1\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexer = new StringIndexer()\n",
    "  .setInputCol(\"feature1\")\n",
    "  .setOutputCol(\"feature1Index\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf73673",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Fit the StringIndexer to the data</span>\n",
    "<br>\n",
    "<li><span style=\"color:red\">fit</span> parameterizes a model</li>\n",
    "<li>For StringIndexer, that means building a mapping from each string to a unique number</li>\n",
    "<li>This step was not necessary for VectorAssembler because there are no parameters</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "352dbbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stringMapper: org.apache.spark.ml.feature.StringIndexerModel = StringIndexerModel: uid=strIdx_823ea4a5c0b1, handleInvalid=error\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stringMapper = indexer.fit(df) //label encoder //for future set, testing set //unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb72ab",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Use transform to generate numerical values for strings</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "216825bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexedStrings: org.apache.spark.sql.DataFrame = [feature1: string, feature2: double ... 2 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexedStrings = stringMapper.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74f7d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---+-------------+\n",
      "|feature1|feature2| dv|feature1Index|\n",
      "+--------+--------+---+-------------+\n",
      "|     MIA|    17.2|  2|          4.0|\n",
      "|     NYC|    23.1|  3|          0.0|\n",
      "|     SFO|    13.0|  2|          1.0|\n",
      "|     NYC|    16.2|  4|          0.0|\n",
      "|     CHI|    34.8|  3|          2.0|\n",
      "|     SFO|    71.0|  3|          1.0|\n",
      "|     LAX|    22.1|  7|          3.0|\n",
      "+--------+--------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexedStrings.show //most frequent => 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0672ff2a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Putting it all together</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a62b3a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---+-------------+\n",
      "|feature1|feature2| dv|feature1Index|\n",
      "+--------+--------+---+-------------+\n",
      "|     MIA|    17.2|  2|          4.0|\n",
      "|     NYC|    23.1|  3|          0.0|\n",
      "|     SFO|    13.0|  2|          1.0|\n",
      "|     NYC|    16.2|  4|          0.0|\n",
      "|     CHI|    34.8|  3|          2.0|\n",
      "|     SFO|    71.0|  3|          1.0|\n",
      "|     LAX|    22.1|  7|          3.0|\n",
      "+--------+--------+---+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "df: org.apache.spark.sql.DataFrame = [feature1: string, feature2: double ... 1 more field]\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_4a09fb538a4f\n",
       "indexed: org.apache.spark.sql.DataFrame = [feature1: string, feature2: double ... 2 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val df = spark.createDataFrame(Seq(\n",
    "  (\"MIA\", 17.2,2),  \n",
    "  (\"NYC\", 23.1,3),\n",
    "  (\"SFO\", 13.0,2),\n",
    "  (\"NYC\", 16.2,4),\n",
    "  (\"CHI\", 34.8,3),\n",
    "  (\"SFO\", 71.0,3),\n",
    "  (\"LAX\", 22.1,7)\n",
    ")).toDF(\"feature1\", \"feature2\",\"dv\")\n",
    "\n",
    "//Create a StringIndexer object with the column specifications\n",
    "val indexer = new StringIndexer()\n",
    "  .setInputCol(\"feature1\")\n",
    "  .setOutputCol(\"feature1Index\")\n",
    "\n",
    "//The \"fit\" operation determines the category to number relationship\n",
    "//The \"transform\" operation does the actual assigning of values\n",
    "val indexed = indexer\n",
    "                .fit(df)\n",
    "                .transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f74ca",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">One Hot Encoding</span>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98406b43",
   "metadata": {},
   "source": [
    "<li>One hot encoding maps a categorical (numerical) column with k distinct categories into k-1 binary columns</li>\n",
    "<li>Example: if possible values of a column are 1, 2, and 3; then one hot encoding will produce two columns<li>\n",
    "    <ul>\n",
    "        <li>column 1 will take the value 1 if the original value was 1 and 0 otherwise</li>\n",
    "        <li>column 2 will take the value 1 if the original value was 2 and 0 otherwise</li>\n",
    "        <li>column 1 and column 2 will both take the value 0 if the original value was 0</li>\n",
    "    </ul>\n",
    "<li>The spark feature transformer OneHotEncoder does this for us</li> \n",
    "<li>The one hot coded data is returned as a vector in a single column</li>\n",
    "<li>The input values to the encoder must be numeric. Use StringIndexer to convert strings to numeric first</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2e5db5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.OneHotEncoder\n",
       "df: org.apache.spark.sql.DataFrame = [student: string, grade: string ... 1 more field]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "\n",
    "val df = spark.createDataFrame(Seq(\n",
    "    (\"Jack\",\"A\",\"IEOR\"),\n",
    "    (\"Jill\",\"B\",\"IEOR\"),\n",
    "    (\"Jiahuo\",\"A\",\"CS\"),\n",
    "    (\"Pierre\",\"C\",\"APAM\"),\n",
    "    (\"Clemence\",\"B\",\"APAM\"),\n",
    "    (\"Savitri\",\"A\",\"CS\"),\n",
    "    (\"Bjorn\",\"A\",\"QMSS\")\n",
    ")).toDF(\"student\",\"grade\",\"department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4126188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------+\n",
      "| student|grade|department|\n",
      "+--------+-----+----------+\n",
      "|    Jack|    A|      IEOR|\n",
      "|    Jill|    B|      IEOR|\n",
      "|  Jiahuo|    A|        CS|\n",
      "|  Pierre|    C|      APAM|\n",
      "|Clemence|    B|      APAM|\n",
      "| Savitri|    A|        CS|\n",
      "|   Bjorn|    A|      QMSS|\n",
      "+--------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82682a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Convert strings to numeric labels</span>\n",
    "\n",
    "<li>Note that we can index multiple columns simultaneously</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "827cec1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------+----------+---------------+\n",
      "| student|grade|department|gradeIndex|departmentIndex|\n",
      "+--------+-----+----------+----------+---------------+\n",
      "|    Jack|    A|      IEOR|       0.0|            2.0|\n",
      "|    Jill|    B|      IEOR|       1.0|            2.0|\n",
      "|  Jiahuo|    A|        CS|       0.0|            1.0|\n",
      "|  Pierre|    C|      APAM|       2.0|            0.0|\n",
      "|Clemence|    B|      APAM|       1.0|            0.0|\n",
      "| Savitri|    A|        CS|       0.0|            1.0|\n",
      "|   Bjorn|    A|      QMSS|       0.0|            3.0|\n",
      "+--------+-----+----------+----------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_d4321fa15102\n",
       "indexedDf: org.apache.spark.sql.DataFrame = [student: string, grade: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    "  .setInputCols(Array(\"grade\",\"department\"))\n",
    "  .setOutputCols(Array(\"gradeIndex\",\"departmentIndex\"))\n",
    "\n",
    "//The \"fit\" operation determines the category to number relationship\n",
    "//The \"transform\" operation does the actual assigning of values\n",
    "val indexedDf = indexer\n",
    "                .fit(df)\n",
    "                .transform(df)\n",
    "indexedDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2d6f7",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Create an Encoder object</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c676052c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.OneHotEncoder\n",
       "encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHotEncoder_016c480b470c\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "\n",
    "val encoder = new OneHotEncoder()\n",
    "  .setInputCols(Array(\"gradeIndex\", \"departmentIndex\")) //since scala tuple is not iteratble in scala, so it need Array\n",
    "  .setOutputCols(Array(\"gradeVec\", \"departmentVec\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd2136",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Fit the model</span>\n",
    "\n",
    "<li>Why do we need to fit?</li>\n",
    "<li>Also, note that we're now using indexedDf</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3d21be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.feature.OneHotEncoderModel = OneHotEncoderModel: uid=oneHotEncoder_016c480b470c, dropLast=true, handleInvalid=error, numInputCols=2, numOutputCols=2\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = encoder.fit(indexedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c83cd",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Run transform to generate the one hot encoded columns</span>\n",
    "<li>The one hot encoded values are returned in the form of a sparse vector</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5481fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------+----------+---------------+-------------+-------------+\n",
      "| student|grade|department|gradeIndex|departmentIndex|     gradeVec|departmentVec|\n",
      "+--------+-----+----------+----------+---------------+-------------+-------------+\n",
      "|    Jack|    A|      IEOR|       0.0|            2.0|(2,[0],[1.0])|(3,[2],[1.0])|\n",
      "|    Jill|    B|      IEOR|       1.0|            2.0|(2,[1],[1.0])|(3,[2],[1.0])|\n",
      "|  Jiahuo|    A|        CS|       0.0|            1.0|(2,[0],[1.0])|(3,[1],[1.0])|\n",
      "|  Pierre|    C|      APAM|       2.0|            0.0|    (2,[],[])|(3,[0],[1.0])|\n",
      "|Clemence|    B|      APAM|       1.0|            0.0|(2,[1],[1.0])|(3,[0],[1.0])|\n",
      "| Savitri|    A|        CS|       0.0|            1.0|(2,[0],[1.0])|(3,[1],[1.0])|\n",
      "|   Bjorn|    A|      QMSS|       0.0|            3.0|(2,[0],[1.0])|    (3,[],[])|\n",
      "+--------+-----+----------+----------+---------------+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "encoded: org.apache.spark.sql.DataFrame = [student: string, grade: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "val encoded = model.transform(indexedDf)\n",
    "encoded.show()\n",
    "//gradeVec: length is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65dd1bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a3e6ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[25] at rdd at <console>:43\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37c8470e",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "43: error: not found: value Row",
     "output_type": "error",
     "traceback": [
      "<console>:43: error: not found: value Row",
      "       encoded.select(\"departmentVec\").where($\"student\"===\"Jack\").rdd.map{ case Row(v: Vector) => v.toDense}",
      "                                                                                ^",
      "<console>:43: error: type Vector takes type parameters",
      "       encoded.select(\"departmentVec\").where($\"student\"===\"Jack\").rdd.map{ case Row(v: Vector) => v.toDense}",
      "                                                                                       ^",
      ""
     ]
    }
   ],
   "source": [
    "encoded.select(\"departmentVec\").where($\"student\"===\"Jack\").rdd.map{ case Row(v: Vector) => v.toDense}\n",
    ".collect()(0)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5722366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.sql.Row\n",
       "res17: org.apache.spark.ml.linalg.DenseVector = [0.0,0.0,1.0]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "\n",
    "encoded.select(\"departmentVec\").where($\"student\"===\"Jack\").rdd.map { case Row(v: Vector) => v}.first.toDense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d353dc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Array[org.apache.spark.ml.linalg.DenseVector] = Array([0.0,0.0,1.0], [0.0,0.0,1.0], [0.0,1.0,0.0], [1.0,0.0,0.0], [1.0,0.0,0.0], [0.0,1.0,0.0], [0.0,0.0,0.0])\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.select(\"departmentVec\").rdd.map { case Row(v: Vector) => v}.map(v=>v.toDense).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1eb66b",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<h2 style=\"color:red;font-size:50px\">Example: California home values</h2>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3957466",
   "metadata": {},
   "source": [
    "<li>California housing data from 1980</li>\n",
    "<ol>\n",
    "<li>longitude: A measure of how far west a house is; a higher value is farther west\n",
    "\n",
    "<li>latitude: A measure of how far north a house is; a higher value is farther north\n",
    "\n",
    "<li>housingMedianAge: Median age of a house within a block; a lower number is a newer building\n",
    "\n",
    "<li>totalRooms: Total number of rooms within a block\n",
    "<li>totalBedrooms: Total number of bedrooms within a block\n",
    "<li>population: Total number of people residing within a block\n",
    "<li>households: Total number of households, a group of people residing within a home unit, for a block\n",
    "<li>medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "<li>medianHouseValue: Median house value for households within a block (measured in US Dollars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf5654",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Predict median home values from california housing data</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a989dbe",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Read the data </span>\n",
    "<li>set inferschema to true, header to true</li> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b006409",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19a0ec86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000\r\n",
      "-122.220000,37.860000,21.000000,7099.000000,1106.000000,2401.000000,1138.000000,8.301400,358500.000000\r\n",
      "-122.240000,37.850000,52.000000,1467.000000,190.000000,496.000000,177.000000,7.257400,352100.000000\r\n",
      "-122.250000,37.850000,52.000000,1274.000000,235.000000,558.000000,219.000000,5.643100,341300.000000\r\n",
      "-122.250000,37.850000,52.000000,1627.000000,280.000000,565.000000,259.000000,3.846200,342200.000000\r\n",
      "-122.250000,37.850000,52.000000,919.000000,213.000000,413.000000,193.000000,4.036800,269700.000000\r\n",
      "-122.250000,37.840000,52.000000,2535.000000,489.000000,1094.000000,514.000000,3.659100,299200.000000\r\n",
      "-122.250000,37.840000,52.000000,3104.000000,687.000000,1157.000000,647.000000,3.120000,241400.000000\r\n",
      "-122.260000,37.840000,42.000000,2555.000000,665.000000,1206.000000,595.000000,2.080400,226700.000000\r\n",
      "-122.250000,37.840000,52.000000,3549.000000,707.000000,1551.000000,714.000000,3.691200,261100.000000\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head \"cal_housing.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1938a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+------+------+------+------+------+--------+\n",
      "|    _c0|  _c1| _c2|   _c3|   _c4|   _c5|   _c6|   _c7|     _c8|\n",
      "+-------+-----+----+------+------+------+------+------+--------+\n",
      "|-122.23|37.88|41.0| 880.0| 129.0| 322.0| 126.0|8.3252|452600.0|\n",
      "|-122.22|37.86|21.0|7099.0|1106.0|2401.0|1138.0|8.3014|358500.0|\n",
      "|-122.24|37.85|52.0|1467.0| 190.0| 496.0| 177.0|7.2574|352100.0|\n",
      "|-122.25|37.85|52.0|1274.0| 235.0| 558.0| 219.0|5.6431|341300.0|\n",
      "|-122.25|37.85|52.0|1627.0| 280.0| 565.0| 259.0|3.8462|342200.0|\n",
      "|-122.25|37.85|52.0| 919.0| 213.0| 413.0| 193.0|4.0368|269700.0|\n",
      "|-122.25|37.84|52.0|2535.0| 489.0|1094.0| 514.0|3.6591|299200.0|\n",
      "|-122.25|37.84|52.0|3104.0| 687.0|1157.0| 647.0|  3.12|241400.0|\n",
      "|-122.26|37.84|42.0|2555.0| 665.0|1206.0| 595.0|2.0804|226700.0|\n",
      "|-122.25|37.84|52.0|3549.0| 707.0|1551.0| 714.0|3.6912|261100.0|\n",
      "|-122.26|37.85|52.0|2202.0| 434.0| 910.0| 402.0|3.2031|281500.0|\n",
      "|-122.26|37.85|52.0|3503.0| 752.0|1504.0| 734.0|3.2705|241800.0|\n",
      "|-122.26|37.85|52.0|2491.0| 474.0|1098.0| 468.0| 3.075|213500.0|\n",
      "|-122.26|37.84|52.0| 696.0| 191.0| 345.0| 174.0|2.6736|191300.0|\n",
      "|-122.26|37.85|52.0|2643.0| 626.0|1212.0| 620.0|1.9167|159200.0|\n",
      "|-122.26|37.85|50.0|1120.0| 283.0| 697.0| 264.0| 2.125|140000.0|\n",
      "|-122.27|37.85|52.0|1966.0| 347.0| 793.0| 331.0| 2.775|152500.0|\n",
      "|-122.27|37.85|52.0|1228.0| 293.0| 648.0| 303.0|2.1202|155500.0|\n",
      "|-122.26|37.84|50.0|2239.0| 455.0| 990.0| 419.0|1.9911|158700.0|\n",
      "|-122.27|37.84|52.0|1503.0| 298.0| 690.0| 275.0|2.6033|162900.0|\n",
      "+-------+-----+----+------+------+------+------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"csv\")\n",
    "        .option(\"header\",\"false\")\n",
    "        .option(\"inferschema\",\"true\")\n",
    "        .load(\"cal_housing.data\")\n",
    "        .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b4eede4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "df: org.apache.spark.sql.DataFrame = [Longitude: double, Latitude: double ... 7 more fields]\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val df = spark.read.format(\"csv\")\n",
    "        .option(\"header\",\"false\")\n",
    "        .option(\"inferschema\",\"true\")\n",
    "        .load(\"cal_housing.data\")\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2b5cb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+----------+-------------+----------+----------+------------+---------------+\n",
      "|Longitude|Latitude|MedianAge|TotalRooms|TotalBedrooms|Population|Households|MedianIncome|MedianHomeValue|\n",
      "+---------+--------+---------+----------+-------------+----------+----------+------------+---------------+\n",
      "|  -122.23|   37.88|     41.0|     880.0|        129.0|     322.0|     126.0|      8.3252|       452600.0|\n",
      "|  -122.22|   37.86|     21.0|    7099.0|       1106.0|    2401.0|    1138.0|      8.3014|       358500.0|\n",
      "|  -122.24|   37.85|     52.0|    1467.0|        190.0|     496.0|     177.0|      7.2574|       352100.0|\n",
      "|  -122.25|   37.85|     52.0|    1274.0|        235.0|     558.0|     219.0|      5.6431|       341300.0|\n",
      "|  -122.25|   37.85|     52.0|    1627.0|        280.0|     565.0|     259.0|      3.8462|       342200.0|\n",
      "|  -122.25|   37.85|     52.0|     919.0|        213.0|     413.0|     193.0|      4.0368|       269700.0|\n",
      "|  -122.25|   37.84|     52.0|    2535.0|        489.0|    1094.0|     514.0|      3.6591|       299200.0|\n",
      "|  -122.25|   37.84|     52.0|    3104.0|        687.0|    1157.0|     647.0|        3.12|       241400.0|\n",
      "|  -122.26|   37.84|     42.0|    2555.0|        665.0|    1206.0|     595.0|      2.0804|       226700.0|\n",
      "|  -122.25|   37.84|     52.0|    3549.0|        707.0|    1551.0|     714.0|      3.6912|       261100.0|\n",
      "|  -122.26|   37.85|     52.0|    2202.0|        434.0|     910.0|     402.0|      3.2031|       281500.0|\n",
      "|  -122.26|   37.85|     52.0|    3503.0|        752.0|    1504.0|     734.0|      3.2705|       241800.0|\n",
      "|  -122.26|   37.85|     52.0|    2491.0|        474.0|    1098.0|     468.0|       3.075|       213500.0|\n",
      "|  -122.26|   37.84|     52.0|     696.0|        191.0|     345.0|     174.0|      2.6736|       191300.0|\n",
      "|  -122.26|   37.85|     52.0|    2643.0|        626.0|    1212.0|     620.0|      1.9167|       159200.0|\n",
      "|  -122.26|   37.85|     50.0|    1120.0|        283.0|     697.0|     264.0|       2.125|       140000.0|\n",
      "|  -122.27|   37.85|     52.0|    1966.0|        347.0|     793.0|     331.0|       2.775|       152500.0|\n",
      "|  -122.27|   37.85|     52.0|    1228.0|        293.0|     648.0|     303.0|      2.1202|       155500.0|\n",
      "|  -122.26|   37.84|     50.0|    2239.0|        455.0|     990.0|     419.0|      1.9911|       158700.0|\n",
      "|  -122.27|   37.84|     52.0|    1503.0|        298.0|     690.0|     275.0|      2.6033|       162900.0|\n",
      "+---------+--------+---------+----------+-------------+----------+----------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a285981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- MedianAge: double (nullable = true)\n",
      " |-- TotalRooms: double (nullable = true)\n",
      " |-- TotalBedrooms: double (nullable = true)\n",
      " |-- Population: double (nullable = true)\n",
      " |-- Households: double (nullable = true)\n",
      " |-- MedianIncome: double (nullable = true)\n",
      " |-- MedianHomeValue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ca993",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Eyeball the data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f54dd681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+----------+-------------+----------+----------+------------+---------------+\n",
      "|Longitude|Latitude|MedianAge|TotalRooms|TotalBedrooms|Population|Households|MedianIncome|MedianHomeValue|\n",
      "+---------+--------+---------+----------+-------------+----------+----------+------------+---------------+\n",
      "|-122.23  |37.88   |41.0     |880.0     |129.0        |322.0     |126.0     |8.3252      |452600.0       |\n",
      "|-122.22  |37.86   |21.0     |7099.0    |1106.0       |2401.0    |1138.0    |8.3014      |358500.0       |\n",
      "|-122.24  |37.85   |52.0     |1467.0    |190.0        |496.0     |177.0     |7.2574      |352100.0       |\n",
      "|-122.25  |37.85   |52.0     |1274.0    |235.0        |558.0     |219.0     |5.6431      |341300.0       |\n",
      "|-122.25  |37.85   |52.0     |1627.0    |280.0        |565.0     |259.0     |3.8462      |342200.0       |\n",
      "|-122.25  |37.85   |52.0     |919.0     |213.0        |413.0     |193.0     |4.0368      |269700.0       |\n",
      "|-122.25  |37.84   |52.0     |2535.0    |489.0        |1094.0    |514.0     |3.6591      |299200.0       |\n",
      "|-122.25  |37.84   |52.0     |3104.0    |687.0        |1157.0    |647.0     |3.12        |241400.0       |\n",
      "|-122.26  |37.84   |42.0     |2555.0    |665.0        |1206.0    |595.0     |2.0804      |226700.0       |\n",
      "|-122.25  |37.84   |52.0     |3549.0    |707.0        |1551.0    |714.0     |3.6912      |261100.0       |\n",
      "|-122.26  |37.85   |52.0     |2202.0    |434.0        |910.0     |402.0     |3.2031      |281500.0       |\n",
      "|-122.26  |37.85   |52.0     |3503.0    |752.0        |1504.0    |734.0     |3.2705      |241800.0       |\n",
      "|-122.26  |37.85   |52.0     |2491.0    |474.0        |1098.0    |468.0     |3.075       |213500.0       |\n",
      "|-122.26  |37.84   |52.0     |696.0     |191.0        |345.0     |174.0     |2.6736      |191300.0       |\n",
      "|-122.26  |37.85   |52.0     |2643.0    |626.0        |1212.0    |620.0     |1.9167      |159200.0       |\n",
      "|-122.26  |37.85   |50.0     |1120.0    |283.0        |697.0     |264.0     |2.125       |140000.0       |\n",
      "|-122.27  |37.85   |52.0     |1966.0    |347.0        |793.0     |331.0     |2.775       |152500.0       |\n",
      "|-122.27  |37.85   |52.0     |1228.0    |293.0        |648.0     |303.0     |2.1202      |155500.0       |\n",
      "|-122.26  |37.84   |50.0     |2239.0    |455.0        |990.0     |419.0     |1.9911      |158700.0       |\n",
      "|-122.27  |37.84   |52.0     |1503.0    |298.0        |690.0     |275.0     |2.6033      |162900.0       |\n",
      "+---------+--------+---------+----------+-------------+----------+----------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f35f75",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Feature Engineering</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8336801c",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Setting up the dependent variable</span>\n",
    "<li>We'll simplify the median home value by dividing it by 100,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5016a5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+----------+-------------+----------+----------+------------+---------------+\n",
      "|Longitude|Latitude|MedianAge|TotalRooms|TotalBedrooms|Population|Households|MedianIncome|MedianHomeValue|\n",
      "+---------+--------+---------+----------+-------------+----------+----------+------------+---------------+\n",
      "|  -122.23|   37.88|     41.0|     880.0|        129.0|     322.0|     126.0|      8.3252|          4.526|\n",
      "|  -122.22|   37.86|     21.0|    7099.0|       1106.0|    2401.0|    1138.0|      8.3014|          3.585|\n",
      "|  -122.24|   37.85|     52.0|    1467.0|        190.0|     496.0|     177.0|      7.2574|          3.521|\n",
      "|  -122.25|   37.85|     52.0|    1274.0|        235.0|     558.0|     219.0|      5.6431|          3.413|\n",
      "|  -122.25|   37.85|     52.0|    1627.0|        280.0|     565.0|     259.0|      3.8462|          3.422|\n",
      "|  -122.25|   37.85|     52.0|     919.0|        213.0|     413.0|     193.0|      4.0368|          2.697|\n",
      "|  -122.25|   37.84|     52.0|    2535.0|        489.0|    1094.0|     514.0|      3.6591|          2.992|\n",
      "|  -122.25|   37.84|     52.0|    3104.0|        687.0|    1157.0|     647.0|        3.12|          2.414|\n",
      "|  -122.26|   37.84|     42.0|    2555.0|        665.0|    1206.0|     595.0|      2.0804|          2.267|\n",
      "|  -122.25|   37.84|     52.0|    3549.0|        707.0|    1551.0|     714.0|      3.6912|          2.611|\n",
      "|  -122.26|   37.85|     52.0|    2202.0|        434.0|     910.0|     402.0|      3.2031|          2.815|\n",
      "|  -122.26|   37.85|     52.0|    3503.0|        752.0|    1504.0|     734.0|      3.2705|          2.418|\n",
      "|  -122.26|   37.85|     52.0|    2491.0|        474.0|    1098.0|     468.0|       3.075|          2.135|\n",
      "|  -122.26|   37.84|     52.0|     696.0|        191.0|     345.0|     174.0|      2.6736|          1.913|\n",
      "|  -122.26|   37.85|     52.0|    2643.0|        626.0|    1212.0|     620.0|      1.9167|          1.592|\n",
      "|  -122.26|   37.85|     50.0|    1120.0|        283.0|     697.0|     264.0|       2.125|            1.4|\n",
      "|  -122.27|   37.85|     52.0|    1966.0|        347.0|     793.0|     331.0|       2.775|          1.525|\n",
      "|  -122.27|   37.85|     52.0|    1228.0|        293.0|     648.0|     303.0|      2.1202|          1.555|\n",
      "|  -122.26|   37.84|     50.0|    2239.0|        455.0|     990.0|     419.0|      1.9911|          1.587|\n",
      "|  -122.27|   37.84|     52.0|    1503.0|        298.0|     690.0|     275.0|      2.6033|          1.629|\n",
      "+---------+--------+---------+----------+-------------+----------+----------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"MedianHomeValue\",$\"MedianHomeValue\"/100000).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a5a2a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "df: org.apache.spark.sql.DataFrame = [Longitude: double, Latitude: double ... 7 more fields]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val df = spark.read.format(\"csv\")\n",
    "        .option(\"header\",\"false\")\n",
    "        .option(\"inferschema\",\"true\")\n",
    "        .load(\"cal_housing.data\")\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\n",
    "        .withColumn(\"MedianHomeValue\",$\"MedianHomeValue\"/100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6122fb",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Setting up independent variables</span>\n",
    "<li>We'll divide total rooms, total bedrooms, and population by the number of households to get per household data</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46f4e26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+----------+-------------+----------+----------+------------+---------------+------------------+------------------+------------------+\n",
      "|Longitude|Latitude|MedianAge|TotalRooms|TotalBedrooms|Population|Households|MedianIncome|MedianHomeValue|     RoomsPerHouse|    PeoplePerHouse|  BedroomsPerHouse|\n",
      "+---------+--------+---------+----------+-------------+----------+----------+------------+---------------+------------------+------------------+------------------+\n",
      "|  -122.23|   37.88|     41.0|     880.0|        129.0|     322.0|     126.0|      8.3252|          4.526| 6.984126984126984|2.5555555555555554|1.0238095238095237|\n",
      "|  -122.22|   37.86|     21.0|    7099.0|       1106.0|    2401.0|    1138.0|      8.3014|          3.585| 6.238137082601054| 2.109841827768014|0.9718804920913884|\n",
      "|  -122.24|   37.85|     52.0|    1467.0|        190.0|     496.0|     177.0|      7.2574|          3.521| 8.288135593220339|2.8022598870056497| 1.073446327683616|\n",
      "|  -122.25|   37.85|     52.0|    1274.0|        235.0|     558.0|     219.0|      5.6431|          3.413|5.8173515981735155| 2.547945205479452|1.0730593607305936|\n",
      "|  -122.25|   37.85|     52.0|    1627.0|        280.0|     565.0|     259.0|      3.8462|          3.422| 6.281853281853282|2.1814671814671813|1.0810810810810811|\n",
      "|  -122.25|   37.85|     52.0|     919.0|        213.0|     413.0|     193.0|      4.0368|          2.697| 4.761658031088083| 2.139896373056995|1.1036269430051813|\n",
      "|  -122.25|   37.84|     52.0|    2535.0|        489.0|    1094.0|     514.0|      3.6591|          2.992|4.9319066147859925|2.1284046692607004|0.9513618677042801|\n",
      "|  -122.25|   37.84|     52.0|    3104.0|        687.0|    1157.0|     647.0|        3.12|          2.414| 4.797527047913447|1.7882534775888717| 1.061823802163833|\n",
      "|  -122.26|   37.84|     42.0|    2555.0|        665.0|    1206.0|     595.0|      2.0804|          2.267| 4.294117647058823| 2.026890756302521|1.1176470588235294|\n",
      "|  -122.25|   37.84|     52.0|    3549.0|        707.0|    1551.0|     714.0|      3.6912|          2.611| 4.970588235294118| 2.172268907563025|0.9901960784313726|\n",
      "|  -122.26|   37.85|     52.0|    2202.0|        434.0|     910.0|     402.0|      3.2031|          2.815| 5.477611940298507| 2.263681592039801|1.0796019900497513|\n",
      "|  -122.26|   37.85|     52.0|    3503.0|        752.0|    1504.0|     734.0|      3.2705|          2.418| 4.772479564032698|2.0490463215258856|1.0245231607629428|\n",
      "|  -122.26|   37.85|     52.0|    2491.0|        474.0|    1098.0|     468.0|       3.075|          2.135| 5.322649572649572|2.3461538461538463|1.0128205128205128|\n",
      "|  -122.26|   37.84|     52.0|     696.0|        191.0|     345.0|     174.0|      2.6736|          1.913|               4.0|1.9827586206896552|1.0977011494252873|\n",
      "|  -122.26|   37.85|     52.0|    2643.0|        626.0|    1212.0|     620.0|      1.9167|          1.592| 4.262903225806451|1.9548387096774194|1.0096774193548388|\n",
      "|  -122.26|   37.85|     50.0|    1120.0|        283.0|     697.0|     264.0|       2.125|            1.4| 4.242424242424242| 2.640151515151515| 1.071969696969697|\n",
      "|  -122.27|   37.85|     52.0|    1966.0|        347.0|     793.0|     331.0|       2.775|          1.525|5.9395770392749245| 2.395770392749245|1.0483383685800605|\n",
      "|  -122.27|   37.85|     52.0|    1228.0|        293.0|     648.0|     303.0|      2.1202|          1.555| 4.052805280528053|2.1386138613861387| 0.966996699669967|\n",
      "|  -122.26|   37.84|     50.0|    2239.0|        455.0|     990.0|     419.0|      1.9911|          1.587| 5.343675417661098|2.3627684964200477|1.0859188544152745|\n",
      "|  -122.27|   37.84|     52.0|    1503.0|        298.0|     690.0|     275.0|      2.6033|          1.629| 5.465454545454546|2.5090909090909093|1.0836363636363637|\n",
      "+---------+--------+---------+----------+-------------+----------+----------+------------+---------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "    .withColumn(\"RoomsPerHouse\", col(\"TotalRooms\")/col(\"Households\"))\n",
    "    .withColumn(\"PeoplePerHouse\", col(\"Population\")/col(\"Households\"))\n",
    "    .withColumn(\"BedroomsPerHouse\", col(\"TotalBedrooms\")/col(\"Households\"))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff42228b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "df: org.apache.spark.sql.DataFrame = [Longitude: double, Latitude: double ... 10 more fields]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val df = spark.read.format(\"csv\")\n",
    "        .option(\"header\",\"false\")\n",
    "        .option(\"inferschema\",\"true\")\n",
    "        .load(\"cal_housing.data\")\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\n",
    "        .withColumn(\"MedianHomeValue\",$\"MedianHomeValue\"/100000)\n",
    "    .withColumn(\"RoomsPerHouse\", col(\"TotalRooms\")/col(\"Households\"))\n",
    "    .withColumn(\"PeoplePerHouse\", col(\"Population\")/col(\"Households\"))\n",
    "    .withColumn(\"BedroomsPerHouse\", col(\"TotalBedrooms\")/col(\"Households\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf498e",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Select the features we need</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02dee178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- MedianAge: double (nullable = true)\n",
      " |-- TotalRooms: double (nullable = true)\n",
      " |-- TotalBedrooms: double (nullable = true)\n",
      " |-- Population: double (nullable = true)\n",
      " |-- Households: double (nullable = true)\n",
      " |-- MedianIncome: double (nullable = true)\n",
      " |-- MedianHomeValue: double (nullable = true)\n",
      " |-- RoomsPerHouse: double (nullable = true)\n",
      " |-- PeoplePerHouse: double (nullable = true)\n",
      " |-- BedroomsPerHouse: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f856aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"MedianHomeValue\", \n",
    "              \"MedianAge\", \n",
    "              \"Population\", \n",
    "              \"Households\", \n",
    "              \"MedianIncome\", \n",
    "              \"RoomsPerHouse\", \n",
    "              \"PeoplePerHouse\", \n",
    "              \"BedroomsPerHouse\",\n",
    "               \"Latitude\",\n",
    "               \"Longitude\").printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02f4d348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "df: org.apache.spark.sql.DataFrame = [MedianHomeValue: double, MedianAge: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val df = spark.read.format(\"csv\")\n",
    "        .option(\"header\",\"false\")\n",
    "        .option(\"inferschema\",\"true\")\n",
    "        .load(\"cal_housing.data\")\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\n",
    "        .withColumn(\"MedianHomeValue\",$\"MedianHomeValue\"/100000)\n",
    "    .withColumn(\"RoomsPerHouse\", col(\"TotalRooms\")/col(\"Households\"))\n",
    "    .withColumn(\"PeoplePerHouse\", col(\"Population\")/col(\"Households\"))\n",
    "    .withColumn(\"BedroomsPerHouse\", col(\"TotalBedrooms\")/col(\"Households\"))\n",
    "    .select(\"MedianHomeValue\", \n",
    "              \"MedianAge\", \n",
    "              \"Population\", \n",
    "              \"Households\", \n",
    "              \"MedianIncome\", \n",
    "              \"RoomsPerHouse\", \n",
    "              \"PeoplePerHouse\", \n",
    "              \"BedroomsPerHouse\",\n",
    "               \"Latitude\",\n",
    "               \"Longitude\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc56c1f",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Machine Learning Pipelines</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084903cb",
   "metadata": {},
   "source": [
    "<li>A machine learning pipeline is an end-to-end framework that takes raw data as an input and produces the output of the model</li>\n",
    "<li>A pipeline is designed to manage the flow of data as it goes through the feature engineering and feature transformation process, into the ML model, and gathers the results</li>\n",
    "<li>Spark contains pipleline support</li>\n",
    "<li>Spark pipelines contain transform, evaluate, and fit steps</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dcabc9",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Package the initial data preparation steps</span>\n",
    "<li>Package the various steps that massage the data to get it ready for Spark ML into a function</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf39e11",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Read the data from a file and split into train and test</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be56ac57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "readData: ()(org.apache.spark.sql.DataFrame, org.apache.spark.sql.DataFrame)\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "\n",
    "def readData(): (DataFrame,DataFrame) = {\n",
    "    val df = spark.read.format(\"csv\")\n",
    "        .option(\"header\",\"false\")\n",
    "        .option(\"inferschema\",\"true\")\n",
    "        .load(\"cal_housing.data\")\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\n",
    "    val Array(train,test) = df.randomSplit(Array(0.8,0.2),seed=1234L)\n",
    "    (train,test)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23a1b99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.DataFrame = [Longitude: double, Latitude: double ... 7 more fields]\n",
       "test: org.apache.spark.sql.DataFrame = [Longitude: double, Latitude: double ... 7 more fields]\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (train, test) = readData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08c6826",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Do the preprocessing steps</span>\n",
    "<li>train and test can be separately passed through this function</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6535613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "prepareData: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "\n",
    "def prepareData(df: DataFrame): DataFrame = {\n",
    "    df.withColumn(\"MedianHomeValue\",$\"MedianHomeValue\"/100000)\n",
    "        .withColumn(\"RoomsPerHouse\", col(\"TotalRooms\")/col(\"Households\"))\n",
    "        .withColumn(\"PeoplePerHouse\", col(\"Population\")/col(\"Households\"))\n",
    "        .withColumn(\"BedroomsPerHouse\", col(\"TotalBedrooms\")/col(\"Households\"))\n",
    "        .select(\"MedianHomeValue\", \n",
    "                  \"MedianAge\", \n",
    "                  \"Population\", \n",
    "                  \"Households\", \n",
    "                  \"MedianIncome\", \n",
    "                  \"RoomsPerHouse\", \n",
    "                  \"PeoplePerHouse\", \n",
    "                  \"BedroomsPerHouse\",\n",
    "                   \"Latitude\",\n",
    "                   \"Longitude\")\n",
    "        .withColumnRenamed(\"MedianHomeValue\",\"label\")//in spark, dependent variable default named label\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc97d754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: org.apache.spark.sql.DataFrame = [label: double, MedianAge: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepareData(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45033921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+----------+------------+------------------+------------------+------------------+--------+---------+\n",
      "|label|MedianAge|Population|Households|MedianIncome|     RoomsPerHouse|    PeoplePerHouse|  BedroomsPerHouse|Latitude|Longitude|\n",
      "+-----+---------+----------+----------+------------+------------------+------------------+------------------+--------+---------+\n",
      "|0.858|     19.0|    1298.0|     478.0|      1.9797| 5.589958158995816| 2.715481171548117|1.1548117154811715|    41.8|   -124.3|\n",
      "|1.114|     52.0|     907.0|     369.0|      2.3571| 6.008130081300813|2.4579945799457996| 1.067750677506775|   40.58|  -124.26|\n",
      "|0.705|     39.0|     883.0|     337.0|       1.745| 5.448071216617211| 2.620178041543027|1.0445103857566767|   40.79|  -124.18|\n",
      "|1.289|     17.0|     873.0|     313.0|      4.0357| 6.472843450479234|2.7891373801916934|1.0798722044728435|   40.74|  -124.17|\n",
      "|1.161|     13.0|     951.0|     353.0|      4.8516|  6.15014164305949|2.6940509915014164|0.9603399433427762|   40.75|  -124.17|\n",
      "|0.828|     26.0|     992.0|     380.0|      2.8056| 4.673684210526316| 2.610526315789474|              0.95|   40.76|  -124.17|\n",
      "|0.813|     30.0|     990.0|     359.0|      2.2227| 5.278551532033426|2.7576601671309193|1.0194986072423398|   40.77|  -124.17|\n",
      "|0.856|     35.0|    1053.0|     434.0|      2.8529| 4.933179723502304|2.4262672811059907|1.0092165898617511|   40.77|  -124.16|\n",
      "|0.805|     52.0|    1150.0|     571.0|      1.7308| 4.231173380035027|2.0140105078809105|1.0823117338003503|    40.8|  -124.16|\n",
      "|0.755|     23.0|    1060.0|     390.0|      2.1726| 4.287179487179487| 2.717948717948718|0.9871794871794872|   41.02|  -124.16|\n",
      "|  0.9|     36.0|     829.0|     368.0|      3.3984| 5.739130434782608|2.2527173913043477| 1.016304347826087|   40.78|  -124.15|\n",
      "|0.675|     50.0|     235.0|      83.0|        1.75| 4.096385542168675|2.8313253012048194| 0.891566265060241|   40.81|  -124.15|\n",
      "|0.575|     33.0|    1165.0|     441.0|       1.725| 5.068027210884353|2.6417233560090705|1.1473922902494331|   40.88|  -124.15|\n",
      "|1.031|     17.0|    3546.0|     585.0|      2.2868|               5.6| 6.061538461538461|1.0735042735042735|   41.81|  -124.15|\n",
      "|1.281|     27.0|     521.0|     219.0|       4.025| 5.242009132420091|2.3789954337899544|0.9406392694063926|    40.6|  -124.14|\n",
      "|0.996|     27.0|    1407.0|     571.0|      2.9143| 5.334500875656743|2.4640980735551663|1.0595446584938704|   40.77|  -124.14|\n",
      "|1.224|     21.0|    1208.0|     494.0|       2.275|5.4574898785425106| 2.445344129554656|1.1700404858299596|   41.95|  -124.14|\n",
      "|0.948|     38.0|     240.0|      91.0|        3.25| 5.978021978021978|2.6373626373626373| 0.978021978021978|   40.55|  -124.13|\n",
      "|0.928|     32.0|     855.0|     346.0|      3.5833| 5.829479768786127|2.4710982658959537|1.0375722543352601|   40.79|  -124.13|\n",
      "|0.723|     30.0|     996.0|     374.0|      2.2357| 5.152406417112299| 2.663101604278075|1.0508021390374331|    40.5|   -124.1|\n",
      "+-----+---------+----------+----------+------------+------------------+------------------+------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepareData(test).show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e718ec7c",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Applying Spark feature transformations</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea99164c",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Prepare data for ML</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33cf717",
   "metadata": {},
   "source": [
    "<li>Gather independent features into a column of vectors</li>\n",
    "<li>Each vector corresponds to one data point</li>\n",
    "<li>Specify the dependent variable (by default, ml models look for a column named <span style=\"color:blue\">label</span>)</li>\n",
    "<li>Keep only the column of vectors (independent variables) and the dependent variable</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d47b1",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">VectorAssembler</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dfe930",
   "metadata": {},
   "source": [
    "<li>Spark ML provided feature transformer that constructs a vector column given a set of columns</li>\n",
    "<li><a href=\"https://spark.apache.org/docs/latest/ml-features#vectorassembler\">https://spark.apache.org/docs/latest/ml-features#vectorassembler</a></li>\n",
    "<li>And constructs a dataframe with the columns from the input df and with the group of selected columns concatenated into a vector</li> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a1fb41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cols: Array[String] = Array(MedianAge, Population, Households, MedianIncome, RoomsPerHouse, PeoplePerHouse, BedroomsPerHouse, Latitude, Longitude)\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//IV column names\n",
    "val cols = prepareData(train).columns\n",
    "    .map(l => if (l != \"label\") Some(l) else None)\n",
    "    .flatten\n",
    "\n",
    "//if there is a missing value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0e41edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|0.946|[52.0,806.0,270.0...|\n",
      "|1.036|[17.0,1244.0,456....|\n",
      "| 0.79|[36.0,1194.0,465....|\n",
      "|0.761|[32.0,434.0,187.0...|\n",
      "|1.067|[52.0,1152.0,435....|\n",
      "|0.508|[52.0,544.0,172.0...|\n",
      "|0.732|[11.0,1343.0,479....|\n",
      "|0.783|[28.0,1530.0,653....|\n",
      "|0.581|[32.0,620.0,268.0...|\n",
      "|0.669|[20.0,1993.0,721....|\n",
      "|0.684|[17.0,1947.0,647....|\n",
      "|0.901|[21.0,2907.0,972....|\n",
      "| 0.69|[30.0,1367.0,583....|\n",
      "|  0.7|[37.0,640.0,260.0...|\n",
      "|0.746|[15.0,1645.0,640....|\n",
      "| 1.07|[35.0,480.0,179.0...|\n",
      "|0.722|[33.0,656.0,236.0...|\n",
      "| 0.67|[34.0,950.0,317.0...|\n",
      "|0.702|[37.0,867.0,310.0...|\n",
      "|0.646|[40.0,788.0,279.0...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
       "cols: Array[String] = Array(MedianAge, Population, Households, MedianIncome, RoomsPerHouse, PeoplePerHouse, BedroomsPerHouse, Latitude, Longitude)\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_d48b0f798214, handleInvalid=error, numInputCols=9\n",
       "vector_df: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
    "\n",
    "//Get the names of all columns except MedianHomeValue\n",
    "val cols = prepareData(train).columns\n",
    "    .map(l => if (l != \"label\") Some(l) else None)\n",
    "    .flatten\n",
    "\n",
    "\n",
    "//Create a vectorassembler from the list of columns and specify the name of the column of vectors\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(cols)\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "//Apply the transform function on the data frame, select the dv and features column\n",
    "//And rename the dv column to label\n",
    "\n",
    "val vector_df = assembler.transform(prepareData(train))\n",
    "    .select(\"label\",\"features\")\n",
    "\n",
    "vector_df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6607c4",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Scaling</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7944054",
   "metadata": {},
   "source": [
    "<li>Scale all independent variables </li>\n",
    "<li><a href=\"https://spark.apache.org/docs/latest/ml-features#standardscaler\">https://spark.apache.org/docs/latest/ml-features#standardscaler</a></li>\n",
    "<li><span style=\"color:blue\">withStd</span> scales the data to std of 1</li>\n",
    "<li><span style=\"color:blue\">withMean set to false</span> scales the data to std of 1 with the mean unchanged</li>\n",
    "<li><span style=\"color:blue\">withMean set to true</span> scales the data to std of 1 with the mean 0</li>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b1b1cd",
   "metadata": {},
   "source": [
    "<li>Create a Scaler object</li>\n",
    "<li>Specify the column to be scaled (features)</li>\n",
    "<li>Specify the options (withMean/withStd)</li>\n",
    "<li>Then apply fit to generate the parameters for scaling (the mean and the std)</li>\n",
    "<li>And apply transform to scale the data (to the fitted mean and std)</li>\n",
    "<li>A new column will be added to the dataframe with the scaled values</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4e040d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                         |scaledFeatures                                                                                                                                                                         |\n",
      "+-----+-------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.946|[52.0,806.0,270.0,3.0147,6.7407407407407405,2.9851851851851854,1.1111111111111112,40.54,-124.35] |[1.8529357551237848,-0.5466466981863762,-0.6068111954850205,-0.4491212351358302,0.5002442481908159,-0.009721921815120613,0.02525476705933747,2.3021252821080576,-2.383150611160217]    |\n",
      "|1.036|[17.0,1244.0,456.0,3.0313,5.870614035087719,2.7280701754385963,1.1644736842105263,41.84,-124.3]  |[-0.9237482035396988,-0.15946509278578908,-0.11418940034843383,-0.44042701274117857,0.16721183663803885,-0.031924919232474994,0.1303247242403456,2.912045664108187,-2.35819331581432]  |\n",
      "|0.79 |[36.0,1194.0,465.0,2.5179,5.051612903225807,2.567741935483871,1.135483870967742,40.69,-124.27]   |[0.5835945168776209,-0.20366390618768257,-0.09035286187408287,-0.7093194089709467,-0.1462527659725978,-0.04576995885560793,0.07324429765690262,2.3725007108003795,-2.34321893860678]   |\n",
      "|0.761|[32.0,434.0,187.0,1.9417,7.647058823529412,2.320855614973262,2.2406417112299466,40.28,-124.25]   |[0.26625920731607994,-0.8754858698964639,-0.8266370503040349,-1.0111032007660234,0.8471285522629198,-0.06708966454715751,2.2492804257925205,2.180141205708033,-2.333236020468423]      |\n",
      "|1.067|[52.0,1152.0,435.0,3.0806,6.1931034482758625,2.6482758620689655,1.0413793103448277,40.54,-124.23]|[1.8529357551237848,-0.24079090944527312,-0.1698079901219194,-0.4146062197257372,0.2906414777196617,-0.03881550463786081,-0.112045908210167,2.3021252821080576,-2.323253102330065]     |\n",
      "|0.508|[52.0,544.0,172.0,3.3462,6.465116279069767,3.1627906976744184,1.2151162790697674,40.81,-124.23]  |[1.8529357551237848,-0.7782484804122982,-0.8663646144279532,-0.2754986614113109,0.3947517049130742,0.005615085304739516,0.23003909265137554,2.4288010537542393,-2.323253102330065]     |\n",
      "|0.732|[11.0,1343.0,479.0,2.4805,6.5949895615866385,2.8037578288100207,1.2860125260960333,41.75,-124.23]|[-1.3997511678820103,-0.07195144225003992,-0.05327380202509247,-0.7289075967757641,0.444459422411997,-0.02538896174797173,0.3696325418927799,2.8698204068927917,-2.323253102330065]    |\n",
      "|0.783|[28.0,1530.0,653.0,1.7038,4.598774885145483,2.343032159264931,1.0704441041347625,41.73,-124.22]  |[-0.05107610224546106,0.0933521198730418,0.4075659418123595,-1.1357029301447963,-0.3195720495025626,-0.06517462366285028,-0.054817846263192464,2.8604370164004806,-2.318261643260883]  |\n",
      "|0.581|[32.0,620.0,268.0,1.6528,4.544776119402985,2.3134328358208953,1.2350746268656716,40.75,-124.21]  |[0.26625920731607994,-0.71106628404142,-0.6121082040348762,-1.1624140953331836,-0.34023954431597747,-0.06773065375471884,0.26933672442809936,2.400650882277309,-2.3132701841917007]    |\n",
      "|0.669|[20.0,1993.0,721.0,2.0074,5.284327323162275,2.764216366158114,1.0915395284327323,41.75,-124.21]  |[-0.6857467213685431,0.5026331319745757,0.5876642325074557,-0.976692935023336,-0.05718361780022481,-0.02880353872659306,-0.013281331081034381,2.8698204068927917,-2.3132701841917007]  |\n",
      "|0.684|[17.0,1947.0,647.0,2.5795,5.349304482225657,3.009273570324575,1.1159196290571871,41.77,-124.21]  |[-0.9237482035396988,0.46197022364483364,0.3916749161627922,-0.6770565114100707,-0.032314251260685926,-0.007641785172902664,0.03472265335779347,2.879203797385103,-2.3132701841917007] |\n",
      "|0.901|[21.0,2907.0,972.0,3.5363,5.8580246913580245,2.990740740740741,1.0864197530864197,40.73,-124.19] |[-0.6064128939781578,1.310587440961189,1.252438805514355,-0.17593410254243871,0.16239338953725654,-0.009242175472137598,-0.02336207766368633,2.391267491784998,-2.303287266053343]     |\n",
      "|0.69 |[30.0,1367.0,583.0,2.442,5.102915951972555,2.3447684391080617,1.0874785591766725,40.77,-124.19]  |[0.10759155253530944,-0.05073601181713104,0.22217064256740757,-0.7490719077513116,-0.12661703024222865,-0.0650246883599256,-0.021277307306781092,2.4100342727696202,-2.303287266053343]|\n",
      "|0.7  |[37.0,640.0,260.0,1.8242,5.273076923076923,2.4615384615384617,1.226923076923077,40.78,-124.19]   |[0.6629283442680062,-0.6933867586806627,-0.6332962382342993,-1.0726436303667204,-0.06148959743171587,-0.05494108993834034,0.25328646759675005,2.4147259680157744,-2.303287266053343]   |\n",
      "|0.746|[15.0,1645.0,640.0,1.6654,4.90625,2.5703125,1.115625,41.78,-124.19]                              |[-1.0824158583204693,0.19500939069739687,0.373135386238297,-1.1558148662866408,-0.20188898297075517,-0.04554797944892528,0.034142533985265296,2.883895492631257,-2.303287266053343]    |\n",
      "|1.07 |[35.0,480.0,179.0,3.0536,5.318435754189944,2.6815642458100557,0.994413407821229,40.62,-124.18]   |[0.5042606894872357,-0.8348229615667219,-0.847825084503458,-0.42874742482547185,-0.04412895235250554,-0.03594090817241221,-0.20452093481595854,2.339658844077295,-2.298295806984168]   |\n",
      "|0.722|[33.0,656.0,236.0,2.5096,4.559322033898305,2.7796610169491527,0.940677966101695,40.78,-124.18]   |[0.3455930347064652,-0.6792431383920567,-0.6968603408325685,-0.7136665201682726,-0.33467223905900173,-0.027469826074175946,-0.31032506367712615,2.4147259680157744,-2.298295806984168] |\n",
      "|0.67 |[34.0,950.0,317.0,2.1607,5.022082018927445,2.996845425867508,1.1482649842271293,40.78,-124.18]   |[0.42492686209685043,-0.4193541155889229,-0.48233149456340985,-0.8964023149570652,-0.15755542057367525,-0.008715009406504328,0.09841008230502263,2.4147259680157744,-2.298295806984168]|\n",
      "|0.702|[37.0,867.0,310.0,2.5536,4.687096774193549,2.796774193548387,0.9451612903225807,40.78,-124.18]   |[0.6629283442680062,-0.4927241458360661,-0.500871024487905,-0.6906215933390755,-0.2857677178903071,-0.025992028973920993,-0.3014974780387904,2.4147259680157744,-2.298295806984168]    |\n",
      "|0.646|[40.0,788.0,279.0,1.4668,5.010752688172043,2.824372759856631,1.114695340501792,40.79,-124.18]    |[0.9009298264391619,-0.5625582710110579,-0.5829746570106695,-1.259831286020244,-0.16189161014036466,-0.02360877294748924,0.032312050970743966,2.419417663261928,-2.298295806984168]    |\n",
      "+-----+-------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StandardScaler\n",
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_1c8c420141a8\n",
       "fitted_scaler: org.apache.spark.ml.feature.StandardScalerModel = StandardScalerModel: uid=stdScal_1c8c420141a8, numFeatures=9, withMean=true, withStd=true\n",
       "scaled_df: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 1 more field]\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "val scaler = new StandardScaler()\n",
    "      .setInputCol(\"features\")\n",
    "      .setOutputCol(\"scaledFeatures\")\n",
    "      .setWithStd(true)\n",
    "      .setWithMean(true)\n",
    "\n",
    "//Generate the parameters (fit the scaling object to the data)\n",
    "val fitted_scaler = scaler.fit(vector_df)\n",
    "\n",
    "//scale the data\n",
    "val scaled_df = fitted_scaler.transform(vector_df)\n",
    "scaled_df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "346f1f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaledFeatures: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794ed28",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Select and setup a model</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c12c6",
   "metadata": {},
   "source": [
    "<li>We'll use a straightforward regression model</li>\n",
    "<li>and fit the training data to the model</li>\n",
    "<li><a href=\"https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression\">https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression</a></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b4110c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_58e54e320c3e\n",
       "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = LinearRegressionModel: uid=linReg_58e54e320c3e, numFeatures=9\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setMaxIter(10)\n",
    "    .setRegParam(0.3) //Regularization parameter\n",
    "    .setElasticNetParam(0.8) //elastic net regularization parameter (L1 + L2 penalties)\n",
    "    .setFeaturesCol(\"scaledFeatures\") //independent variables\n",
    "    .setLabelCol(\"label\") //dependent variable (we don't need to specify this since we've called our col label)\n",
    "\n",
    "val lrModel = lr.fit(scaled_df) //fit the regression to the training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5460c119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res34: org.apache.spark.ml.regression.LinearRegressionModel = LinearRegressionModel: uid=linReg_58e54e320c3e, numFeatures=9\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f2b00",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Set up the pipeline</span>\n",
    "<li>and send data through it</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019932d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52efe306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_20ad1d99528f\n",
       "model: org.apache.spark.ml.PipelineModel = pipeline_20ad1d99528f\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(assembler,scaler,lr))\n",
    "val model = pipeline.fit(prepareData(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d2c70",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Model evaluation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c9683",
   "metadata": {},
   "source": [
    "<li>The model contains the estimated parameters</li>\n",
    "<li>And can be used to get predictions on the training and testing data</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d3468",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Create an evaluator</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08949297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_99ff2961ba17, metricName=rmse, throughOrigin=false\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7604bec",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Get predictions</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9c444c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [label: double, MedianAge: double ... 11 more fields]\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.transform(prepareData(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22514cd",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Get evaluation metrics</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3fa4c6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Test  RMSE: ,0.8771558134520177, Test  r2: ,0.41378266806495867)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rmse_test: Double = 0.8771558134520177\n",
       "r2_test: Double = 0.41378266806495867\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rmse_test = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "val r2_test = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "println(\"Test  RMSE: \",rmse_test,\" Test  r2: \",r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c91cb",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Get model estimated parameters</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5a191a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res41: org.apache.spark.ml.Transformer = LinearRegressionModel: uid=linReg_58e54e320c3e, numFeatures=9\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages(2)\n",
    "//Transformer is an umbrella type for lr, mlp, onehoteconding etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "672fd0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Coefficients: ,[0.0,0.0,0.0,0.5287039916961691,0.0,0.0,0.0,0.0,0.0])\n",
      "(Intercept: ,2.074400715885009)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegressionModel\n",
       "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = LinearRegressionModel: uid=linReg_58e54e320c3e, numFeatures=9\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegressionModel\n",
    "\n",
    "val lrModel = model.stages(2).asInstanceOf[LinearRegressionModel]//cast Transformer to lr\n",
    "println(\"Coefficients: \",lrModel.coefficients)\n",
    "println(\"Intercept: \",lrModel.intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3ac86",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Hyperparameter tuning</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee49fb",
   "metadata": {},
   "source": [
    "<li>Pipelines are useful for hyperparameter tuning</li>\n",
    "<li>Create a parameter grid with parameter options</li>\n",
    "<li>Create a cross validation model</li>\n",
    "<li>fit the model (finds the best model)</li>\n",
    "<li>extract model information</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141ae197",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Create a parameter grid</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8c9ff835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinReg_58e54e320c3e-elasticNetParam: 0.7,\n",
       "\tlinReg_58e54e320c3e-regParam: 0.1\n",
       "}, {\n",
       "\tlinReg_58e54e320c3e-elasticNetParam: 0.7,\n",
       "\tlinReg_58e54e320c3e-regParam: 0.01\n",
       "}, {\n",
       "\tlinReg_58e54e320c3e-elasticNetParam: 0.8,\n",
       "\tlinReg_58e54e320c3e-regParam: 0.1\n",
       "}, {\n",
       "\tlinReg_58e54e320c3e-elasticNetParam: 0.8,\n",
       "\tlinReg_58e54e320c3e-regParam: 0.01\n",
       "}, {\n",
       "\tlinReg_58e54e320c3e-elasticNetParam: 0.9,\n",
       "\tlinReg_58e54e320c3e-regParam: 0.1\n",
       "}, {\n",
       "\tlinReg_58e54e320c3e-elasticNetParam: 0.9,\n",
       "\tlinReg_58e54e320c3e-regParam: 0.01\n",
       "})\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, Array(0.1, 0.01))\n",
    "    .addGrid(lr.elasticNetParam,Array(0.7,0.8, 0.9))\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca793c7",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Specify cross-validation parameters</span>\n",
    "<li>Spark pipelines can do the cross validation in parallel</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "18285dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_c5e3ce0e94d9\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline) \n",
    "  .setEvaluator(new RegressionEvaluator()) //Will try to minimize mean square error\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(3)  // Use at least 3 in practice!\n",
    "  .setParallelism(3)  // Evaluate up to 2 parameter settings in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d5c3da",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Fit the cross validation model</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "32c68714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 10:23:30 WARN BlockManager: Block rdd_258_0 already exists on this machine; not re-adding it\n",
      "22/12/07 10:23:30 WARN BlockManager: Block rdd_258_0 already exists on this machine; not re-adding it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cvModel: org.apache.spark.ml.tuning.CrossValidatorModel = CrossValidatorModel: uid=cv_c5e3ce0e94d9, bestModel=pipeline_20ad1d99528f, numFolds=3\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cvModel = cv.fit(prepareData(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301f9e4",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Get predictions</span>\n",
    "\n",
    "<li>The cross validation model contains the best model found in the grid search</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9c239ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_r: org.apache.spark.sql.DataFrame = [label: double, MedianAge: double ... 11 more fields]\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test_r = cvModel.transform(prepareData(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d75d4",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Get best model evaluation metrics</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "537b2016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_2ffe5920ff1a, metricName=r2, throughOrigin=false\n",
       "rmse: Double = 0.6866377577428902\n",
       "r2: Double = 0.6407799863685091\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  \n",
    "\n",
    "val rmse = evaluator.setMetricName(\"rmse\").evaluate(test_r)\n",
    "val r2 = evaluator.setMetricName(\"r2\").evaluate(test_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc62fef",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Extract the best model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7de7466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1701674396025173,-0.42697526239126027,0.4872216691862174,0.7698920441756001,-0.08262425865397742,0.0,0.1234461715492403,-0.6780525787325298,-0.6117890691324789]\n",
      "2.0744007158850004\n"
     ]
    }
   ],
   "source": [
    "println(cvModel.bestModel.asInstanceOf[PipelineModel]\n",
    "    .stages(2)\n",
    "    .asInstanceOf[LinearRegressionModel]\n",
    "    .coefficients)\n",
    "\n",
    "println(cvModel.bestModel.asInstanceOf[PipelineModel]\n",
    "    .stages(2)\n",
    "    .asInstanceOf[LinearRegressionModel]\n",
    "    .intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fce23d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- MedianAge: double (nullable = true)\n",
      " |-- Population: double (nullable = true)\n",
      " |-- Households: double (nullable = true)\n",
      " |-- MedianIncome: double (nullable = true)\n",
      " |-- RoomsPerHouse: double (nullable = true)\n",
      " |-- PeoplePerHouse: double (nullable = true)\n",
      " |-- BedroomsPerHouse: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaledFeatures: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9411d557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.CrossValidator\n",
       "res44: String => org.apache.spark.ml.param.Param[Any] = $Lambda$6020/0x0000000801df6040@5023bfda\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "cvModel.bestModel.getParam _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55058905",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Get the best scores for each model</span>\n",
    "<li>linear regression defaults to rmse as the metric</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "052a859f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res45: org.apache.spark.ml.evaluation.Evaluator = RegressionEvaluator: uid=regEval_8fd4b9399a8b, metricName=rmse, throughOrigin=false\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.getEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a8c51526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res46: Array[Double] = Array(0.8110491985964897, 0.7042989389165119, 0.8147699859940992, 0.7047059724075506, 0.8189638446724051, 0.704868132797246)\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe22217",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Try a different metric</span>\n",
    "<li>Since we're using regression, the algorithm is the same</li>\n",
    "<li>However, the cross validation model will choose the best model based on the value of the new metric</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0de3f425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_1c4e941b9178\n",
       "cvModel: org.apache.spark.ml.tuning.CrossValidatorModel = CrossValidatorModel: uid=cv_1c4e941b9178, bestModel=pipeline_20ad1d99528f, numFolds=3\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_9fb908b99570, metricName=r2, throughOrigin=false\n",
       "test_r: org.apache.spark.sql.DataFrame = [label: double, MedianAge: double ... 11 more fields]\n",
       "rmse: Double = 0.6866377577428902\n",
       "r2: Double = 0.6407799863685091\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline) \n",
    "  .setEvaluator(new RegressionEvaluator().setMetricName(\"r2\")) //Will try to minimize rmse but will report r2\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(3)  // Use at least 3 in practice!\n",
    "  .setParallelism(3)  // Evaluate up to 2 parameter settings in parallel\n",
    "\n",
    "val cvModel = cv.fit(prepareData(train))\n",
    "\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  \n",
    "val test_r = cvModel.transform(prepareData(test))\n",
    "val rmse = evaluator.setMetricName(\"rmse\").evaluate(test_r)\n",
    "val r2 = evaluator.setMetricName(\"r2\").evaluate(test_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eb6fe25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res47: Array[Double] = Array(0.5075670843870324, 0.6286806429919505, 0.5030403790572823, 0.6282521923184208, 0.4979128859605077, 0.6280815855468848)\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6c2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86f84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
