{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a204a7",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">Spark Transformations</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16645a01",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Map/Reduce in Spark</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e054f249",
   "metadata": {},
   "source": [
    "<li>hadoop map/reduce shuffles data in and out of memory</li>\n",
    "<li>Spark, uses RDDs (minimal data), keeps data in memory (faster)</li>\n",
    "<li>Spark provides powerful <span style=\"color:blue\">by key</span> support for transformations</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e45e3",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">Map and flatMap</span>\n",
    "<br><br>\n",
    "<li>flatMap and map work like Scala map and flatMap</li>\n",
    "<li>Except, lazily</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b375be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://dyn-209-2-224-60.dyn.columbia.edu:4042\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1665090532157)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "x: org.apache.spark.rdd.RDD[Array[String]] = ParallelCollectionRDD[0] at parallelize at <console>:24\n",
       "t1: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at flatMap at <console>:25\n",
       "t2: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:26\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = sc.parallelize(Array(Array(\"Scala\"),Array(\"Spark\")))\n",
    "val t1 = x.flatMap(e => e)\n",
    "val t2 = x.map(e=>e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e23c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35593bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r1: Array[String] = Array(Scala, Spark)\n",
       "r2: Array[Array[String]] = Array(Array(Scala), Array(Spark))\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r1 = t1.collect //Flatmap maps and then flattens\n",
    "val r2 = t2.collect //map only maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec7c48",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">reduce and reduceByKey</span>\n",
    "<br><br>\n",
    "<li><span style=\"color:green\">reduce</span> is an <span style=\"color:red\">action</span> and returns a <b>value (not an RDD)</b>. Reduce works like a scala reduce, except that it works on an RDD</li>\n",
    "<li><span style=\"color:green\">reduceByKey</span> is a <span style=\"color:red\">transformation</span> and returns an <b>RDD (not a value)</b>. reduceByKey works on (key,value) pairs, applies a reduce function on each key independently, and returns an RDD</li>\n",
    "<li>The key in reduceByKey is <span style=\"color:green\">implicit</span></li>\n",
    "<li>reduceByKey returns a <span style=\"color:green\">ShuffledRDD</span>, the shuffle operation is automatically done for you</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4ce5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at <console>:25\n",
       "res0: Int = 36\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val x = sc.parallelize(Array(1,2,3,4,5,6,7,8))\n",
    "x.reduce(_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f481b9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: org.apache.spark.rdd.RDD[(Char, Int)] = ParallelCollectionRDD[10] at parallelize at <console>:25\n",
       "y: org.apache.spark.rdd.RDD[(Char, Int)] = ShuffledRDD[11] at reduceByKey at <console>:26\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = sc.parallelize(Array(('A',6),('B',2),('A',1),('X',4),('B',17)))//array of tuples of two elments:(k,v)\n",
    "val y = x.reduceByKey((a,b) => a + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a061564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[(Char, Int)] = Array((X,4), (A,7), (B,19))\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val y = x.reduceByKey(_+_)\n",
    "y.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cca94c",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">Spark Word Count Example</span>\n",
    "<br><br>\n",
    "<li>We'll redo our word count in Spark</li>\n",
    "<li>Note that in (key,value) terms, the key is a word and the value is its count</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dc5e6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[String] = shakespeare.txt MapPartitionsRDD[13] at textFile at <console>:24\n",
       "words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[14] at flatMap at <console>:25\n",
       "fwords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[15] at filter at <console>:26\n",
       "word_map: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[16] at map at <console>:27\n",
       "result: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[17] at reduceByKey at <console>:28\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = sc.textFile(\"shakespeare.txt\")\n",
    "val words = text.flatMap(line => line.split(\" \"))\n",
    "val fwords = words.filter(l => l.length == 4)\n",
    "val word_map = fwords.map(word => (word,1))  //Construct (key,value) paired RDD - required for reduceByKey\n",
    "val result = word_map.reduceByKey((a,b) => a + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6109847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[(String, Int)] = Array((PETO,2), (ban;,1), (jowl,1), (joy.,20), (bone,7), (pate,19), (Trow,2), (shot,30), (fawn,11), (ENDS,1), (woe!,6), (are.,55), (dole,4), (NYM,,5), (hoof,1), (Oyes,1), (\"Tis,2), (Wake,3), (been,639), (bout,6), (Eve,,3), (rots,1), (iiii,1), (jade,6), (ope,,2), (toe,,2), (lad-,3), (dim.,2), (fowl,7), (nigh,6), (file,16), (so's,1), (O's!,1), (tang,2), (dive,6), (tune,32), (see?,12), (tips,1), (one,,115), (yet,,102), (cart,5), (fit;,5), (plus,2), (mild,19), (shut,45), (morn,19), (Why,,745), (1601,2), (rubs,5), (feu.,1), (Hers,2), (box;,2), (Chop,1), (aims,3), (Iago,17), (snip,2), (knot,18), (rail,22), (dead,170), (men,,116), (robs,6), (Alla,2), ((out,1), (map,,1), (ago!,1), (feil,1), (thus,399), (mad,,43), (dine,19), (hit?,1), (py'r,1), (them,1305), (iron,30)...\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2696c938",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">Comparing scala spark and python</span>\n",
    "<img src=\"scala spark python.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744bdd6",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">Try this!</span>\n",
    "<br><br>\n",
    "<span style=\"font-size:large\">\n",
    "Given a set of scores in quizzes, use map and reduce by key to calculate the total score for each student in the class\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c226d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: Array[(String, String, Int)] = Array((John,Q1,10), (Jill,Q1,8), (John,Q2,3), (Jill,Q2,9))\n",
       "y: org.apache.spark.rdd.RDD[(String, String, Int)] = ParallelCollectionRDD[18] at parallelize at <console>:29\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = Array((\"John\",\"Q1\",10),\n",
    "              (\"Jill\",\"Q1\",8),\n",
    "              (\"John\",\"Q2\",3),\n",
    "              (\"Jill\",\"Q2\",9))\n",
    "val y = sc.parallelize(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c406b3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_scores_by_student: Array[(String, Int)] = Array((John,13), (Jill,17))\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val total_scores_by_student = y.map(x=>(x._1,x._3)).reduceByKey((a,b)=>a+b).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe379c0",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">Example: NYC 311 Data</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "// !wc -l ../../DataAnalytics/DataVisualization/nyc_311_2022_clean.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10ebe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8835630 90365104 1628192091 nyc_311_2022_clean.csv\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wc nyc_311_2022_clean.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "578b8e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8835630 nyc_311_2022_clean.csv\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wc -l nyc_311_2022_clean.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b64288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "// val NYC_Data_Path = \"../../DataAnalytics/DataVisualization/nyc_311_2022_clean.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29696781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.149:4043\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1666119492355)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "NYC_Data_Path: String = nyc_311_2022_clean.csv\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NYC_Data_Path = \"nyc_311_2022_clean.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757bd4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "8835630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "raw_data: org.apache.spark.rdd.RDD[String] = nyc_311_2022_clean.csv MapPartitionsRDD[1] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val raw_data = sc.textFile(NYC_Data_Path)\n",
    "println(raw_data.partitions.length)\n",
    "println(raw_data.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7828196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Date,Closed Date,Agency,Agency Name,Complaint Type,Incident Zip,Borough,Latitude,Longitude,processing_time,processing_days\n",
      "2020-01-07 14:09:00,2020-01-13 11:20:00,DSNY,Department of Sanitation,Electronics Waste Appointment,11692,QUEENS,40.58993519447414,-73.78942049765358,5 days 21:11:00,5.882638888888889\n"
     ]
    }
   ],
   "source": [
    "raw_data.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68252891",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Remove the header row from the data</span>\n",
    "<br>\n",
    "<li><span style=\"color:red\">mapPartitions</span> let's us map a function to each partition. mapPartitionsWithIndex returns the index and an iterator to the partition</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f2328db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_data_nohead: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at mapPartitionsWithIndex at <console>:24\n",
       "r1: Long = 8835630\n",
       "r2: Long = 8835629\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val raw_data_nohead = raw_data.mapPartitionsWithIndex{ (idx,iter) => if (idx==0) iter.drop(1) else iter}\n",
    "val r1 = raw_data.count\n",
    "val r2 = raw_data_nohead.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b4a09",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Complaints by agency</span>\n",
    "<br><br>\n",
    "<li>Let's calculate the number of complaints by agency</li>\n",
    "<li><span style=\"color:red\">countByKey</span> returns a Map object with counts by key given an RDD of (key,value) pairs</li>\n",
    "<li>Construct an (agency,processing time) paired RDD and apply countByKey</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdf44fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[(String, Double)] = Array((DSNY,5.882638888888889), (DSNY,4.070833333333334))\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_nohead\n",
    "    .map(l=>l.split(\",\"))\n",
    "    .map(t => (t(2),t(10).toDouble))\n",
    "    .take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98458ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: scala.collection.Map[String,Long] = Map(DOB -> 273229, DOT -> 583951, DHS -> 59416, OFFICE OF TECHNOLOGY AND INNOVATION -> 2, OSE -> 3, DOF -> 3042, MAYORâS OFFICE OF SPECIAL ENFORCEMENT -> 57511, DOE -> 3672, DPR -> 248101, DOITT -> 503, EDC -> 11897, TLC -> 6035, DFTA -> 6, DCA -> 12623, DEP -> 227815, HPD -> 1233934, DSNY -> 1527321, DOHMH -> 151211, FDNY -> 1, NYPD -> 4435356)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_nohead\n",
    "    .map(l=>l.split(\",\"))\n",
    "    .map(t => (t(2),t(10).toDouble))\n",
    "    .countByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1aa2d29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DOB,273229)\n",
      "(DOT,583951)\n",
      "(DHS,59416)\n",
      "(OFFICE OF TECHNOLOGY AND INNOVATION,2)\n",
      "(OSE,3)\n",
      "(DOF,3042)\n",
      "(MAYORâS OFFICE OF SPECIAL ENFORCEMENT,57511)\n",
      "(DOE,3672)\n",
      "(DPR,248101)\n",
      "(DOITT,503)\n",
      "(EDC,11897)\n",
      "(TLC,6035)\n",
      "(DFTA,6)\n",
      "(DCA,12623)\n",
      "(DEP,227815)\n",
      "(HPD,1233934)\n",
      "(DSNY,1527321)\n",
      "(DOHMH,151211)\n",
      "(FDNY,1)\n",
      "(NYPD,4435356)\n"
     ]
    }
   ],
   "source": [
    "raw_data_nohead\n",
    "    .map(l=>l.split(\",\"))\n",
    "    .map(t => (t(2),t(10).toDouble))\n",
    "    .countByKey.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25210d1",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Total Processing time by agency</span>\n",
    "<br><br>\n",
    "<li>we'll use <span style=\"color:red\">reduceByKey</span> for this</li>\n",
    "<li>and convert processing time in days into processing time in hours</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b86f0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "proc_time_by_agency: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[12] at map at <console>:28\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val proc_time_by_agency = raw_data_nohead\n",
    "                            .map(l=>l.split(\",\"))\n",
    "                            .map(t => (t(2),t(10).toDouble))\n",
    "                            .reduceByKey((a,b)=>a+b)\n",
    "                            .map(t => (t._1,t._2*24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56e4ee1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2.509887933731079 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res4: Array[(String, Double)] = Array((DOE,3828935.48), (DOF,1442816.484444445), (OFFICE OF TECHNOLOGY AND INNOVATION,37.346111111111114), (HPD,3.9216531026861185E8), (MAYORâS OFFICE OF SPECIAL ENFORCEMENT,1.517341014472222E7), (OSE,9.106944444444444), (DOT,2.031404898344442E8), (DCA,943223.69), (FDNY,9651.465555555555), (DSNY,2.5603643572305554E8), (EDC,1.5989842325000033E7), (DOHMH,5.58771262333334E7), (TLC,7775306.0655555595), (DHS,1793047.605277778), (NYPD,3.6147025671944425E7), (DOITT,342750.3583333331), (DEP,2.737430160000001E7), (DPR,3.929524421725001E8), (DFTA,1928.2644444444445), (DOB,2.564403408769446E8))\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "proc_time_by_agency.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d90915",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:large\">Spark tries to keep the data in memory</span>\n",
    "<p>\n",
    "    <li>If we run collect again, it (SHOULD!) run faster</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baae85c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.1536400318145752 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res5: Array[(String, Double)] = Array((DOE,3828935.48), (DOF,1442816.484444445), (OFFICE OF TECHNOLOGY AND INNOVATION,37.346111111111114), (HPD,3.9216531026861185E8), (MAYORâS OFFICE OF SPECIAL ENFORCEMENT,1.517341014472222E7), (OSE,9.106944444444444), (DOT,2.031404898344442E8), (DCA,943223.69), (FDNY,9651.465555555555), (DSNY,2.5603643572305554E8), (EDC,1.5989842325000033E7), (DOHMH,5.58771262333334E7), (TLC,7775306.0655555595), (DHS,1793047.605277778), (NYPD,3.6147025671944425E7), (DOITT,342750.3583333331), (DEP,2.737430160000001E7), (DPR,3.929524421725001E8), (DFTA,1928.2644444444445), (DOB,2.564403408769446E8))\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "proc_time_by_agency.collect\n",
    "//persistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4812f",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:large\">Formatting using Scala match</span>\n",
    "<p>\n",
    "    <li>formatted print of processing time by agency</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82368448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOE\t3828935.48\n",
      "DOF\t1442816.48\n",
      "OFFICE OF TECHNOLOGY AND INNOVATION\t37.35\n",
      "HPD\t392165310.27\n",
      "MAYORâS OFFICE OF SPECIAL ENFORCEMENT\t15173410.14\n",
      "OSE\t9.11\n",
      "DOT\t203140489.83\n",
      "DCA\t943223.69\n",
      "FDNY\t9651.47\n",
      "DSNY\t256036435.72\n",
      "EDC\t15989842.33\n",
      "DOHMH\t55877126.23\n",
      "TLC\t7775306.07\n",
      "DHS\t1793047.61\n",
      "NYPD\t36147025.67\n",
      "DOITT\t342750.36\n",
      "DEP\t27374301.60\n",
      "DPR\t392952442.17\n",
      "DFTA\t1928.26\n",
      "DOB\t256440340.88\n"
     ]
    }
   ],
   "source": [
    "proc_time_by_agency.collect.foreach(t=>t match { case(a,b) => println(f\"$a%s\\t$b%1.2f\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43cf64d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOE\t3828935.48\n",
      "DOF\t1442816.48\n",
      "OFFICE OF TECHNOLOGY AND INNOVATION\t37.35\n",
      "HPD\t392165310.27\n",
      "MAYORâS OFFICE OF SPECIAL ENFORCEMENT\t15173410.14\n",
      "OSE\t9.11\n",
      "DOT\t203140489.83\n",
      "DCA\t943223.69\n",
      "FDNY\t9651.47\n",
      "DSNY\t256036435.72\n",
      "EDC\t15989842.33\n",
      "DOHMH\t55877126.23\n",
      "TLC\t7775306.07\n",
      "DHS\t1793047.61\n",
      "NYPD\t36147025.67\n",
      "DOITT\t342750.36\n",
      "DEP\t27374301.60\n",
      "DPR\t392952442.17\n",
      "DFTA\t1928.26\n",
      "DOB\t256440340.88\n"
     ]
    }
   ],
   "source": [
    "proc_time_by_agency.collect.foreach(t=>t match { case(a,b) => println(f\"$a\\t$b%1.2f\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a2a2d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSE               9.11\n",
      "DOF         1442816.48\n",
      "OFFICE OF        37.35\n",
      "HPD       392165310.27\n",
      "DOT       203140489.83\n",
      "MAYORâS  15173410.14\n",
      "DOE         3828935.48\n",
      "DCA          943223.69\n",
      "DFTA           1928.26\n",
      "FDNY           9651.47\n",
      "TLC         7775306.07\n",
      "DHS         1793047.61\n",
      "NYPD       36147025.67\n",
      "EDC        15989842.33\n",
      "DOHMH      55877126.23\n",
      "DOITT        342750.36\n",
      "DEP        27374301.60\n",
      "DPR       392952442.17\n",
      "DSNY      256036435.72\n",
      "DOB       256440340.88\n"
     ]
    }
   ],
   "source": [
    "proc_time_by_agency.foreach(t=>t match { case(a,b) => {val c = a.slice(0,10); println(f\"$c%-10s$b%12.2f\")}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b921ecf",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:large\">Accessing keys and values</span>\n",
    "<p>\n",
    "        <li>In a (Key,Value) pair RDD, Spark automatically recognizes the first element as the key and the second as the value</li>\n",
    "<li>The attribute <span style=\"color:blue\">keys</span> returns an RDD containing the keys</li>\n",
    "<li>The attribute <span style=\"color:blue\">values</span> returns an RDD containing the values</li>\n",
    "    <li>Note that only tuples of size 2 will work for by key functions</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1cea3c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agency_time_map: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[17] at map at <console>:26\n",
       "keys: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[18] at keys at <console>:27\n",
       "values: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[19] at values at <console>:28\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val agency_time_map = raw_data_nohead\n",
    "                            .map(l=>l.split(\",\"))\n",
    "                            .map(t => (t(2),t(10).toDouble))\n",
    "val keys = agency_time_map.keys\n",
    "val values = agency_time_map.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20069eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res32: Array[String] = Array(DSNY, DSNY, DSNY)\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd0b26b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res36: Array[Double] = Array(5.882638888888889, 4.070833333333334, 1.3104166666666668)\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f59e3",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">aggregate and aggregateByKey</span>\n",
    "<p>\n",
    "<li><span style=\"color:blue\">aggregate</span>: a two stage reducer. In the first stage, a reduce function is applied to each partition separately. In the second stage, a reduce function is applied to combine the results across all paritions</li>\n",
    "        <p>\n",
    "<li><span style=\"color:blue\">aggregateByKey</span>: similar to aggregate but is applied to each key separately</li>\n",
    "<p>\n",
    "<li>Both functions take three arguments</li>\n",
    "    <p>\n",
    "<ol>\n",
    "    <li>an initial value</li>\n",
    "    <li>a function that will be applied to each partition</li>\n",
    "    <li>a function that will accumulate results from each partition</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1706f5ec",
   "metadata": {},
   "source": [
    "<li>Let's compute the average across all students and the average for each student</li>\n",
    "<li>aggregate for the average for all students</li>\n",
    "<li>aggregateByKey for the average for each student</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e208757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total scores: 1478\n",
      "average score: 82.11111111111111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "grades: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[71] at parallelize at <console>:25\n",
       "f1: (Int, (String, Int)) => Int\n",
       "f2: (Int, Int) => Int\n",
       "result: Int = 1478\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val grades = sc.parallelize(List((\"Jack\",74),(\"Jill\",92),(\"Jiahou\",66),(\"Jahangir\",89),(\"Jack\",54),(\"Jahangir\",99),\n",
    "                 (\"Jill\",87),(\"Jack\",76),(\"Jiahou\",95),(\"Jill\",67),(\"Jahangir\",84),(\"Jack\",93),\n",
    "                 (\"Jill\",98),(\"Jahangir\",89),(\"Jiahou\",71),(\"Jack\",65),(\"Jack\",80),(\"Jill\",99)))\n",
    "\n",
    "//f1 accumulates scores in a single partition and returns an Int (total score)\n",
    "//f1's arguments are therefore an Int (the accumulator) and (String, Int) (the data pairs)\n",
    "def f1= (accu:Int, v:(String,Int)) => accu + v._2 \n",
    "\n",
    "//f2 accumulates the result from across all partitions\n",
    "//f2's arguments are an Int (the accumulator) and an Int (the accumulated value in each partition)\n",
    "def f2= (accu1:Int,accu2:Int) => accu1 + accu2\n",
    "\n",
    "val result = grades.aggregate(0)(f1,f2)\n",
    "println(\"total scores: \" + result)\n",
    "println(\"average score: \" + result.toDouble/grades.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b8ab06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res38: Int = 8\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades.partitions.length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90f569",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">aggregateByKey</span>\n",
    "<p>\n",
    "<li>Works just like aggregate but the \"by key\" part makes the key implicit</li>\n",
    "<li>f2 is the same as in the aggregate function (but there will be an implicit key)</li>\n",
    "<li>f1 no longer has a tuple argument (because the key is implicit)</li>\n",
    "        <li>aggregateByKey works on PairRDDs</li>     \n",
    "    <p>\n",
    "        To calculate averages by key we must track the sum and the count for each key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8d13e",
   "metadata": {},
   "source": [
    "<h4>Calculating totals by student</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bdabc21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grades: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[72] at parallelize at <console>:28\n",
       "f1: (Int, Int) => Int\n",
       "f2: (Int, Int) => Int\n",
       "result: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[73] at aggregateByKey at <console>:34\n",
       "res39: Array[(String, Int)] = Array((Jahangir,361), (Jiahou,232), (Jill,443), (Jack,442))\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val grades = sc.parallelize(List((\"Jack\",74),(\"Jill\",92),(\"Jiahou\",66),(\"Jahangir\",89),(\"Jack\",54),(\"Jahangir\",99),\n",
    "                 (\"Jill\",87),(\"Jack\",76),(\"Jiahou\",95),(\"Jill\",67),(\"Jahangir\",84),(\"Jack\",93),\n",
    "                 (\"Jill\",98),(\"Jahangir\",89),(\"Jiahou\",71),(\"Jack\",65),(\"Jack\",80),(\"Jill\",99)))\n",
    "\n",
    "def f1 = (accu:Int, v:Int) => accu + v //f1 automatically looks for the value element for each key\n",
    "def f2 = (accu1:Int,accu2:Int) => accu1 + accu2 //f2 accumulates across partitions for each key separately\n",
    "val result = grades.aggregateByKey(0)(f1,f2) // 0 is initialized to each new key\n",
    "result.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483da5a7",
   "metadata": {},
   "source": [
    "<h4>Calculating averages by student</h4>\n",
    "<li>We'll need to calculate totals as well as counts for each student</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16c9fafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grades: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[77] at parallelize at <console>:28\n",
       "f1: ((Int, Int), Int) => (Int, Int)\n",
       "f2: ((Int, Int), (Int, Int)) => (Int, Int)\n",
       "result: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[79] at map at <console>:40\n",
       "res41: Array[(String, Double)] = Array((Jahangir,90.25), (Jiahou,77.33333333333333), (Jill,88.6), (Jack,73.66666666666667))\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val grades = sc.parallelize(List((\"Jack\",74),(\"Jill\",92),(\"Jiahou\",66),(\"Jahangir\",89),(\"Jack\",54),(\"Jahangir\",99),\n",
    "                 (\"Jill\",87),(\"Jack\",76),(\"Jiahou\",95),(\"Jill\",67),(\"Jahangir\",84),(\"Jack\",93),\n",
    "                 (\"Jill\",98),(\"Jahangir\",89),(\"Jiahou\",71),(\"Jack\",65),(\"Jack\",80),(\"Jill\",99)))\n",
    "\n",
    "//The f1 accumulator tracks both the total and the count\n",
    "def f1= (accu:(Int,Int), v:Int) => (accu._1 + v,accu._2+1) //f1 automatically looks for the value element\n",
    "\n",
    "//f2 adds up the totals and the counts from each partition into a tuple\n",
    "def f2= (accu1:(Int,Int),accu2:(Int,Int)) => (accu1._1 + accu2._1,accu1._2+accu2._2) //f2 accumulates across partitions for each key separately\n",
    "\n",
    "//Divide the total by the count for each key to get averages\n",
    "val result = grades.aggregateByKey((0,0))(f1,f2)\n",
    "    .map(r=>(r._1,r._2._1*1.0/r._2._2))\n",
    "\n",
    "result.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "08f76431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res42: org.apache.spark.rdd.RDD[(String, (Int, Int))] = ShuffledRDD[80] at aggregateByKey at <console>:28\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades.aggregateByKey((0,0))(f1,f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b82d2",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">groupByKey</span>\n",
    "<p>\n",
    "<li>groups the data by key</li>\n",
    "<li>similar to python's group by</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57dfa40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grades: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[13] at parallelize at <console>:24\n",
       "temp: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[14] at groupByKey at <console>:27\n",
       "total_scores: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[15] at map at <console>:28\n",
       "res43: Array[(String, Int)] = Array((Jahangir,361), (Jiahou,232), (Jill,443), (Jack,442))\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val grades = sc.parallelize(List((\"Jack\",74),(\"Jill\",92),(\"Jiahou\",66),(\"Jahangir\",89),(\"Jack\",54),(\"Jahangir\",99),\n",
    "                 (\"Jill\",87),(\"Jack\",76),(\"Jiahou\",95),(\"Jill\",67),(\"Jahangir\",84),(\"Jack\",93),\n",
    "                 (\"Jill\",98),(\"Jahangir\",89),(\"Jiahou\",71),(\"Jack\",65),(\"Jack\",80),(\"Jill\",99)))\n",
    "val temp = grades.groupByKey\n",
    "val total_scores = temp.map(l => (l._1,l._2.reduce((x,y)=>x+y)))\n",
    "total_scores.collect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572552f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(String, Iterable[Int])] = Array((Jahangir,CompactBuffer(89, 99, 84, 89)), (Jiahou,CompactBuffer(66, 95, 71)), (Jill,CompactBuffer(92, 87, 67, 98, 99)), (Jack,CompactBuffer(74, 54, 76, 93, 65, 80)))\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414dc9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CompactBuffer // scala object, mutable array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76102818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: Array[Array[Int]] = Array(Array(1, 2), Array(3, 4))\n",
       "y: org.apache.spark.rdd.RDD[Array[Int]] = ParallelCollectionRDD[3] at parallelize at <console>:25\n",
       "res4: Array[Array[Int]] = Array(Array(1, 2), Array(3, 4))\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = Array(Array(1,2),Array(3,4))\n",
    "val y = sc.parallelize(x)\n",
    "y.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0118cd97",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">combineByKey</span>\n",
    "<p>\n",
    "    <li>groupBy and groupByKey operations are expensive</li>\n",
    "    <li>because grouping needs to be done on the entire data and can't be broken up at the partition level</li>\n",
    "    <li><span style=\"color:red\">combineByKey</span> data at partition level (by key) and then combines results across partitions (also by key)</li>\n",
    "    <p>\n",
    "        \n",
    "<li>three function arguments</li>\n",
    "<li><b>Combiner</b>: Creates an accumulator (e.g. (key,1)) for each unseen key in a partition</li>\n",
    "<li><b>Merger</b>: Merges values of \"seen keys\" into the accumulator in a partition</li>\n",
    "<li><b>Merge Combiner</b>: Merges same key values across partitions</li>\n",
    "<li>A more general version of aggregateByKey (the initial value is replaced by a function) </li>\n",
    "<li><b>Notice that the key is implicit in the entire operation!</b></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab2c6e",
   "metadata": {},
   "source": [
    "<img src=\"combineByKey.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c9d2b1",
   "metadata": {},
   "source": [
    "<br><br><span style=\"color:blue;font-size:x-large\">Average by key using combineByKey</span>\n",
    "<br>\n",
    "<li><span style=\"color:red\">combiner</span>: a function that initializes the accumulator in a single partition. The combiner is called when combineByKey sees a key for the first time</li>\n",
    "<li><span style=\"color:red\">merger</span>: a function that updates the accumulator</li>\n",
    "<li><span style=\"color:red\">mergeAndCombiner</span>: A function that merges the accumulator from two partitions</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0da364",
   "metadata": {},
   "outputs": [],
   "source": [
    "// no shuffle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf159a",
   "metadata": {},
   "source": [
    "<img src=\"combiner.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83d38873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "combiner: Double => (Int, Double) = $Lambda$3772/0x00000008013f2840@7a5d4dd4\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Initializes a new key to a count of 1 and a total of the value of the new key\n",
    "val combiner = (x: Double) => (1,x) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ab3393",
   "metadata": {},
   "source": [
    "<img src=\"merger.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d7e0c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "merger: ((Int, Double), Double) => (Int, Double) = $Lambda$3828/0x00000008013bd840@17e290cb\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//update the accumulator by adding 1 to the count and adding the new value to the running total (for a key)\n",
    "val merger = (x: (Int, Double),y: Double) => {\n",
    "    val (c,acc) = x\n",
    "    (c+1, acc + y)\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4d35d7",
   "metadata": {},
   "source": [
    "<img src=\"combiner_and_merger.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde87ff6",
   "metadata": {},
   "source": [
    "<img src=\"merge_and_combiner.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "823c6b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mergeAndCombiner: ((Int, Double), (Int, Double)) => (Int, Double) = $Lambda$3829/0x00000008013bc040@43ca9172\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mergeAndCombiner = (x1: (Int, Double), x2: (Int, Double)) => {\n",
    "    val (c1, acc1) = x1\n",
    "    val (c2, acc2) = x2\n",
    "    (c1+c2,acc1+acc2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a502685f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res47: Array[(String, (Int, Double))] = Array((DOE,(3672,159538.97833333333)), (DOF,(3042,60117.35351851855)), (OFFICE OF TECHNOLOGY AND INNOVATION,(2,1.556087962962963)), (HPD,(1233934,1.634022126119216E7)), (MAYORâS OFFICE OF SPECIAL ENFORCEMENT,(57511,632225.4226967591)), (OSE,(3,0.3794560185185185)), (DOT,(583951,8464187.076435175)), (DCA,(12623,39300.98708333333)), (FDNY,(1,402.1443981481481)), (DSNY,(1527321,1.066818482179398E7)), (EDC,(11897,666243.4302083347)), (DOHMH,(151211,2328213.5930555584)), (TLC,(6035,323971.086064815)), (DHS,(59416,74710.31688657409)), (NYPD,(4435356,1506126.069664351)), (DOITT,(503,14281.264930555546)), (DEP,(227815,1140595.9000000004)), (DPR,(248101,1.637301842385417E7)), (DFTA,(6,80.34435185185185)), (DOB,(273229,1.0685014203206025E7)))\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val combiner = (x: Double) => (1,x) \n",
    "val merger = (x: (Int, Double),y: Double) => {\n",
    "    val (c,acc) = x\n",
    "    (c+1, acc + y)\n",
    "}\n",
    "val mergeAndCombiner = (x1: (Int, Double), x2: (Int, Double)) => {\n",
    "    val (c1, acc1) = x1\n",
    "    val (c2, acc2) = x2\n",
    "    (c1+c2,acc1+acc2)\n",
    "}\n",
    "agency_time_map.combineByKey(combiner,merger,mergeAndCombiner).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45bace",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:blue;font-size:large\">Calculating the average</span>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d40d7053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res49: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[25] at map at <console>:31\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agency_time_map\n",
    "    .combineByKey(combiner,merger,mergeAndCombiner) //Get counts and totals\n",
    "    .map(t => (t._1,t._2._2.toDouble/t._2._1))   //average by dividing totals by counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8809936f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res54: Array[(String, Double)] = Array((DOE,43.447434186637615), (DOF,19.76244362870432), (OFFICE OF TECHNOLOGY AND INNOVATION,0.7780439814814815), (HPD,13.242378653308977), (MAYORâS OFFICE OF SPECIAL ENFORCEMENT,10.993121710572918), (OSE,0.12648533950617283), (DOT,14.494687185115147), (DCA,3.1134426905912487), (FDNY,402.1443981481481), (DSNY,6.984900241530092), (EDC,56.00096076391819), (DOHMH,15.397117888616293), (TLC,53.682035801957745), (DHS,1.257410746037668), (NYPD,0.33957275800732817), (DOITT,28.392176800309237), (DEP,5.006676030990059), (DPR,65.99335925229713), (DFTA,13.390725308641976), (DOB,39.10644259286542))\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agency_time_map\n",
    "    .combineByKey(combiner,merger,mergeAndCombiner) //Get counts and totals\n",
    "    .map(t => (t._1,t._2._2/t._2._1))   //average by dividing totals by counts\n",
    "    .collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "91abfa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2.0663950443267822 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res53: Array[(String, Double)] = Array((DOE,43.447434186637615), (DOF,19.76244362870432), (OFFICE OF TECHNOLOGY AND INNOVATION,0.7780439814814815), (HPD,13.242378653308977), (MAYORâS OFFICE OF SPECIAL ENFORCEMENT,10.993121710572918), (OSE,0.12648533950617283), (DOT,14.494687185115147), (DCA,3.1134426905912487), (FDNY,402.1443981481481), (DSNY,6.984900241530092), (EDC,56.00096076391819), (DOHMH,15.397117888616293), (TLC,53.682035801957745), (DHS,1.257410746037668), (NYPD,0.33957275800732817), (DOITT,28.392176800309237), (DEP,5.006676030990059), (DPR,65.99335925229713), (DFTA,13.390725308641976), (DOB,39.10644259286542))\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "agency_time_map\n",
    "    .combineByKey(combiner,merger,mergeAndCombiner) //Get counts and totals\n",
    "    .map(t => (t._1,t._2._2.toDouble/t._2._1))   //average by dividing totals by counts\n",
    "    .collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b306bf61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultRDD: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[37] at map at <console>:29\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val resultRDD = agency_time_map\n",
    "    .combineByKey(combiner,merger,mergeAndCombiner) //Get counts and totals\n",
    "    .map(t => (t._1,t._2._2.toDouble/t._2._1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dab28e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2.3064072132110596 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res55: Array[(String, Double)] = Array((DOE,43.447434186637615), (DOF,19.76244362870432), (OFFICE OF TECHNOLOGY AND INNOVATION,0.7780439814814815), (HPD,13.242378653308977), (MAYORâS OFFICE OF SPECIAL ENFORCEMENT,10.993121710572918), (OSE,0.12648533950617283), (DOT,14.494687185115147), (DCA,3.1134426905912487), (FDNY,402.1443981481481), (DSNY,6.984900241530092), (EDC,56.00096076391819), (DOHMH,15.397117888616293), (TLC,53.682035801957745), (DHS,1.257410746037668), (NYPD,0.33957275800732817), (DOITT,28.392176800309237), (DEP,5.006676030990059), (DPR,65.99335925229713), (DFTA,13.390725308641976), (DOB,39.10644259286542))\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "resultRDD.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f57f2",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">folding in Spark</span>\n",
    "<p>\n",
    "    <li><b>Important</b>Note that these are Spark API functions that share the same name as Scala functions. They are not the scala functions!</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e60223",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:blue;font-size:large\">fold</span>\n",
    "<p>\n",
    "    <li>fold is an <span style=\"color:red\">action</span> on an RDD</li>\n",
    "    <li>similar to scala fold (initial value + function)</li>\n",
    "    <p>\n",
    "        <span style=\"color:red\">Example</span>: find a student with the highest score in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6594408b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grades: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[26] at parallelize at <console>:26\n",
       "start: (String, Int) = (xyz,0)\n",
       "highest: (String, Int) = (Jahangir,99)\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val grades = sc.parallelize(List((\"Jack\",74),(\"Jill\",92),(\"Jiahou\",66),(\"Jahangir\",89),(\"Jack\",54),(\"Jahangir\",99),\n",
    "                 (\"Jill\",87),(\"Jack\",76),(\"Jiahou\",95),(\"Jill\",67),(\"Jahangir\",84),(\"Jack\",93),\n",
    "                 (\"Jill\",98),(\"Jahangir\",89),(\"Jiahou\",71),(\"Jack\",65),(\"Jack\",80),(\"Jill\",99)))\n",
    "\n",
    "val start = (\"xyz\",0)\n",
    "val highest = grades.fold(start)((acc,score) => {\n",
    "    if (acc._2 <= score._2) score else acc\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a7b74",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:blue;font-size:large\">foldByKey</span>\n",
    "<p>\n",
    "    <li>Like fold, but works on keys and on RDDs</li>\n",
    "    <li>Example: What is the highest score for each student</li>\n",
    "    <li>Note that, below, the key is implicit</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99603829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[27] at foldByKey at <console>:26\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades.foldByKey(0)((acc,score) => if (acc < score) score else acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d0fe60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Array[(String, Int)] = Array((Jahangir,99), (Jiahou,95), (Jill,99), (Jack,93))\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades.foldByKey(0)((acc,score) => if (acc < score) score else acc).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a099138",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">foldByKey vs reduceByKey vs combineByKey</span>\n",
    "<p>\n",
    "            <li>The underlying implementation of the three byKey operations is more or less the same</li>\n",
    "        <li>reduceByKey calls combineByKey. Use combineByKey when you want more control over what happens in partitions</li>\n",
    "        <li>use reduceByKey when you want code simplicity and control over what happens in partitions doesn't help</li>\n",
    "        <li>foldByKey also calls combineByKey and is essentially reduceByKey with the ability to set the initial value. Use foldByKey when you want to set the initial value</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "446a26ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Array[(String, Int)] = Array((Jahangir,99), (Jiahou,95), (Jill,99), (Jack,93))\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades.reduceByKey((a,b) => if (a<b) b else a).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0666e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Array[(String, Int)] = Array((Jahangir,99), (Jiahou,95), (Jill,99), (Jack,93))\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades.combineByKey(((a: Int) => a),((a: Int, b: Int) => if (a<b) b else a),((a: Int,b: Int) => if (a<b) b else a)).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db95efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
