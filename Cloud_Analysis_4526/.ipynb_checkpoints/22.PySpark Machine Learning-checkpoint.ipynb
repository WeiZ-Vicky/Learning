{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd24bf1",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Set up PySpark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark ML Basics\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1343243",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Vectors</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "x = np.array([3.2,0,0,0,4.7,1.6,0,0,0,0,10.2,0,0,11.1])\n",
    "data_dense = Vectors.dense(x) \n",
    "data_sparse = Vectors.sparse(14,np.array([0,4,5,10,13]),np.array([3.2,4.7,1.6,10.2,11.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c3b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca6531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Vectors.norm(data_dense,1)) #returns the p=1 norm (taxicab)\n",
    "print(Vectors.norm(data_dense,2)) #returns the euclidean norm\n",
    "print(Vectors.squared_distance(data_dense,data_sparse)) #distance between two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad8565",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Matrices</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea26ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.linalg import Matrices\n",
    "\n",
    "data = np.array([1.0, 0.0, 4.0, 0.0, 3.0, 5.0, 2.0, 0.0, 6.0])\n",
    "dense_m = Matrices.dense(3,3,data)\n",
    "sparse_m = dense_m.toSparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d34c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93dcd18",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<h2 style=\"color:red;font-size:50px\">feature transformers</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca5862",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Vector Assembler</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376df370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "  (22.0, 23.1,3),\n",
    "  (12.2, 13.0,2),\n",
    "  (43.7, 16.2,4),\n",
    "  (36.4, 34.8,3),\n",
    "  (6.1, 71.0,3),\n",
    "  (28.2, 22.1,7)\n",
    "]).toDF(\"feature1\", \"feature2\",\"dv\")\n",
    "\n",
    "#Create an assembler object identifying the columns that need to be vectorized\n",
    "assembler = VectorAssembler()\\\n",
    "  .setInputCols([\"feature1\",\"feature2\"])\\\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "#Call transform on the dataframe. This creates a new dataframe using the specifications\n",
    "#(specs = which columns to keep)\n",
    "#by default, spark ml models assume the dv is in a column called label and the iv in a column called features\n",
    "df_lr = assembler.transform(df)\\\n",
    "    .select(\"dv\",\"features\")\\\n",
    "    .withColumnRenamed(\"dv\",\"label\")\n",
    "\n",
    "df_lr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019911f5",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:green;font-size:xx-large\">StringIndexer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4491a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "  (\"MIA\", 17.2,2),  \n",
    "  (\"NYC\", 23.1,3),\n",
    "  (\"SFO\", 13.0,2),\n",
    "  (\"NYC\", 16.2,4),\n",
    "  (\"CHI\", 34.8,3),\n",
    "  (\"SFO\", 71.0,3),\n",
    "  (\"SFO\", 22.12,6),\n",
    "  (\"LAX\", 22.1,7)\n",
    "]).toDF(\"feature1\", \"feature2\",\"dv\")\n",
    "\n",
    "#Create a StringIndexer object with the column specifications\n",
    "indexer = StringIndexer()\\\n",
    "  .setInputCol(\"feature1\")\\\n",
    "  .setOutputCol(\"feature1Index\")\n",
    "\n",
    "#The \"fit\" operation determines the category to number relationship\n",
    "#The \"transform\" operation does the actual assigning of values\n",
    "indexed = indexer\\\n",
    "                .fit(df)\\\n",
    "                .transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072318d",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">One Hot Encoding</span>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d577795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (\"Jack\",\"A\",\"IEOR\"),\n",
    "    (\"Jill\",\"B\",\"IEOR\"),\n",
    "    (\"Jiahuo\",\"A\",\"CS\"),\n",
    "    (\"Pierre\",\"C\",\"APAM\"),\n",
    "    (\"Clemence\",\"B\",\"APAM\"),\n",
    "    (\"Savitri\",\"A\",\"CS\"),\n",
    "    (\"Bjorn\",\"A\",\"QMSS\")]).toDF(\"student\",\"grade\",\"department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc8f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "indexer = StringIndexer()\\\n",
    "  .setInputCols((\"grade\",\"department\"))\\\n",
    "  .setOutputCols((\"gradeIndex\",\"departmentIndex\"))\n",
    "\n",
    "#The \"fit\" operation determines the category to number relationship\n",
    "#The \"transform\" operation does the actual assigning of values\n",
    "indexedDf = indexer\\\n",
    "                .fit(df)\\\n",
    "                .transform(df)\n",
    "indexedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\\\n",
    "  .setInputCols((\"gradeIndex\", \"departmentIndex\"))\\\n",
    "  .setOutputCols((\"gradeVec\", \"departmentVec\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoder.fit(indexedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f006cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.transform(indexedDf)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d1d8a4",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">toPandas</span>\n",
    "\n",
    "<li>Handy function that converts a Spark DF into a Pandas DF</li>\n",
    "<li>But, beware, a pandas dataframe is not lazy. Only use this when you're sure that the dataframe is small enough</li>\n",
    "<li>toArray() converts a vector into a numpy array</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295cf433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame((\n",
    "  (\"MIA\", 17.2,2,4),  \n",
    "  (\"NYC\", 23.1,3,4),\n",
    "  (\"SFO\", 13.0,2,2),\n",
    "  (\"NYC\", 16.2,1,1),\n",
    "  (\"CHI\", 34.8,3,7),\n",
    "  (\"SFO\", 71.0,3,6),\n",
    "  (\"SFO\", 22.12,3,3),  \n",
    "  (\"LAX\", 22.1,2,4)\n",
    ")).toDF(\"feature1\", \"feature2\",\"feature3\",\"dv\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc017f7",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<h2 style=\"color:red;font-size:50px\">Example: California home values</h2>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acae5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\",\"false\")\\\n",
    "        .option(\"inferschema\",\"true\")\\\n",
    "        .load(\"../Module06-sparkstreaming/cal_housing.data\")\\\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\\\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\\\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a543d",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Setting up the dependent variable</span>\n",
    "<li>We'll simplify the median home value by dividing it by 100,000\n",
    "<li>With PySpark, we can't use the dollar sign to represent a column and need to explicitly use col</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\",\"false\")\\\n",
    "        .option(\"inferschema\",\"true\")\\\n",
    "        .load(\"../Module06-sparkstreaming/cal_housing.data\")\\\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\\\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\\\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\\\n",
    "        .withColumn(\"MedianHomeValue\",col(\"MedianHomeValue\")/100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0841e5e9",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Setting up independent variables</span>\n",
    "<li>We'll divide total rooms, total bedrooms, and population by the number of households to get per household data</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\",\"false\")\\\n",
    "        .option(\"inferschema\",\"true\")\\\n",
    "        .load(\"../Module06-sparkstreaming/cal_housing.data\")\\\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\\\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\\\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\\\n",
    "        .withColumn(\"MedianHomeValue\",col(\"MedianHomeValue\")/100000)    \\\n",
    "    .withColumn(\"RoomsPerHouse\", col(\"TotalRooms\")/col(\"Households\")) \\\n",
    "    .withColumn(\"PeoplePerHouse\", col(\"Population\")/col(\"Households\")) \\\n",
    "    .withColumn(\"BedroomsPerHouse\", col(\"TotalBedrooms\")/col(\"Households\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2638354a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Select the features we need</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16545b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\",\"false\")\\\n",
    "        .option(\"inferschema\",\"true\")\\\n",
    "        .load(\"../Module06-sparkstreaming/cal_housing.data\")\\\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\\\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\\\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\\\n",
    "        .withColumn(\"MedianHomeValue\",col(\"MedianHomeValue\")/100000)    \\\n",
    "    .withColumn(\"RoomsPerHouse\", col(\"TotalRooms\")/col(\"Households\")) \\\n",
    "    .withColumn(\"PeoplePerHouse\", col(\"Population\")/col(\"Households\")) \\\n",
    "    .withColumn(\"BedroomsPerHouse\", col(\"TotalBedrooms\")/col(\"Households\"))\\\n",
    "    .select(\"MedianHomeValue\", \n",
    "              \"MedianAge\", \n",
    "              \"Population\", \n",
    "              \"Households\", \n",
    "              \"MedianIncome\", \n",
    "              \"RoomsPerHouse\", \n",
    "              \"PeoplePerHouse\", \n",
    "              \"BedroomsPerHouse\",\n",
    "               \"Latitude\",\n",
    "               \"Longitude\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2af5c",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;font-size:xx-large\">Machine Learning Pipelines</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee9d94f",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Read the data from a file and split into train and test</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def readData(): \n",
    "    df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\",\"false\")\\\n",
    "        .option(\"inferschema\",\"true\")\\\n",
    "        .load(\"../Module06-sparkstreaming/cal_housing.data\")\\\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\\\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\\\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\n",
    "    train,test = df.randomSplit((0.8,0.2),seed=1234)\n",
    "    return train,test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = readData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c7c80e",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Do the preprocessing steps</span>\n",
    "<li>train and test can be separately passed through this function</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "def prepareData(df):\n",
    "    return df.withColumn(\"MedianHomeValue\",col(\"MedianHomeValue\")/100000)\\\n",
    "        .withColumn(\"RoomsPerHouse\", col(\"TotalRooms\")/col(\"Households\"))\\\n",
    "        .withColumn(\"PeoplePerHouse\", col(\"Population\")/col(\"Households\"))\\\n",
    "        .withColumn(\"BedroomsPerHouse\", col(\"TotalBedrooms\")/col(\"Households\"))\\\n",
    "        .select(\"MedianHomeValue\", \\\n",
    "                  \"MedianAge\", \\\n",
    "                  \"Population\", \\\n",
    "                  \"Households\", \\\n",
    "                  \"MedianIncome\", \\\n",
    "                  \"RoomsPerHouse\", \\\n",
    "                  \"PeoplePerHouse\", \\\n",
    "                  \"BedroomsPerHouse\",\\\n",
    "                   \"Latitude\",\\\n",
    "                   \"Longitude\")\\\n",
    "        .withColumnRenamed(\"MedianHomeValue\",\"label\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab653f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepareData(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68110a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Vector Assembler</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ff0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(filter(lambda l : l if (l != \"label\")  else None,prepareData(train).columns))\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0847df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Matrix\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "#Get the names of all columns except MedianHomeValue (label)\n",
    "cols = list(filter(lambda l : l if (l != \"label\")  else None,prepareData(train).columns))\n",
    "\n",
    "\n",
    "\n",
    "#Create a vectorassembler from the list of columns and specify the name of the column of vectors\n",
    "assembler = VectorAssembler()\\\n",
    "  .setInputCols(cols)\\\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "#Apply the transform function on the data frame, select the dv and features column\n",
    "#And rename the dv column to label\n",
    "\n",
    "vector_df = assembler.transform(prepareData(train))\\\n",
    "    .select(\"label\",\"features\")\n",
    "\n",
    "vector_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f203b7",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Scaling</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add3e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler()\\\n",
    "      .setInputCol(\"features\")\\\n",
    "      .setOutputCol(\"scaledFeatures\")\\\n",
    "      .setWithStd(True)\\\n",
    "      .setWithMean(True)\n",
    "\n",
    "#Generate the parameters (fit the scaling object to the data)\n",
    "fitted_scaler = scaler.fit(vector_df)\n",
    "\n",
    "#scale the data\n",
    "scaled_df = fitted_scaler.transform(vector_df)\n",
    "scaled_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b28c0d",
   "metadata": {},
   "source": [
    "<h3>Regression model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b762e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\\\n",
    "    .setMaxIter(10)\\\n",
    "    .setRegParam(0.3) \\\n",
    "    .setElasticNetParam(0.8) \\\n",
    "    .setFeaturesCol(\"scaledFeatures\") \\\n",
    "    .setLabelCol(\"label\") \n",
    "\n",
    "lrModel = lr.fit(scaled_df) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d868e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf11a4f",
   "metadata": {},
   "source": [
    "<h2>The pipeline</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd7bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "pipeline = Pipeline().setStages((assembler,scaler,lr))\n",
    "model = pipeline.fit(prepareData(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084af51",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Model evaluation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc2931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator()\\\n",
    "  .setLabelCol(\"label\")\\\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(prepareData(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2880957",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_test = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "r2_test = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "print(\"Test  RMSE: \",rmse_test,\" Test  r2: \",r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4459e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "lrModel = model.stages[2]\n",
    "print(\"Coefficients: \",lrModel.coefficients)\n",
    "print(\"Intercept: \",lrModel.intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46348d8",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Hyperparameter tuning</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbfa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, (0.1, 0.01))\\\n",
    "    .addGrid(lr.elasticNetParam,(0.7,0.8, 0.9))\\\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a33342",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator()\\\n",
    "    .setEstimator(pipeline)\\\n",
    "    .setEvaluator(RegressionEvaluator())\\\n",
    "    .setEstimatorParamMaps(paramGrid)\\\n",
    "    .setNumFolds(3)\\\n",
    "    .setParallelism(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316387da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(prepareData(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33435d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r = cvModel.transform(prepareData(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5046a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator()\\\n",
    "  .setLabelCol(\"label\")\\\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  \n",
    "\n",
    "rmse = evaluator.setMetricName(\"rmse\").evaluate(test_r)\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(test_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvModel.bestModel\\\n",
    "    .stages[2]\\\n",
    "    .coefficients)\n",
    "\n",
    "print(cvModel.bestModel\n",
    "    .stages[2]\n",
    "    .intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13811543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
