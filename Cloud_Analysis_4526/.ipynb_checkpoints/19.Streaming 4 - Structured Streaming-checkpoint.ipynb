{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">Structured streaming</span>\n",
    "<br><br>\n",
    "<li>Scalable, fault tolerant, stream processing engine built on Spark SQL </li>\n",
    "<li>Stream data is maintained in dataframe tables</li>\n",
    "<li>Tables are <b>unbounded</b> and grow with each new batch arrival</li>\n",
    "<li>Batches are similar to DStream batches but can be made \"almost continuous\" 1 nanosecond batches (this is experimental as of now)</li>\n",
    "<li>Tables are structured as dataframes with two columns <span style=\"color:blue\">value</span> which contains the data and <span style=\"color:blue\">timestamp</span> which contains the timestamp associated with the microbatch</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">the dataframe</span>\n",
    "<br><br>\n",
    "<li>the dataframe needs some context information to be set up</li>\n",
    "<li>the stream source (we will use a <span style=\"color:blue\">socket</span> source)</li>\n",
    "<li>the url and port associated with the socket (<span style=\"color:blue\">localhost, 4444</span>)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lines = spark\n",
    "    .readStream //Creates a readable stream\n",
    "    .format(\"socket\") //We will read it from a socket\n",
    "    .option(\"host\", \"localhost\") //the host\n",
    "    .option(\"port\", 4444) //the port\n",
    "    .option(\"includeTimestamp\", \"true\") //true if we want the event time stamp\n",
    "    .load() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>from the dataframe, get the data</li>\n",
    "<li>we will do a word count for all the words that arrive through the socket</li>\n",
    "<li>then group them and count them</li>\n",
    "<li>note that this will count all the words that ever arrived at the socket!</li>\n",
    "<li>note also that we haven't yet started the stream, this is just what we want to do</li>\n",
    "<li><span style=\"color:red\">value</span>: Extract the value column from the dataframe</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val words = lines.select(\"value\").as[String].flatMap(_.split(\" \"))\n",
    "val counts = words.groupBy(\"value\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Initiate the stream</span>\n",
    "<li>Above, we've created a program that creates a dataframe and counts the words in that dataframe</li>\n",
    "<li>This returns an <span style=\"color:blue\">unbounded table</span> that is constantly updated as each batch arrives</li>\n",
    "<li>We need to apply a query to this table</li>\n",
    "<li>Each time a new line is added to the table, the query will run</li>\n",
    "<li>The query does something on the dataframe, sets the batch size, and starts listening on the stream</li>\n",
    "<li>Note that the lines, words, counts tables are not retained from batch to batch</li>\n",
    "<li>the query is processed, and the minimum data required to recreate it is retained</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">writeStream</span>\n",
    "<li>the data stream writer - applies the program to the stream</li>\n",
    "<li><span style=\"color:blue\">outputMode</span>: What to output (entire table/new data/etc.)</li>\n",
    "<ul>\n",
    "    <li>use <span style=\"color:blue\">complete</span> to see all results from all batches at each time point</li>\n",
    "    <li>use <span style=\"color:blue\">append</span> when using watermarks (see event time handling below)</li>\n",
    "</ul>\n",
    "<li><span style=\"color:red\">Note</span>: Make sure you initiate the stream before running the query below!</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Trigger</span>\n",
    "<br>\n",
    "<li>The <span style=\"color:red\">trigger</span> defines the batch size. In the below example, the batch size is set to 10 seconds</li>\n",
    "<li>If the trigger is omitted, the batch size drops to 1 nano second and we get, essentially, record-at-a-time continuous streaming</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//For setting batch sizes\n",
    "\n",
    "//For defining a time unit\n",
    "import java.util.concurrent.TimeUnit\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "val query = counts\n",
    "    .orderBy(\"count\")\n",
    "    .writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    ".start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Stop the stream</span>\n",
    "<br><br>\n",
    "<li>Note: Once the stream stops, you cannot access the data any more</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Continuous streaming</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val query = counts.orderBy(\"count\")\n",
    "    .writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Window operations</span>\n",
    "<br><br>\n",
    "<li>Window operations use the <span style=\"color:red\">window</span> function</li>\n",
    "<li>arguments:</li>\n",
    "<ul>\n",
    "    <li><span style=\"color:red\">time column</span>: a dataframe column that contains a timestamp object</li>\n",
    "    <li><span style=\"color:red\">window duration</span>: the window length</li>\n",
    "    <li><span style=\"color:red\">slide length</span> (optional): if provided, then the window is a sliding window. If not, it is a fixed (tumbling) window</li>\n",
    "</ul>\n",
    "<li>Group the dataframe by window and word and generate word counts</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.sql.Timestamp\n",
    "\n",
    "val lines = spark\n",
    "    .readStream //Creates a readable stream\n",
    "    .format(\"socket\") //We will read it from a socket\n",
    "    .option(\"host\", \"localhost\") //the host\n",
    "    .option(\"port\", 4444) //the port\n",
    "    .option(\"includeTimestamp\", \"true\") //true if we want the time stamp\n",
    "    .load() \n",
    "\n",
    "val words = lines\n",
    "    .as[(String, Timestamp)] //Creates a Dataset of (value,timestamp)\n",
    "    .flatMap(line => line._1.\n",
    "             split(\" \").\n",
    "             map(word => (word, line._2))) //Creates (word,timestamp) pairs for each word\n",
    "    .toDF(\"word\", \"timestamp\") //back to a dataframe\n",
    "\n",
    "val windowedCounts = words.groupBy(\n",
    "  window($\"timestamp\", \"10 seconds\", \"10 seconds\"),\n",
    "  $\"word\"\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query = windowedCounts.writeStream\n",
    " .outputMode(\"complete\")  \n",
    " .format(\"console\")\n",
    " .option(\"truncate\", \"false\")\n",
    " .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Output modes</span>\n",
    "<br><br>\n",
    "<li><span style=\"color:red\">complete</span>: Send complete results to the sink. Complete, for word count, will return the total number of occurrences of each word from stream start to now</li>\n",
    "<li><span style=\"color:red\">update</span>: only report the data that has changed. For word count, the total number of words from stream start to now will be reported but only for the words for which the total has changed</li>\n",
    "<li><span style=\"color:red\">append</span>: only new data added will be sent to the sink. Append only works with watermarks or with non-aggregatio queries</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query = windowedCounts.writeStream\n",
    " .outputMode(\"update\")  \n",
    " .format(\"console\")\n",
    " .option(\"truncate\", \"false\")\n",
    " .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.sql.Timestamp\n",
    "\n",
    "val lines = spark\n",
    "    .readStream //Creates a readable stream\n",
    "    .format(\"socket\") //We will read it from a socket\n",
    "    .option(\"host\", \"localhost\") //the host\n",
    "    .option(\"port\", 4444) //the port\n",
    "    .option(\"includeTimestamp\", \"true\") //true if we want the time stamp\n",
    "    .load() \n",
    "\n",
    "val words = lines\n",
    "    .as[(String, Timestamp)] //Creates a Dataset of (value,timestamp)\n",
    "    .flatMap(line =>\n",
    "                 line._1.split(\" \").map(word => (word, line._2))) //Creates (word,timestamp) pairs for each word\n",
    "    .toDF(\"word\", \"timestamp\") //back to a dataframe\n",
    "\n",
    "val query = words.writeStream\n",
    " .outputMode(\"append\")  \n",
    " .format(\"console\")\n",
    " .option(\"truncate\", \"false\")\n",
    " .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Example: Live streaming revenue calculation</span>\n",
    "<br><br>\n",
    "<li>Static products file contains prices and categories of products</li>\n",
    "<li>Transactions are collected in batches from a stream (we'll use a file stream)</li>\n",
    "<li>Report the accumulated revenue by product category as each batch arrives</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Case classes and Streaming DF schemas</span>\n",
    "<li>Recall that a streaming DF must include a schema when the input stream is defined</li>\n",
    "<li>We can either create the schema the long way (StructField and StructType) or define a case class and use that to create the schema</li>\n",
    "<li><span style=\"color:red\">ScalaReflection</span> is a Spark SQL function that can map a case class to a dataframe schema</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Imports and case class definitions</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.catalyst.ScalaReflection\n",
    "import org.apache.spark.sql.types.StructType\n",
    "\n",
    "//Create case class for each object - can be used to infer the schema\n",
    "case class Product(item_num: Int,desc: String, price: Double,category: String)\n",
    "case class Transaction(item_id: Int, trans_qty: Int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Read the static dataframe</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "//Get products. This is a static dataframe\n",
    "val products = spark.read\n",
    "                    .option(\"inferSchema\",true)\n",
    "                    .option(\"header\",true)\n",
    "                    .csv(\"product_example/items.csv\")\n",
    "                    .as[Product] //Uses the case class to construct the schema\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Define the input stream</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//For the stream, we need to specify a schema\n",
    "//ScalaReflection creates a schema from a case class by making each case class type into\n",
    "//     a scala sql StructType\n",
    "val transactionSchema = (ScalaReflection\n",
    "                         .schemaFor[Transaction]\n",
    "                         .dataType\n",
    "                         .asInstanceOf[StructType])\n",
    "\n",
    "//Source: a stream of transactions (each set of transactions is in a file)\n",
    "val transactionStream = (spark.readStream\n",
    "                        .schema(transactionSchema)\n",
    "                        .option(\"header\",false))\n",
    "                        .option(\"maxFilePerTrigger\",1)\n",
    "                        .csv(\"product_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Output Streaming DataFrame</span>\n",
    "<li>Define the output streaming dataframe</li>\n",
    "<li>The output dataframe must be a streaming dataframe</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val revenue_by_category = (transactionStream\n",
    "                          .join(products,products(\"item_num\")===transactionStream(\"item_id\"))\n",
    "                           .groupBy($\"category\")\n",
    "                           .agg(sum($\"price\"*$\"trans_qty\") as \"revenue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Send the output to the sink</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query = revenue_by_category.writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">In-class problem</span>\n",
    "<li>Modify the code above so that it returns the mean revenue from each product (e.g., eggs, widgets, etc.)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Revenue calculation from a socket stream</span>\n",
    "<br><br>\n",
    "<li>Socket data arrives in a column named \"value\"</li>\n",
    "<li>If we want to impose a schema on it, we need to explicitly provide the schema</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.catalyst.ScalaReflection\n",
    "import org.apache.spark.sql.types.StructType\n",
    "\n",
    "//Create case class for each object - can be used to infer the schema\n",
    "case class Product(item_num: Int,desc: String, price: Double,category: String)\n",
    "case class Transaction(item_id: Int, trans_qty: Int)\n",
    "\n",
    "\n",
    "//Get products. This is a static dataframe\n",
    "val products = spark.read\n",
    "                    .option(\"inferSchema\",true)\n",
    "                    .option(\"header\",true)\n",
    "                    .csv(\"product_example/items.csv\")\n",
    "                    .as[Product] //Uses the case class to construct the schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Read the stream from the socket\n",
    "//Split it on ,\n",
    "//Assign column names to each split value\n",
    "val transactionStream = (spark\n",
    "                         .readStream\n",
    "                         .format(\"socket\")\n",
    "                         .option(\"host\", \"localhost\")\n",
    "                         .option(\"port\", 4444)\n",
    "                         .option(\"includeTimestamp\", true)\n",
    "                         .load()\n",
    "                        .selectExpr(\"split(value, ',')[0] as item_id\",\"split(value, ',')[1] as trans_qty\"))\n",
    "\n",
    "\n",
    "//The rest is the same as before\n",
    "//Do the join with the static dataframe\n",
    "//get revenue updates\n",
    "val revenue_by_category = (transactionStream\n",
    "                          .join(products,products(\"item_num\")===transactionStream(\"item_id\"))\n",
    "                           .groupBy($\"category\")\n",
    "                           .agg(sum($\"price\"*$\"trans_qty\") as \"revenue\"))\n",
    "\n",
    "val query = revenue_by_category.writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .start\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Structured Streaming and Event Time</span>\n",
    "<br><br>\n",
    "<li>Spark uses an event time model</li>\n",
    "<li>Let's look at an example to see how that works</li>\n",
    "<li>Assume the following data flow:</li>\n",
    "<pre>\n",
    "2022-04-18 09:30:00,Bob\n",
    "2022-04-18 09:30:03,Bob\n",
    "2022-04-18 09:30:05,Bob\n",
    "2022-04-18 09:30:06,Bill\n",
    "2022-04-18 09:30:14,Henry\n",
    "2022-04-18 09:30:16,Bob\n",
    "2022-04-18 09:30:22,Mary\n",
    "2022-04-18 09:30:04,Bob\n",
    "2022-04-18 09:30:24,Jane\n",
    "</pre>\n",
    "<li>As we can see, one record is late, the 09:30:04 Bob record</li>\n",
    "<li>Let's see how Spark will handle this</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lineStream = (spark\n",
    "                 .readStream\n",
    "                .format(\"socket\")\n",
    "                .option(\"host\", \"localhost\")\n",
    "                .option(\"port\", 4444)\n",
    "                .option(\"includeTimestamp\", true)\n",
    "                .load()\n",
    "                .selectExpr(\"split(value, ',')[0] as timestamp\",\"split(value, ',')[1] as word\"))\n",
    "\n",
    "val windowedCounts = lineStream.groupBy(\n",
    "  window($\"timestamp\", \"10 seconds\"),\n",
    "  $\"word\"\n",
    ").count()\n",
    "\n",
    "val query = windowedCounts.writeStream\n",
    " .outputMode(\"complete\")  \n",
    " .format(\"console\")\n",
    " .option(\"truncate\", \"false\")\n",
    " .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Spark reports event time results</span>\n",
    "<li>As the above example shows, Spark goes back and updates earlier windows to handle late arriving events</li>\n",
    "<li>To do this, it needs to maintain the state of those earlier windows</li>\n",
    "<li>How long should it maintain those states? It can't do so indefinitely because a long running stream will require a huge amount of resources</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Watermarks</span></span>\n",
    "<br><br>\n",
    "<li>Watermarks are Spark's way of dealing with how long to hold data</li>\n",
    "<li>A watermark tells Spark when to drop old aggregated data</li>\n",
    "<li>Any data that is timestamped before (current time - watermark) is ignored</li>\n",
    "<li>Watermarks only work in <span style=\"color:red\">append</span> or <span style=\"color:red\">update</span> modes. Complete mode will always use all the data</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.sql.Timestamp\n",
    "\n",
    "val lineStream = (spark\n",
    "                 .readStream\n",
    "                .format(\"socket\")\n",
    "                .option(\"host\", \"localhost\")\n",
    "                .option(\"port\", 4444)\n",
    "                .load()\n",
    "                .selectExpr(\"split(value, ',')[0] as timestamp\",\"split(value, ',')[1] as word\")\n",
    "                 .withColumn(\"timestamp\",to_timestamp(col(\"timestamp\")))) //necessary because Spark will do time calcs\n",
    "\n",
    "val windowedCounts = lineStream\n",
    "                        .withWatermark(\"timestamp\", \"10 seconds\") //10 seconds is the delay threshold, timestamp the \"event time column\"\n",
    "                        .groupBy(window($\"timestamp\", \"10 seconds\"), $\"word\") //group data by window and word\n",
    "                        .count() //count the words\n",
    "\n",
    "\n",
    "val query = windowedCounts.writeStream\n",
    " .outputMode(\"update\")  \n",
    " .format(\"console\")\n",
    " .option(\"truncate\", \"false\")\n",
    " .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Conditions for watermarking</span>\n",
    "<br>\n",
    "<li>Watermarking guarantees that any data arriving inside the watermark will be used</li>\n",
    "<li>However, it may continue to use data outside the watermark (a watermark is not a window)</li>\n",
    "<li><span style=\"color:red\">withWatermark</span> must he called on the timestamp column</li>\n",
    "<li><span style=\"color:red\">withWatermark</span> must be called just before the aggregation transformation</li>\n",
    "<li>The aggregation must be on the time column or on a window on the time column (e.g., the groupBy must be on a window on timestamp in the above example)</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Joining data from two streams</span>\n",
    "<br><br>\n",
    "<li>In the revenue example, we joined a streaming df to a static df</li>\n",
    "<li>This is reasonably straightforward because the join executes when the streaming df exists and the static df always exists</li>\n",
    "<li>What would be the implication of reversing the join (i.e., join a static df to a streaming df?)</li>\n",
    "<li>When joining two data streams, data for the join may not be available in both streams at the same time</li>\n",
    "<li>Since the program cannot wait forever for data, Spark requires that watermarks be used when joining two streaming dfs</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Example: Home security monitoring</span>\n",
    "<br>\n",
    "<li>A home security system has two data feeds, a camera feed (with multiple cameras) and an alarm feed (with multiple alarms)</li>\n",
    "<li>An alert occurs if the camera feed detects some activity and an alarm trips within one minute of the camera detecting the activity</li>\n",
    "<li>Each stream (camera stream and alarm stream) is fed by multiple homes</li>\n",
    "<li>Join the streams using the home id as the key and with the alert time window (1 minute) as a condition</li>\n",
    "<li>Report all alerts</li>\n",
    "<li>Sample data</li>\n",
    "<pre>\n",
    "2022-04-18 09:30:00,H1,C1\n",
    "2022-04-18 09:33:00,H1,C2\n",
    "2022-04-18 09:34:00,H2,C1\n",
    "2022-04-18 09:45:00,H2,C1\n",
    "</pre>\n",
    "<pre>\n",
    "2022-04-18 09:30:00,H1,A1\n",
    "2022-04-18 09:32:00,H1,A2\n",
    "2022-04-18 09:36:00,H2,A1\n",
    "2022-04-18 09:51:00,H2,A2\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cameraStream = (spark\n",
    "                 .readStream\n",
    "                .format(\"socket\")\n",
    "                .option(\"host\", \"localhost\")\n",
    "                .option(\"port\", 4444)\n",
    "                .load()\n",
    "                .selectExpr(\"split(value, ',')[0] as timestamp_1\",\"split(value, ',')[1] as house_1\",\"split(value, ',')[2] as camera\")\n",
    "                 .withColumn(\"timestamp_1\",to_timestamp(col(\"timestamp_1\")))) //necessary because Spark will do time calcs\n",
    "\n",
    "\n",
    "val alarmStream = (spark\n",
    "                 .readStream\n",
    "                .format(\"socket\")\n",
    "                .option(\"host\", \"localhost\")\n",
    "                .option(\"port\", 9999)\n",
    "                .load()\n",
    "                .selectExpr(\"split(value, ',')[0] as timestamp_2\",\"split(value, ',')[1] as house_2\",\"split(value, ',')[2] as alarm\")\n",
    "                 .withColumn(\"timestamp_2\",to_timestamp(col(\"timestamp_2\")))) //necessary because Spark will do time calcs\n",
    "\n",
    "\n",
    "//Must watermark the streaming windows before a join\n",
    "val cameraStream_watermark = cameraStream.withWatermark(\"timestamp_1\",\"10 minutes\")\n",
    "val alarmStream_watermark = alarmStream.withWatermark(\"timestamp_2\",\"10 minutes\")\n",
    "\n",
    "//The first clause in a join expression must be equality \n",
    "//subsequent clauses don't need that\n",
    "val query = cameraStream_watermark.join(alarmStream_watermark,expr(\"\"\"\n",
    "        house_1 = house_2 AND \n",
    "        timestamp_1 <= timestamp_2 + interval 1 minutes\n",
    "\n",
    "        \"\"\" ))\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"console\")\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Aggregating data after the join</span>\n",
    "<br>\n",
    "<li>Let's add an additional clause, there must be at least two instances of alerts as defined above for an investigation to be initiated</li>\n",
    "<li>e.g., if \"c1,a1\" and \"c2,a1\" send alerts in the one minute window, the company will investigate</li>\n",
    "<li>if only \"c1,a1\" fires then no investigation</li>\n",
    "<li>We want this to be true in any 10 minute window so we'll create a 10 minute sliding window that slides every 1 minute and count instances of alerts</li>\n",
    "<li>Note the following:</li>\n",
    "<ul>\n",
    "    <li>a join on two streaming windows must be watermarked</li>\n",
    "    <li>a join on two streaming windows must use \"append\" as the outputMode</li>\n",
    "    <li>a streaming window that uses append and an aggregation must use watermarks</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cameraStream = (spark\n",
    "                 .readStream\n",
    "                .format(\"socket\")\n",
    "                .option(\"host\", \"localhost\")\n",
    "                .option(\"port\", 4444)\n",
    "                .load()\n",
    "                .selectExpr(\"split(value, ',')[0] as timestamp_1\",\"split(value, ',')[1] as house_1\",\"split(value, ',')[2] as camera\")\n",
    "                 .withColumn(\"timestamp_1\",to_timestamp(col(\"timestamp_1\")))) //necessary because Spark will do time calcs\n",
    "\n",
    "\n",
    "val alarmStream = (spark\n",
    "                 .readStream\n",
    "                .format(\"socket\")\n",
    "                .option(\"host\", \"localhost\")\n",
    "                .option(\"port\", 9999)\n",
    "                .load()\n",
    "                .selectExpr(\"split(value, ',')[0] as timestamp_2\",\"split(value, ',')[1] as house_2\",\"split(value, ',')[2] as alarm\")\n",
    "                 .withColumn(\"timestamp_2\",to_timestamp(col(\"timestamp_2\")))) //necessary because Spark will do time calcs\n",
    "\n",
    "\n",
    "val cameraStream_watermark = cameraStream.withWatermark(\"timestamp_1\",\"10 minutes\")\n",
    "val alarmStream_watermark = alarmStream.withWatermark(\"timestamp_2\",\"10 minutes\")\n",
    "\n",
    "val query = cameraStream_watermark.join(alarmStream_watermark,expr(\"\"\"\n",
    "        house_1 = house_2 AND\n",
    "        timestamp_1 <= timestamp_2 + interval 1 minutes\n",
    "        \"\"\" ))\n",
    "    .withWatermark(\"timestamp_1\",\"10 minutes\") //Since mode is append, we'll need to watermark this stream as well\n",
    "    .groupBy(window($\"timestamp_1\",\"10 minutes\",\"1 minute\"),$\"house_1\") //sliding windows\n",
    "    .count //aggregation\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"console\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Try this</span>\n",
    "<li>Along with each signal, the producer (camera or alarm) sends a numerical threat level indicator (see the data below)</li>\n",
    "<li>As before, compute alerts as a (camera signal, alarm signal) where the alarm signal is generated inside a minute after the camera signal</li>\n",
    "<li>then, assign a numerical value to every 10 second window (sliding every minute) by adding the camera threat level and the alarm threat level</li>\n",
    "<li>Report this total threat value for each signal</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "2022-04-18 09:30:00,H1,C1,5\n",
    "2022-04-18 09:33:00,H1,C2,8\n",
    "2022-04-18 09:34:00,H2,C1,9\n",
    "2022-04-18 09:45:00,H2,C1,20\n",
    "</pre>\n",
    "<pre>\n",
    "2022-04-18 09:30:00,H1,A1,12\n",
    "2022-04-18 09:32:00,H1,A2,13\n",
    "2022-04-18 09:36:00,H2,A1,2\n",
    "2022-04-18 09:51:00,H2,A2,2\n",
    "</pre>\n",
    "\n",
    "<span style=\"color:blue;font-size:large\">This is what you should get</span><br>\n",
    "<pre>\n",
    "+--------------------+-------+------+\n",
    "|              window|house_1|threat|\n",
    "+--------------------+-------+------+\n",
    "|{2022-04-18 09:25...|     H1|  35.0|\n",
    "|{2022-04-18 09:22...|     H1|  35.0|\n",
    "|{2022-04-18 09:24...|     H1|  35.0|\n",
    "|{2022-04-18 09:21...|     H1|  35.0|\n",
    "|{2022-04-18 09:23...|     H1|  35.0|\n",
    "|{2022-04-18 09:25...|     H2|  22.0|\n",
    "+--------------------+-------+------+\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cameraStream = (spark\n",
    "                 .readStream\n",
    "                .format(\"socket\")\n",
    "                .option(\"host\", \"localhost\")\n",
    "                .option(\"port\", 4444)\n",
    "                .load()\n",
    "                    //WRITE THE selectExpr function call here\n",
    "                 .withColumn(\"timestamp_1\",to_timestamp(col(\"timestamp_1\")))) //necessary because Spark will do time calcs\n",
    "\n",
    "\n",
    "val alarmStream = (spark\n",
    "                 .readStream\n",
    "                .format(\"socket\")\n",
    "                .option(\"host\", \"localhost\")\n",
    "                .option(\"port\", 9999)\n",
    "                .load()\n",
    "                //WRITE THE selectExpr function call here                 \n",
    "                .withColumn(\"timestamp_2\",to_timestamp(col(\"timestamp_2\")))) //necessary because Spark will do time calcs\n",
    "\n",
    "\n",
    "val cameraStream_watermark = cameraStream.withWatermark(\"timestamp_1\",\"10 minutes\")\n",
    "val alarmStream_watermark = alarmStream.withWatermark(\"timestamp_2\",\"10 minutes\")\n",
    "\n",
    "val query = cameraStream_watermark.join(alarmStream_watermark,expr(\"\"\"\n",
    "        house_1 = house_2 AND\n",
    "        timestamp_1 <= timestamp_2 + interval 10 seconds\n",
    "        \"\"\" ))\n",
    "    .withWatermark(\"timestamp_1\",\"10 minutes\") //Since mode is append, we'll need to watermark this stream as well\n",
    "    //FILL IN THE REST OF THE CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Example: Model evaluation with streaming data</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Given a pre-trained ML model, we might want to get predictions and model metrics from a real time stream</li>\n",
    "<li>As we'll see, there are some limitations with how structured streaming can handle this</li>\n",
    "<li>Let's start with training our model the usual way</li>\n",
    "<li>And creating a pipeline (the streaming data will pass through the pipeline)</li>\n",
    "<li>The data is California housing data</li>\n",
    "<ul>\n",
    "    <li>Each data item is housing data rolled up into blocks</li>\n",
    "    <li>The dependent variable is the median home value in the block</li>\n",
    "    </ul>\n",
    "<li>We'll create an ML pipeline</li>\n",
    "<ul>\n",
    "    <li>Read the data into a df</li>\n",
    "    <li>Do some feature engineering to get the data inot the right format (prepareData function)</li>\n",
    "    <li>Assemble the input features into a vector (required for ML models)</li>\n",
    "    <li>Scale them to mean 0, std 1</li>\n",
    "    <li>And run a linear regression model on the data</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "//Read the data function\n",
    "def readData(): (DataFrame,DataFrame) = {\n",
    "    val df = spark.read.format(\"csv\")\n",
    "        .option(\"header\",\"false\")\n",
    "        .option(\"inferschema\",\"true\")\n",
    "        .load(\"cal_housing.data\")\n",
    "        .toDF(\"Longitude\",\"Latitude\",\"MedianAge\",\n",
    "                     \"TotalRooms\",\"TotalBedrooms\",\"Population\",\"Households\",\n",
    "                     \"MedianIncome\",\"MedianHomeValue\")\n",
    "    val Array(train,test) = df.randomSplit(Array(0.8,0.2),seed=1234L)\n",
    "    (train,test)\n",
    "}\n",
    "\n",
    "//Do basic feature engineering to get the right set of features\n",
    "def prepareData(df: DataFrame): DataFrame = {\n",
    "    df.withColumn(\"MedianHomeValue\",$\"MedianHomeValue\"/100000)\n",
    "        .withColumn(\"RoomsPerHouse\", col(\"TotalRooms\")/col(\"Households\"))\n",
    "        .withColumn(\"PeoplePerHouse\", col(\"Population\")/col(\"Households\"))\n",
    "        .withColumn(\"BedroomsPerHouse\", col(\"TotalBedrooms\")/col(\"Households\"))\n",
    "        .select(\"MedianHomeValue\", \n",
    "                  \"MedianAge\", \n",
    "                  \"Population\", \n",
    "                  \"Households\", \n",
    "                  \"MedianIncome\", \n",
    "                  \"RoomsPerHouse\", \n",
    "                  \"PeoplePerHouse\", \n",
    "                  \"BedroomsPerHouse\",\n",
    "                   \"Latitude\",\n",
    "                   \"Longitude\")\n",
    "        .withColumnRenamed(\"MedianHomeValue\",\"label\")\n",
    "}\n",
    "\n",
    "//Split the data into train and test\n",
    "//We will use the training data to train the model but will ignore the testing data\n",
    "\n",
    "val (train,test) = readData()\n",
    "\n",
    "//Assemble features vector and scale the data\n",
    "//Set up the regression\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val cols = Array(\"Longitude\", \"Latitude\", \"MedianAge\", \"RoomsPerHouse\", \"BedroomsPerHouse\", \"PeoplePerHouse\", \n",
    "                 \"Households\", \"MedianIncome\")\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(cols)\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val scaler = new StandardScaler()\n",
    "      .setInputCol(\"features\")\n",
    "      .setOutputCol(\"scaledFeatures\")\n",
    "      .setWithStd(true)\n",
    "      .setWithMean(true)\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setMaxIter(10)\n",
    "    .setRegParam(0.3) //Regularization parameter\n",
    "    .setElasticNetParam(0.8) //elastic net regularization parameter (L1 + L2 penalties)\n",
    "    .setFeaturesCol(\"scaledFeatures\") //independent variables\n",
    "    .setLabelCol(\"label\") //dependent variable (we don't need to specify this since we've called our col label)\n",
    "\n",
    "\n",
    "//Create a pipeline\n",
    "//fit training data to the pipeline\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(assembler,scaler,lr))\n",
    "val model = pipeline.fit(prepareData(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Set up the streaming data reader</span>\n",
    "<li>New data will arrive in batches in a stream</li>\n",
    "<li>Note that we need to explicitly provide a schema</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.types.StructField\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "\n",
    "val schema = StructType(Array(\n",
    "    StructField(\"Longitude\",DoubleType),\n",
    "    StructField(\"Latitude\",DoubleType),\n",
    "    StructField(\"MedianAge\",DoubleType),\n",
    "    StructField(\"TotalRooms\",DoubleType),\n",
    "    StructField(\"TotalBedrooms\",DoubleType),\n",
    "    StructField(\"Population\",DoubleType),\n",
    "    StructField(\"Households\",DoubleType),\n",
    "    StructField(\"MedianIncome\",DoubleType),\n",
    "    StructField(\"MedianHomeValue\",DoubleType)\n",
    "    ))\n",
    "\n",
    "    \n",
    "val streaming_data = spark\n",
    "    .readStream \n",
    "    .option(\"header\", \"false\") \n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .schema(schema)\n",
    "    .csv(\"datafiledir\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Write a function that converts the input stream DF to an output stream DF</span>\n",
    "<br><br>\n",
    "<li>We'll need to call model.transform to add the predictions column to the df</li>\n",
    "<li>And then, if necessary, do some transformations on the df</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def prepare_sink_df(): DataFrame = {\n",
    "    val result = model.transform(prepareData(streaming_data))\n",
    "                    .select(\"label\",\"prediction\")\n",
    "                    .withColumnRenamed(\"label\",\"Actual\")\n",
    "                    .withColumnRenamed(\"prediction\",\"Predicted\")\n",
    "    result\n",
    "                        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Write the output stream df to the sink</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//import java.util.concurrent.TimeUnit\n",
    "//import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "val query = prepare_sink_df\n",
    "            .writeStream\n",
    "            .format(\"console\")\n",
    "            .outputMode(\"update\")\n",
    "            .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Evaluation metrics</span>\n",
    "<br><br>\n",
    "<li>Unfortunately, Streaming DataFrames are limited to dataframe transformations</li>\n",
    "<li>Evaluation metrics (rmse,r2) are returned as Double, and we would need to create a dataframe of results</li>\n",
    "<li>Which is not allowed - we can only run transformations on the input stream df</li>\n",
    "<li>So, we'll need to calculate the metrics ourselves, using dataframe operations</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def prepare_sink_df(): DataFrame = {\n",
    "    val result = model.transform(prepareData(streaming_data))\n",
    "                    .select(\"label\",\"prediction\")\n",
    "    result\n",
    "        .withColumn(\"sq_diff\",(col(\"prediction\")-col(\"label\")) * (col(\"prediction\")-col(\"label\")))\n",
    "        .agg(mean(\"sq_diff\") as \"mse\")\n",
    "        .withColumn(\"rmse\",sqrt(\"mse\"))\n",
    "        .select(\"rmse\")\n",
    "                        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query = prepare_sink_df\n",
    "            .writeStream\n",
    "            .format(\"console\")\n",
    "            .outputMode(\"complete\")\n",
    "            .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
