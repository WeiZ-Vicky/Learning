{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d865e7b8",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">PySpark</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10568db",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. untyped\n",
    "2. python way: index, col, lambda, continous line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aedf52a",
   "metadata": {},
   "source": [
    "<li>The PySpark package should already be installed</li>\n",
    "<li>If it isn't, use pip or conda to install the correct version of pyspark</li>\n",
    "<li>Check the spark version <span style=\"color:blue\">spark.version</span> (mine is 3.3.0) and install the same version of pyspark (e.g., <span style=\"color:blue\">pip install pyspark==3.3.0</span>)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2aba0a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Start a spark session</span>\n",
    "<li>Unlike Scala Spark, you need to explicitly start a spark session</li>\n",
    "<li>And you need to explicitly extract the spark context</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd88aa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/02 16:33:15 WARN Utils: Your hostname, VickyZMacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 192.168.11.104 instead (on interface en0)\n",
      "23/02/02 16:33:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/02 16:33:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/02 16:33:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/02/02 16:33:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark ML Basics\").getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6791cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3ae83",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "<span style=\"color:green;font-size:xx-large\">Working with RDDs in pyspark</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38943639",
   "metadata": {},
   "source": [
    "<li>The spark API for Python is similar to the API for Scala</li>\n",
    "<li>With the difference that actual data objects are Python objects (untyped) and not Scala objects</li>\n",
    "<li>Compare the data type of the RDD below with the equivalent RDD in Scala</li>\n",
    "<pre>\n",
    "x: Array[(String, Int, Int)] = Array((John,20,32), (Jill,15,45))\n",
    "res6: org.apache.spark.rdd.RDD[(String, Int, Int)] = ParallelCollectionRDD[7] at parallelize at <console>:33\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2055d7",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Creating an RDD</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca311412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [(\"John\",20,32),(\"Jill\",15,45)]\n",
    "sc.parallelize(x) #No type information!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af8242",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Reading a text file</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb56267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Created Date,Closed Date,Agency,Agency Name,Complaint Type,Incident Zip,Borough,Latitude,Longitude,processing_time,processing_days'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.textFile(\"nyc_311_2022_clean.csv\")\n",
    "data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "026ac52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Created Date,Closed Date,Agency,Agency Name,Complaint Type,Incident Zip,Borough,Latitude,Longitude,processing_time,processing_days',\n",
       " '2020-01-07 14:09:00,2020-01-13 11:20:00,DSNY,Department of Sanitation,Electronics Waste Appointment,11692,QUEENS,40.58993519447414,-73.78942049765358,5 days 21:11:00,5.882638888888889',\n",
       " '2020-01-04 11:37:00,2020-01-08 13:19:00,DSNY,Department of Sanitation,Electronics Waste Appointment,10310,STATEN ISLAND,40.62719924888892,-74.11245623591475,4 days 01:42:00,4.070833333333334',\n",
       " '2020-01-03 16:33:00,2020-01-05 00:00:00,DSNY,Department of Sanitation,Request Large Bulky Item Collection,11213,BROOKLYN,40.6672052181697,-73.93463635283278,1 days 07:27:00,1.3104166666666668',\n",
       " '2020-01-06 17:27:00,2020-01-11 00:00:00,DSNY,Department of Sanitation,Request Large Bulky Item Collection,11379,QUEENS,40.72687041685842,-73.8769198480706,4 days 06:33:00,4.272916666666666']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202e5256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nyc_311_2022_clean.csv MapPartitionsRDD[2] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a87ed2",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Using map</span>\n",
    "<li>Note the use of lambda functions (Python's anonymous function)</li>\n",
    "<li>Actions like <span style=\"color:blue\">take</span>, <span style=\"color:blue\">first</span>, <span style=\"color:blue\">collect</span>, etc., are the same as in the scala API</li>\n",
    "<li>But, you need to always use parentheses to indicate that these are functions</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade60a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Agency', 'processing_days')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data = data.map(lambda l: l.split(\",\")).map(lambda l: (l[2],l[10]))\n",
    "time_data.first()\n",
    "#time_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db13b46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Agency', 'processing_days'),\n",
       " ('DSNY', '5.882638888888889'),\n",
       " ('DSNY', '4.070833333333334'),\n",
       " ('DSNY', '1.3104166666666668')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e872b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10f91a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8835630"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data.count() #python integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c8cc5",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Drop the header row</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e41bdc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8835629"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data.mapPartitionsWithIndex(lambda i,it: iter(list(it)[1:] if i==0 else it)).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f495bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DSNY', '5.882638888888889'),\n",
       " ('DSNY', '4.070833333333334'),\n",
       " ('DSNY', '1.3104166666666668'),\n",
       " ('DSNY', '4.272916666666666')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data.mapPartitionsWithIndex(lambda i,it: iter(list(it)[1:] if i==0 else it)).take(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103cfe59",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Convert processing time to float</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "646996bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DSNY', 5.882638888888889),\n",
       " ('DSNY', 4.070833333333334),\n",
       " ('DSNY', 1.3104166666666668),\n",
       " ('DSNY', 4.272916666666666)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data.mapPartitionsWithIndex(lambda i,it: iter(list(it)[1:] if i==0 else it)).\\\n",
    "    map(lambda l: (l[0],float(l[1]))).\\\n",
    "    take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cff8464",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = time_data.mapPartitionsWithIndex(lambda i,it: iter(list(it)[1:] if i==0 else it)).\\\n",
    "    map(lambda l: (l[0],float(l[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca18cd3",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Calculate averages using combineByKey</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9bafcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('DHS', 1.257410746037668),\n",
       " ('OSE', 0.12648533950617283),\n",
       " ('DEP', 5.006676030990059),\n",
       " ('HPD', 13.242378653308977),\n",
       " ('DOT', 14.494687185115147),\n",
       " ('MAYORâ\\x80\\x99S OFFICE OF SPECIAL ENFORCEMENT', 10.993121710572918),\n",
       " ('EDC', 56.00096076391819),\n",
       " ('DOB', 39.10644259286542),\n",
       " ('DFTA', 13.390725308641976),\n",
       " ('DOHMH', 15.397117888616293),\n",
       " ('DOF', 19.76244362870432),\n",
       " ('TLC', 53.682035801957745),\n",
       " ('DSNY', 6.984900241530092),\n",
       " ('OFFICE OF TECHNOLOGY AND INNOVATION', 0.7780439814814815),\n",
       " ('DOITT', 28.392176800309237),\n",
       " ('NYPD', 0.33957275800732817),\n",
       " ('DCA', 3.1134426905912487),\n",
       " ('DPR', 65.99335925229713),\n",
       " ('FDNY', 402.1443981481481),\n",
       " ('DOE', 43.447434186637615)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combiner(x): #initialize \n",
    "    return (1,x)\n",
    "\n",
    "def merger(x,y):\n",
    "    return ((x[0]+1,x[1]+y))\n",
    "\n",
    "def merge_and_combiner(x,y): #across different partition\n",
    "    return ((x[0]+y[0],x[1]+y[1]))\n",
    "\n",
    "processed_data.combineByKey(combiner,merger,merge_and_combiner)\\\n",
    "    .map(lambda x: (x[0], x[1][1]/x[1][0]))\\\n",
    "    .collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99b5b0",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">PySpark Dataframes</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "514aba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark\\\n",
    ".read\\\n",
    ".option('header',True).csv(\"nyc_311_2022_clean.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70ca50c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Created Date: string (nullable = true)\n",
      " |-- Closed Date: string (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Agency Name: string (nullable = true)\n",
      " |-- Complaint Type: string (nullable = true)\n",
      " |-- Incident Zip: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- processing_time: string (nullable = true)\n",
      " |-- processing_days: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91476852",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Reassigning values is possible in PySpark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39b83abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Created Date: string, Closed Date: string, Agency: string, Agency Name: string, Complaint Type: string, Zip Code: string, Borough: string, Latitude: string, Longitude: string, processing_time: string, processing_days: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumnRenamed(\"Incident Zip\",\"Zip Code\") #in scala, you cannot reassign df since it is val\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f746d",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">df.select</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9e5741c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|Agency|   processing_days|\n",
      "+------+------------------+\n",
      "|  DSNY| 5.882638888888889|\n",
      "|  DSNY| 4.070833333333334|\n",
      "|  DSNY|1.3104166666666668|\n",
      "|  DSNY| 4.272916666666666|\n",
      "|  DSNY|1.5930555555555554|\n",
      "|  DSNY| 2.272222222222222|\n",
      "|  DSNY| 5.195138888888889|\n",
      "|  DSNY|2.1868055555555554|\n",
      "|  DSNY|2.4805555555555556|\n",
      "|  DSNY|2.5652777777777778|\n",
      "|  DSNY| 4.317361111111111|\n",
      "|  DSNY|1.2472222222222222|\n",
      "|  DSNY| 3.395138888888889|\n",
      "|  DSNY| 5.979166666666667|\n",
      "|  DSNY|1.3180555555555555|\n",
      "|  DSNY| 3.870833333333333|\n",
      "|  DSNY|1.5958333333333332|\n",
      "|  DSNY| 9.120138888888889|\n",
      "|  DSNY|3.5493055555555557|\n",
      "|  DSNY|               2.0|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Agency\",\"processing_days\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab8b72",
   "metadata": {},
   "source": [
    "import org.apache.spark.sql.functions.col #in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ae26dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Agency: string, processing_hours: double]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col #cannot use $ sign in pyspark\n",
    "df.select(\"Agency\",col(\"processing_days\")*24).toDF(\"Agency\",\"processing_hours\") #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b252286e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Agency: string, processing_hours: double]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"Agency\",df[\"processing_days\"]*24).toDF(\"Agency\",\"processing_hours\") #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "922df3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Created Date='2020-01-07 14:09:00', Closed Date='2020-01-13 11:20:00', Agency='DSNY', Agency Name='Department of Sanitation', Complaint Type='Electronics Waste Appointment', Zip Code='11692', Borough='QUEENS', Latitude='40.58993519447414', Longitude='-73.78942049765358', processing_time='5 days 21:11:00', processing_days='5.882638888888889', processing_hours=141.18333333333334),\n",
       " Row(Created Date='2020-01-04 11:37:00', Closed Date='2020-01-08 13:19:00', Agency='DSNY', Agency Name='Department of Sanitation', Complaint Type='Electronics Waste Appointment', Zip Code='10310', Borough='STATEN ISLAND', Latitude='40.62719924888892', Longitude='-74.11245623591475', processing_time='4 days 01:42:00', processing_days='4.070833333333334', processing_hours=97.70000000000002),\n",
       " Row(Created Date='2020-01-03 16:33:00', Closed Date='2020-01-05 00:00:00', Agency='DSNY', Agency Name='Department of Sanitation', Complaint Type='Request Large Bulky Item Collection', Zip Code='11213', Borough='BROOKLYN', Latitude='40.6672052181697', Longitude='-73.93463635283278', processing_time='1 days 07:27:00', processing_days='1.3104166666666668', processing_hours=31.450000000000003),\n",
       " Row(Created Date='2020-01-06 17:27:00', Closed Date='2020-01-11 00:00:00', Agency='DSNY', Agency Name='Department of Sanitation', Complaint Type='Request Large Bulky Item Collection', Zip Code='11379', Borough='QUEENS', Latitude='40.72687041685842', Longitude='-73.8769198480706', processing_time='4 days 06:33:00', processing_days='4.272916666666666', processing_hours=102.54999999999998)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"processing_hours\",df[\"processing_days\"]*24).take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071a17c",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">SQL</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb22848f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=======================================>                 (9 + 4) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              Agency|avg(processing_days)|\n",
      "+--------------------+--------------------+\n",
      "|MAYORâS OFFICE ...|  10.993121710572932|\n",
      "|                 DOT|  14.494687185115172|\n",
      "|                 HPD|  13.242378653308977|\n",
      "|                 DCA|  3.1134426905912487|\n",
      "|                 DPR|   65.99335925229708|\n",
      "|                 TLC|   53.68203580195775|\n",
      "|                 EDC|   56.00096076391825|\n",
      "|                 DOF|   19.76244362870433|\n",
      "|                NYPD|  0.3395727580073328|\n",
      "|                 DOB|  39.106442592865434|\n",
      "|                 DEP|   5.006676030990062|\n",
      "|                 DOE|   43.44743418663761|\n",
      "|               DOHMH|  15.397117888616267|\n",
      "|                DSNY|   6.984900241530156|\n",
      "|                 DHS|   1.257410746037668|\n",
      "|               DOITT|  28.392176800309258|\n",
      "|                DFTA|  13.390725308641976|\n",
      "|OFFICE OF TECHNOL...|  0.7780439814814815|\n",
      "|                 OSE| 0.12648533950617283|\n",
      "|                FDNY|   402.1443981481481|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"dataDB\")\n",
    "spark.sql(\"select Agency, AVG(processing_days) from dataDB GROUP BY Agency\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "052482ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 17:>                                                        (0 + 8) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 09:01:53 ERROR Executor: Exception in task 0.0 in stage 17.0 (TID 220)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1849)\n",
      "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:708)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:244)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:53)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda$1604/0x0000000800be0c40.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1470)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:51)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1450)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1421)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:657)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/12/07 09:01:53 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 17.0 (TID 220),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1849)\n",
      "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:708)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:244)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:53)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda$1604/0x0000000800be0c40.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1470)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:51)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1450)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1421)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:657)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/12/07 09:01:53 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@d4b7013 rejected from java.util.concurrent.ThreadPoolExecutor@5914c39a[Shutting down, pool size = 8, active threads = 8, queued tasks = 0, completed tasks = 220]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:305)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/12/07 09:01:53 WARN TaskSetManager: Lost task 0.0 in stage 17.0 (TID 220) (dyn-209-2-224-215.dyn.columbia.edu executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1849)\n",
      "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:708)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:244)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:53)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda$1604/0x0000000800be0c40.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1470)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:51)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1450)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1421)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:657)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "[953.671s][warning][gc,alloc] Executor task launch worker for task 1.0 in stage 17.0 (TID 221): Retried waiting for GCLocker too often allocating 8953858 words\n",
      "22/12/07 09:01:53 ERROR Executor: Exception in task 1.0 in stage 17.0 (TID 221)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1849)\n",
      "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:708)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:244)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:53)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda$1604/0x0000000800be0c40.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1470)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:51)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1450)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1421)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:657)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/12/07 09:01:53 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 17.0 (TID 221),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1849)\n",
      "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:708)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:244)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:53)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda$1604/0x0000000800be0c40.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1470)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:51)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1450)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1421)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:657)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/12/07 09:01:53 ERROR TaskSetManager: Task 0 in stage 17.0 failed 1 times; aborting job\n",
      "22/12/07 09:01:53 WARN TaskSetManager: Lost task 4.0 in stage 17.0 (TID 224) (dyn-209-2-224-215.dyn.columbia.edu executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/07 09:01:53 WARN TaskSetManager: Lost task 3.0 in stage 17.0 (TID 223) (dyn-209-2-224-215.dyn.columbia.edu executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/07 09:01:53 WARN TaskSetManager: Lost task 6.0 in stage 17.0 (TID 226) (dyn-209-2-224-215.dyn.columbia.edu executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/07 09:01:53 WARN TaskSetManager: Lost task 5.0 in stage 17.0 (TID 225) (dyn-209-2-224-215.dyn.columbia.edu executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/07 09:01:53 WARN TaskSetManager: Lost task 2.0 in stage 17.0 (TID 222) (dyn-209-2-224-215.dyn.columbia.edu executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/07 09:01:53 WARN TaskSetManager: Lost task 7.0 in stage 17.0 (TID 227) (dyn-209-2-224-215.dyn.columbia.edu executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 62851)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3397, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/vz/pjwcblyj0519y8k2hl3dhmrw0000gn/T/ipykernel_38432/3523621387.py\", line 1, in <cell line: 1>\n",
      "    df.toPandas()\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/pyspark/sql/pandas/conversion.py\", line 205, in toPandas\n",
      "    pdf = pd.DataFrame.from_records(self.collect(), columns=self.columns)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/pyspark/sql/dataframe.py\", line 817, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/pyspark/sql/pandas/conversion.py:205\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    206\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2003\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2000\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2001\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2003\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2005\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2006\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/ipykernel/zmqshell.py:542\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    536\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    537\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    539\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m'\u001b[39m : stb,\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    543\u001b[0m }\n\u001b[1;32m    545\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "df.toPandas() #OutOfMemoryError: sometimes it is too big for pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93834be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
