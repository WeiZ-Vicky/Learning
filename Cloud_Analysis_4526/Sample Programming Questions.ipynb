{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce035e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.packages= [\"graphframes:graphframes:0.8.2-spark3.2-s_2.12\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302d93c",
   "metadata": {},
   "source": [
    "# Notes: Dataframe, ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7b149",
   "metadata": {},
   "source": [
    "<h1>Sample Problem 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5b464",
   "metadata": {},
   "source": [
    "<p><strong><span style=\"font-size: 14pt; color: #3598db;\">tl;dr jump to the bottom, copy the code into a Jupyter notebook, and fill in the missing code.&nbsp;</span></strong></p>\n",
    "<p>In this question, you will prepare data for text analysis. Starting with the raw text data, you need to do the necessary feature engineering to get the data ready for topic modeling by tokenizing and cleaning the text, extracting entities from the text, and then calculating the tfidf vector for the entities in each document. The operations are described below and some functions are already written for you so all you need to do is to follow the steps carefully!</p>\n",
    "<p><span style=\"font-size: 14pt;\"><strong>Step 1</strong></span>: Write a function&nbsp;<span style=\"color: #e03e2d;\"><em>make_df</em></span> that takes a Seq of documents as an argument (each document is in a <span style=\"color: #e03e2d;\">(String,String) tuple</span> containing (document_id, document_text) and returns a <span style=\"color: #e03e2d;\">dataframe</span> with five columns.</p>\n",
    "<p>column1: <span style=\"color: #e03e2d;\">document_id</span> (the document id)</p>\n",
    "<p>column2: <span style=\"color: #e03e2d;\">document_text</span> (the original text of the document)</p>\n",
    "<p>column3: <span style=\"color: #e03e2d;\">cleaned_text</span> (the text with periods, \\n's and commas removed.</p>\n",
    "<p>column 4: <span style=\"color: #e03e2d;\">document_terms</span> (cleaned_text split on spaces)</p>\n",
    "<p>column 5: <span style=\"color: #e03e2d;\">entity_terms</span> (any bi-gram in which both terms begin with uppercase letters is considered an entity and the two terms should be replaced by the their concatenated value)</p>\n",
    "<p>As an example, if:</p>\n",
    "<pre>val doc1 = (\"d1\", \"New York is a city in the United States.\")</pre>\n",
    "<p><span style=\"color: #e03e2d;\"><em>make_df(Seq(doc1)) </em></span>should return (note that the period has been removed in cleaned_text):</p>\n",
    "<table style=\"border-collapse: collapse; width: 97.8346%; height: 58px;\" border=\"1\">\n",
    "    <tbody>\n",
    "        <tr style=\"height: 29px;\">\n",
    "            <td style=\"width: 7.95465%; height: 29px;\">document_id</td>\n",
    "            <td style=\"width: 29.5373%; height: 29px;\">document_text</td>\n",
    "            <td style=\"width: 22.4459%; height: 29px;\">cleaned_text</td>\n",
    "            <td style=\"width: 15.2029%; height: 29px;\">document_terms</td>\n",
    "            <td style=\"width: 24.7588%; height: 29px;\">entity_terms</td>\n",
    "        </tr>\n",
    "        <tr style=\"height: 29px;\">\n",
    "            <td style=\"width: 7.95465%; height: 29px;\">d1</td>\n",
    "            <td style=\"width: 29.5373%; height: 29px;\">New York is a city in the United States.</td>\n",
    "            <td style=\"width: 22.4459%; height: 29px;\">New York is a city in the United States</td>\n",
    "            <td style=\"width: 15.2029%; height: 29px;\">\n",
    "                <pre>New, York, is, a, city, in, the, United, States</pre>\n",
    "            </td>\n",
    "            <td style=\"width: 24.7588%; height: 29px;\">\n",
    "                <pre>[<br />NewYork, UnitedStates]</pre>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>&nbsp;</p>\n",
    "<p>The way to do this is to</p>\n",
    "<p>1. create a <span style=\"color: #e03e2d;\">dataframe</span> from the input sequence with the first two columns</p>\n",
    "<p>2. write <span style=\"color: #e03e2d;\">udfs</span> (user defined functions) for generating each subsequent column and then apply the udf to the dataframe using the withColumn transformation</p>\n",
    "<p><strong>Step 2</strong>: Use <a class=\"inline_disabled\" href=\"https://spark.apache.org/docs/latest/ml-features#countvectorizer\" target=\"_blank\" rel=\"noopener\">CountVectorizer</a> to generate the&nbsp; vocabulary and the vector of term frequencies for each document in a new column <span style=\"color: #e03e2d;\">term_freqs</span>.&nbsp;</p>\n",
    "<p><strong>Step 3</strong>: Use <a class=\"inline_disabled\" href=\"https://spark.apache.org/docs/latest/ml-features.html#tf-idf\" target=\"_blank\" rel=\"noopener\">IDF</a> to get the tfidf vector from the term frequencies in a dataframe containing two columns, the document_id and <span style=\"color: #e03e2d;\">tfidfVec</span></p>\n",
    "<p><strong>Example</strong>:</p>\n",
    "<p>If the initial data is in the following two documents:</p>\n",
    "<pre>val doc1 = (\"doc 1\",\"\"\"<br />Columbia University is a large university in New York.<br />It has many schools including Columbia College, Engineering School, Law School, and Business School.<br />It was established in 1754<br />\"\"\")<br />val doc2 = (\"doc 2\",\"\"\"<br />Operations Research is a department in the Engineering School of Columbia University.<br />Operations Research was established in 1919.<br />Operations Research has a BS major and offers many MS degrees.<br />Graduates of Operations Research get good jobs and have a very happy life.<br />\"\"\")</pre>\n",
    "<p>then, the final idfMatrix dataframe should be:</p>\n",
    "<table style=\"border-collapse: collapse; width: 97.8346%; height: 87px;\" border=\"1\">\n",
    "    <tbody>\n",
    "        <tr style=\"height: 29px;\">\n",
    "            <td style=\"width: 10.7925%; height: 29px;\">document_id</td>\n",
    "            <td style=\"width: 89.1069%; height: 29px;\">tfidfVec</td>\n",
    "        </tr>\n",
    "        <tr style=\"height: 29px;\">\n",
    "            <td style=\"width: 10.7925%; height: 29px;\">doc1</td>\n",
    "            <td style=\"width: 89.1069%; height: 29px;\">\n",
    "                <pre>(12,<br />[1,2,3,4,5,6,7,9,10,11],<br />[0.0,0.0,0.4054651081081644,0.4054651081081644,0.4054651081081644,<br />   0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644,<br />   0.4054651081081644])</pre>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr style=\"height: 29px;\">\n",
    "            <td style=\"width: 10.7925%; height: 29px;\">doc2</td>\n",
    "            <td style=\"width: 89.1069%; height: 29px;\">\n",
    "                <pre>(12,[0,1,2,8],[1.6218604324326575,0.0,0.0,0.4054651081081644]) </pre>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>The term frequencies should be:</p>\n",
    "<table style=\"border-collapse: collapse; width: 97.8346%;\" border=\"1\">\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"width: 99.8994%;\">term_freqs</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"width: 99.8994%;\">\n",
    "                <pre>(12,[1,2,3,4,5,6,7,9,10,11],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])</pre>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"width: 99.8994%;\">\n",
    "                <pre>(12,[0,1,2,8],[4.0,1.0,1.0,1.0])</pre>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>The entity_terms should be:</p>\n",
    "<table style=\"border-collapse: collapse; width: 97.8346%;\" border=\"1\">\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"width: 99.8994%;\">entity_terms</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"width: 99.8994%;\">\n",
    "                <pre>[ColumbiaUniversity, NewYork, YorkIt, ColumbiaCollege, CollegeEngineering, EngineeringSchool, SchoolLaw, LawSchool, BusinessSchool, SchoolIt]</pre>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"width: 99.8994%;\">\n",
    "                <pre>[OperationsResearch, EngineeringSchool, ColumbiaUniversity, UniversityOperations, OperationsResearch, OperationsResearch, OperationsResearch]</pre>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>The code for this problem is outlined below. Easiest if you cut and paste this into your notebook and then fill in the missing parts<br /><br /></p>\n",
    "<pre>import org.apache.spark.sql.DataFrame<br />val doc1 = (\"doc 1\",\"\"\"<br />Columbia University is a large university in New York.<br />It has many schools including Columbia College, Engineering School, Law School, and Business School.<br />It was established in 1754<br />\"\"\")<br />val doc2 = (\"doc 2\",\"\"\"<br />Operations Research is a department in the Engineering School of Columbia University.<br />Operations Research was established in 1919.<br />Operations Research has a BS major and offers many MS degrees.<br />Graduates of Operations Research get good jobs and have a very happy life.<br />\"\"\")<br /><br />//This function takes two strings as input and returns true if both begin with an uppercase letter<br />//and false otherwise<br />//The function char.isUpper returns true if a character is an uppercase letter and false otherwise<br /><br />def both_uc(w1: String,w2: String): Boolean = //WRITE THIS FUNCTION<br /><br />//both_uc(\"columbia\",\"University\") returns false<br />//both_uc(\"Columbia\",\"University\") returns true<br />//both_uc(\"columbia\",\"university\") returns false<br /><br />//clean_data removes periods, \\n's and commas from the text string<br />//Do also use trim() to remove leading and trailing spaces<br />def clean_data(a: String): String = //WRITE THIS FUNCTION<br /><br />/*<br />val sample = \"\"\"<br />Jim, Jill and John.<br />The three siblings.<br />\"\"\"<br />clean_data(sample)<br /><br />should return:<br /><br />String = Jim &nbsp;Jill and John &nbsp;The three siblings<br /><br />*/<br /><br /><br />//split_data takes a string and splits it on or more spaces (just in case the text has extra&nbsp;<br />//spaces between words). This is written for you.&nbsp;<br />def split_data(a: String): Array[String] = a.split(\"\\\\s+\")<br /><br />/*&nbsp;<br />split_data(clean_data(sample))<br /><br />should return:<br /><br />Array[String] = Array(Jim, Jill, and, John, The, three, siblings)<br /><br />*/<br /><br />//Given an Array of strings (the terms), find entities (pairs of words beginning with&nbsp;<br />// &nbsp;uppercase letters, concatenate them and replace the pair by the concatenation)<br />//I've written this for you as well!<br />def replace_entities(a: Array[String]):Array[String] = {<br />&nbsp; &nbsp; val indices = 0 to a.length-1<br />&nbsp; &nbsp; indices.slice(0,indices.length-1)<br />&nbsp; &nbsp; .flatMap(i =&gt;&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if (both_uc(a(i),a(i+1))) Some(a(i)+a(i+1))<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;else None)<br />&nbsp; &nbsp; .toArray<br />}<br /><br />//Create udfs<br />val clean_data_udf =&nbsp;<br />val split_data_udf =&nbsp;<br />val replace_entities_udf =&nbsp;<br /><br />//Write the make_df function<br />//The function should return a DataFrame&nbsp;<br />//1. Write the signature. make_df takes a Seq of (document_id,document_text) tuples as argument<br />// and returns a DataFrame<br />//2. make an rdd and convert that into a DF with with appropriate column names<br />//3. using withColumn transformations, clean the data (clean_data column),<br />// &nbsp; &nbsp; split the cleaned data (document_terms column)<br />// &nbsp; &nbsp; get the entities (entity_terms column)<br /><br />def make_df //WRITE THIS FUNCTION<br /><br />//make the df using make_df (Written for you)<br />val df = make_df(Array(doc1,doc2))<br /><br />//Using CountVectorizer, generate term_freqs column<br />import org.apache.spark.ml.feature.CountVectorizer<br />val countVectorizer = //WRITE THIS<br /><br />val vocabModel = countVectorizer.fit(df)<br />val freqs = vocabModel.transform(df)<br /><br />//Using IDF get the tfidfVec<br />import org.apache.spark.ml.feature.IDF<br /><br />val idf = //WRITE THIS<br />val idfModel = idf.fit(freqs)<br />val idfMatrix = //WRITE THIS<br /><br />idfMatrix.show(false) //The Result<br /><br /><br /><br /></pre>\n",
    "<pre></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "val doc1 = (\"doc 1\",\"\"\"\n",
    "Columbia University is a large university in New York.\n",
    "It has many schools including Columbia College, Engineering School, Law School, and Business School.\n",
    "It was established in 1754\n",
    "\"\"\")\n",
    "val doc2 = (\"doc 2\",\"\"\"\n",
    "Operations Research is a department in the Engineering School of Columbia University.\n",
    "Operations Research was established in 1919.\n",
    "Operations Research has a BS major and offers many MS degrees.\n",
    "Graduates of Operations Research get good jobs and have a very happy life.\n",
    "\"\"\")\n",
    "\n",
    "def both_uc(w1: String,w2: String): Boolean = if (w1(0).isUpper & w2(0).isUpper) true else false\n",
    "both_uc(\"columbia\",\"University\")\n",
    "\n",
    "def split_data(a: String): Array[String] = a.split(\"\\\\s+\")\n",
    "def clean_data(a: String): String =\n",
    "    a.replace(\"\\n\",\" \").replace(\".\",\" \").replace(\",\",\" \").replace(\"  \",\" \").trim()\n",
    "\n",
    "\n",
    "def replace_entities(a: Array[String]):Array[String] = {\n",
    "    val indices = 0 to a.length-1\n",
    "    indices.slice(0,indices.length-1)\n",
    "    .flatMap(i => \n",
    "         if (both_uc(a(i),a(i+1))) Some(a(i)+a(i+1))\n",
    "         else None)\n",
    "    .toArray\n",
    "}\n",
    "\n",
    "val clean_data_udf = udf(clean_data _)\n",
    "val split_data_udf = udf(split_data _)\n",
    "val replace_entities_udf = udf(replace_entities _)\n",
    "\n",
    "def make_df(s: Seq[(String,String)]): DataFrame = {\n",
    "    sc.parallelize(s)\n",
    "        .toDF(\"document_id\",\"document_text\")\n",
    "        .withColumn(\"cleaned_string\",clean_data_udf($\"document_text\"))\n",
    "        .withColumn(\"document_terms\",split_data_udf($\"cleaned_string\"))\n",
    "        .withColumn(\"entity_terms\",replace_entities_udf($\"document_terms\"))\n",
    "}\n",
    "\n",
    "\n",
    "val df = make_df(Array(doc1,doc2))\n",
    "\n",
    "import org.apache.spark.ml.feature.CountVectorizer\n",
    "val countVectorizer = new CountVectorizer()\n",
    "    .setInputCol(\"entity_terms\")\n",
    "    .setOutputCol(\"term_freqs\")\n",
    "    .setVocabSize(20)\n",
    "\n",
    "val vocabModel = countVectorizer.fit(df)\n",
    "val freqs = vocabModel.transform(df)\n",
    "\n",
    "import org.apache.spark.ml.feature.IDF\n",
    "\n",
    "val idf = new IDF()\n",
    "    .setInputCol(\"term_freqs\")\n",
    "    .setOutputCol(\"tfidfVec\")\n",
    "val idfModel = idf.fit(freqs)\n",
    "val idfMatrix = idfModel\n",
    "                .transform(freqs)\n",
    "                .select(\"document_id\", \"tfidfVec\")\n",
    "\n",
    "idfMatrix.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480c1f5",
   "metadata": {},
   "source": [
    "<h1>Sample Problem 2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadde407",
   "metadata": {},
   "source": [
    "<p><strong>Problem</strong>: Write a function that returns the betweenness centrality for each of the vertices in g. Once again, you can skip down to the code below, cut and paste it into your notebook, and start working. Or, read the long explanation below</p>\n",
    "<h3>Betweenness centrality</h3>\n",
    "<p>Betweenness Centrality is a measure of the criticality of a vertex in a graph and is generally calculated as a function of the number of shortest paths that pass through the vertex. A simple calculation is as follows:</p>\n",
    "<p>1. If there are n vertices in a graph, then the number of possible shortest paths between all pairs of vertices is n*(n-1)</p>\n",
    "<p>2. Calculate the shortest paths between all pairs of vertices in the graph</p>\n",
    "<p>3. For any vertex k, count the number of shortest paths calculated in step 2 that go through vertex k (a shortest path from i to j, i!=k and j!=k, that contains k). Let this count be c<sub>k</sub></p>\n",
    "<p>4. The betweenness centrality measure of vertex k is c<sub>k</sub>/(n*(n-1))</p>\n",
    "<p>Betweenness centrality is important because&nbsp; keeping the vertices with the highest betweenness centrality operational is critical to keeping the graph operational. For example, United Airlines has a hub at Chicago O'Hare airport and that airport is on the shortest path on flights between many pairs of cities in the United States (it has a high betweenness centrality). If the airport shuts down (a snowstorm), the disruptions are monumental!</p>\n",
    "<h3>Betweenness Centrality given a graph g</h3>\n",
    "<p>1. Write a function that returns an Array[(Int,Int)] whose elements are all vertex pairs in the graph. For example, if the graph has 3 vertices: 1, 2, 3; then the function should return (note that a graphframes graph is a directed graph)</p>\n",
    "<pre>Array[(Int, Int)] = Array((1,2),(1,3),(2,1),(2,3),(3,1),(3,2))</pre>\n",
    "<p>&nbsp;2. Write a function that, given a graph g, a vertex i, and a vertex j, returns an Array containing the nodes on the shortest path from i to j. For example, for the example graph below, the shortest path from node 1 to node 8 is</p>\n",
    "<pre>Array[Int] = Array(3, 5, 6)</pre>\n",
    "<p>(assume that node ids' are integers for the purposes of this problem)</p>\n",
    "<p>3. Write a function that, given a graph g, returns a list (List[Array[Int]]) where each element is the shortest path between a pair of vertices. Construct this list recursively (does not need to be tail recursive). For the example graph below, this function should return:</p>\n",
    "<pre>List[Array[Int]] = List(Array(), Array(), Array(), Array(3), Array(3, 5), Array(3, 5, 6), <br />     Array(3, 5, 6), Array(), Array(), Array(), Array(4), Array(3, 5), Array(4, 5, 6), <br />     Array(3, 5, 6), Array(), Array(), Array(), Array(), Array(5), Array(5, 6), Array(5, 6), <br />     Array(), Array(), Array(), Array(), Array(5), Array(5, 6), Array(5, 6), Array(), Array(), <br />     Array(), Array(), Array(), Array(6), Array(6), Array(), Array(), Array(), Array(), Array(), <br />     Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), <br />     Array(), Array(), Array(), Array(), Array(), Array())</pre>\n",
    "<p><strong>Note</strong>: an empty Array signifies that either there is no path between a pair of nodes (e.g., between 8 and 1) or that the path length is 1 and there are no intermediate nodes (e.g. between 1 and 3). Each array only contains the intermediate nodes</p>\n",
    "<p>4. Remove (using filter) all empty arrays and then count the number of occurrences of every remaining node. For our example below, you may get (your values may be different because if there are two candidate shortest paths between a pair of nodes, the one that is included depends on the order in which worker nodes report their results!):</p>\n",
    "<pre>scala.collection.immutable.Map[Int,Int] = Map(3 -&gt; 5, 5 -&gt; 12, 6 -&gt; 10, 4 -&gt; 3)</pre>\n",
    "<p>(you might end up with a different data structure)</p>\n",
    "<p>5. Calculate the betweenness centrality by dividing each count by the number of possible paths. For vertices that are not in the above map, the count is 0 and the betweenness centrality is 0.0. For our example, you should get:</p>\n",
    "<h3>Array[Double] = Array(0.0, 0.0, 0.07142857142857142, 0.07142857142857142, 0.21428571428571427, <br />0.17857142857142858, 0.0, 0.0)<br /><br />EXAMPLE GRAPH</h3>\n",
    "<p><img src=\"betweenness.png\" alt=\"betweenness_sample_graph.png\" /></p>\n",
    "<p><strong>Cut and paste the code below into your Jupyter notebook. Quite a bit is already written for you, you need to fill in the missing parts</strong></p>\n",
    "<pre>import org.apache.spark.sql._<br />import org.apache.spark.sql.functions._<br />import org.graphframes._<br /><br /><br />val vertexArray = Array(<br />&nbsp; (1,1),<br />&nbsp; (2,2),<br />&nbsp; (3,3),<br />&nbsp; (4,4),<br />&nbsp; (5,5),<br />&nbsp; (6,6),<br />&nbsp; &nbsp; (7,7),<br />&nbsp; &nbsp; (8,8)<br />)<br /><br /><br />val edgeArray = Array(<br />&nbsp; (1, 3),<br />&nbsp; (2, 3),<br />&nbsp; (2, 4),<br />&nbsp; (4, 5),<br />&nbsp; (3, 5),<br />&nbsp; (5, 6),<br />&nbsp; (6, 7),<br />&nbsp; (6, 8),<br />&nbsp; (7, 8)<br />)<br /><br />val vertex_df = spark.createDataFrame(vertexArray).toDF(\"id\",\"v_desc\")<br />val edge_df = spark.createDataFrame(edgeArray).toDF(\"src\",\"dst\")<br /><br />val g = GraphFrame(vertex_df, edge_df)<br /><br />//Function to get all vertex pairs. This is written for you<br />def getAllVertexPairs(g: GraphFrame): Array[(Int,Int)] = {<br />&nbsp; &nbsp; def getAllPairs(nums: Seq[Int]) =<br />&nbsp; &nbsp; &nbsp; &nbsp; nums.flatMap(x =&gt; nums.map(y =&gt; (x,y))).filter(p=&gt;p._1 != p._2)<br /><br />&nbsp; &nbsp; val col_vals = g.vertices.select(\"id\").map(_.getInt(0)).collect.toSeq.toArray<br />&nbsp; &nbsp; val all_vertex_pairs = getAllPairs(col_vals).toArray<br />&nbsp; &nbsp; all_vertex_pairs<br />}<br /><br />//Function to get the shortest path between two vertices. This is also already written<br />//for you<br />//Note that this uses the bfs algorithm. So it will take some time to run and should<br />//not be run on large graphs!<br /><br />def getShortestPath(g: GraphFrame,i: Int, j: Int) = {<br />&nbsp; &nbsp; val path_df = g.bfs.fromExpr(s\"id=$i\").toExpr(s\"id=$j\").run()&nbsp;<br />&nbsp; &nbsp; if (path_df.count &gt; 0) {<br />&nbsp; &nbsp; &nbsp; &nbsp; val cols = path_df.columns.filter(n=&gt;n.contains(\"v\")).map(n=&gt;col(n+\".id\"))<br />&nbsp; &nbsp; &nbsp; &nbsp; val a = path_df.select(cols:_*).rdd.collect()(0).toSeq.toArray.map(e =&gt; e.toString.toInt)<br />&nbsp; &nbsp; &nbsp; &nbsp; a<br />&nbsp; &nbsp; }<br />&nbsp; &nbsp; else Array[Int]()<br />}<br /><br />//Function that returns all shortest paths<br />//You need to fill in the loop function<br />//Keep in mind:<br />//if the array a is empty, you should return an empty List<br />//if the array has one element, find the shortest path for that element<br />// and return a List of one element that contains the shortest path<br />//If the array has &gt; 1 element, find the shortest path for the first element<br />// in the list and return that shortest path CONS a call to loop with the remaining elements<br />//Use array.slice(start,end) to get the remaining elements<br />def getAllShortestPaths(g: GraphFrame):List[Array[Int]] &nbsp;= {<br />&nbsp; &nbsp; def loop(a: Array[(Int,Int)]):List[Array[Int]] = {<br />&nbsp; &nbsp; &nbsp; &nbsp; //YOU NEED TO DO THIS. SEE COMMENT IMMEDIATELY ABOVE<br /><br />&nbsp; &nbsp; }<br />&nbsp; &nbsp; val all_vertex_pairs = getAllVertexPairs(g)<br />&nbsp; &nbsp; loop(all_vertex_pairs)<br />}<br /><br />def getBetweenessCentrality(g: GraphFrame) = {<br />&nbsp; &nbsp; //get all shortest paths removing empty paths<br />&nbsp; &nbsp; val all_shortest_paths = //FILL THIS PART<br />&nbsp; &nbsp; //get all vertices in the graph in an array<br />&nbsp; &nbsp; //select the \"id\" column<br />&nbsp; &nbsp; //convert into an rdd<br />&nbsp; &nbsp; //convert each element into an Int<br />&nbsp; &nbsp; val vertices = //FILL THIS PART<br />&nbsp; &nbsp; //Calculate the denominator for betweenness centrality<br />&nbsp; &nbsp; val denominator = //n * (n-1) where n is number of vertices<br />&nbsp; &nbsp;&nbsp;<br />&nbsp; &nbsp; //for each vertex, calculate the betweenness centrality<br />&nbsp; &nbsp; //see notes below<br />&nbsp; &nbsp; vertices.map(v =&gt; //FILL THIS PART)<br />}<br /><br />//Result<br />val b = getBetweenessCentrality(g)<br />b.collect<br /><br />/*<br />NOTES:<br />NOTE 1<br />to get the count of the number of elements in an array, you can do the following:<br /><br />val x = Array(1,1,3,2,1,4,4,4)<br />x.groupBy(identity) groups array elements by value<br />x.groupBy(identity) returns scala.collection.immutable.Map[Int,Array[Int]] = Map(1 -&gt;&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Array(1, 1, 1), 3 -&gt; Array(3), 2 -&gt; Array(2), 4 -&gt; Array(4, 4, 4))<br />x.groupBy(identity).mapValues(_.size) calculates the number of each element<br />x.groupBy(identity).mapValues(_.size) returns Map(1 -&gt; 3, 3 -&gt; 1, 2 -&gt; 1, 4 -&gt; 3)<br /><br />You could also use map and reduceByKey, but this may be easier<br /><br />NOTE 2:<br />Given&nbsp;<br />val y = Map(1 -&gt; 3, 3 -&gt; 1, 2 -&gt; 1, 4 -&gt; 3)<br />y.getOrElse(3,0) 1 because 3 -&gt; 1 in y<br />y.getOrElse(8,0) returns 0 because 8 is not a key in y<br /><br />NOTE 3:<br />If you get a serializability error, make sure that any function you're calling,<br />&nbsp; even an anonymous one, has all the data necessary to compute a value.<br /><br /><br /></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81232aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.graphframes._\n",
    "\n",
    "\n",
    "val vertexArray = Array(\n",
    "  (1,1),\n",
    "  (2,2),\n",
    "  (3,3),\n",
    "  (4,4),\n",
    "  (5,5),\n",
    "  (6,6),\n",
    "    (7,7),\n",
    "    (8,8)\n",
    ")\n",
    "\n",
    "//val vertexArray = Array(1,2,3,4,5,6,7,8)\n",
    "\n",
    "val edgeArray = Array(\n",
    "  (1, 3),\n",
    "  (2, 3),\n",
    "  (2, 4),\n",
    "  (4, 5),\n",
    "  (3, 5),\n",
    "  (5, 6),\n",
    "  (6, 7),\n",
    "  (6, 8),\n",
    "  (7, 8)\n",
    ")\n",
    "\n",
    "val vertex_df = spark.createDataFrame(vertexArray).toDF(\"id\",\"v_desc\").drop(\"v_desc\")\n",
    "val edge_df = spark.createDataFrame(edgeArray).toDF(\"src\",\"dst\")\n",
    "\n",
    "val g = GraphFrame(vertex_df, edge_df)\n",
    "\n",
    "//All vertex pairs\n",
    "def getAllVertexPairs(g: GraphFrame): Array[(Int,Int)] = {\n",
    "    def getAllPairs(nums: Seq[Int]) =\n",
    "        nums.flatMap(x => nums.map(y => (x,y))).filter(p=>p._1 != p._2)\n",
    "\n",
    "    val col_vals = g.vertices.select(\"id\").map(_.getInt(0)).collect.toSeq.toArray\n",
    "    val all_vertex_pairs = getAllPairs(col_vals).toArray\n",
    "    all_vertex_pairs\n",
    "}\n",
    "\n",
    "//getAllVertexPairs(g)\n",
    "\n",
    "def getShortestPath(g: GraphFrame,i: Int, j: Int) = {\n",
    "    val path_df = g.bfs.fromExpr(s\"id=$i\").toExpr(s\"id=$j\").run() \n",
    "    if (path_df.count > 0) {\n",
    "        val cols = path_df.columns.filter(n=>n.contains(\"v\")).map(n=>col(n+\".id\"))\n",
    "        val a = path_df.select(cols:_*).rdd.collect()(0).toSeq.toArray.map(e => e.toString.toInt)\n",
    "        a\n",
    "    }\n",
    "    else Array[Int]()\n",
    "}\n",
    "\n",
    "def getAllShortestPaths(g: GraphFrame):List[Array[Int]]  = {\n",
    "    def loop(a: Array[(Int,Int)]):List[Array[Int]] = {\n",
    "        if (a.length == 0) List[Array[Int]]()\n",
    "        else {\n",
    "            val sp = getShortestPath(g,a(0)._1,a(0)._2)\n",
    "            if (a.length == 1)\n",
    "                List(sp)\n",
    "            else sp ::loop(a.slice(1,a.length))\n",
    "        }\n",
    "\n",
    "    }\n",
    "    val all_vertex_pairs = getAllVertexPairs(g)\n",
    "    loop(all_vertex_pairs)\n",
    "}\n",
    "\n",
    "def getBetweenessCentrality(g: GraphFrame) = {\n",
    "    //get all shortest paths removing empty paths\n",
    "    val all_shortest_paths = getAllShortestPaths(g).filter(p => p.length > 0)\n",
    "    val vertices = g.vertices.select(\"id\").rdd.map(v=>v(0).toString.toInt)\n",
    "    val denominator = vertices.count * (vertices.count -1)\n",
    "    vertices.map(v => all_shortest_paths.flatten.groupBy(identity).mapValues(_.size).getOrElse(v,0)*1.0/denominator)\n",
    "}\n",
    "\n",
    "\n",
    "val b = getBetweenessCentrality(g)\n",
    "b.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a757bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c94ad",
   "metadata": {},
   "source": [
    "<h1>SAMPLE QUESTION 3</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea7208",
   "metadata": {},
   "source": [
    "A social network contains some information about people  in a Demographics object and some information about their relationships in a Connection object. The relationship information contains information on the strength of their relationship (strength attribute) and the probability that a message received by a person will be shared with a connection (the msgProbability attribute). \n",
    "\n",
    "You have been hired by an advertising company to explore this data. The goal of your research is to identify users that the company should target to diffuse its message. A \"good diffuser\" has the following characteristics:\n",
    "\n",
    "1. Must be  older than 21 years\n",
    "\n",
    "2. Must have an income over $20,000\n",
    "\n",
    "3. Must have friends that are 21 years or older and the relationship strength should be greater than or equal to 3\n",
    "\n",
    "Write code that returns the vertex id and the number of friends for each user that satisfies the above criteria. You must use aggregateMessages for this program!\n",
    "\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "case class Demographics(age: Int,gender: Char, income: Double)\n",
    "case class Person(name: String,demographics: Demographics)\n",
    "case class Connection(strength: Int,msgProbability: Double)\n",
    "\n",
    "\n",
    "//Example users and connections\n",
    "\n",
    "val users = Array(\n",
    "(1L, Person(\"Alice\",Demographics(28,'F',150000.0))),\n",
    "(2L, Person(\"Bob\",Demographics(27,'M',50000.0))),\n",
    "(3L, Person(\"Charlie\",Demographics(65,'M',250000.0))),\n",
    "(4L, Person(\"David\",Demographics(42,'M',750000.0))),\n",
    "(5L, Person(\"Ed\",Demographics(55,'M',25000.0))),\n",
    "(6L, Person(\"Fran\",Demographics(50,'F',3150000.0))),\n",
    "(7L, Person(\"Jack\",Demographics(17,'M',5000.0))),\n",
    "(8L, Person(\"Jill\",Demographics(16,'F',1000.0)))\n",
    ")\n",
    "\n",
    "val connections = Array(\n",
    "Edge(2L, 1L, Connection(7,.2)),\n",
    "Edge(2L, 4L, Connection(2,.7)),\n",
    "Edge(3L, 2L, Connection(4,.31)),\n",
    "Edge(3L, 6L, Connection(3,.22)),\n",
    "Edge(4L, 1L, Connection(1,.12)),\n",
    "Edge(5L, 2L, Connection(2,.45)),\n",
    "Edge(5L, 3L, Connection(8,.91))\n",
    ")\n",
    "\n",
    "val vertexRDD: RDD[(Long, Person)] = sc.parallelize(users)\n",
    "val edgeRDD: RDD[Edge[Connection]] = sc.parallelize(connections)\n",
    "\n",
    "val social_graph = Graph(vertexRDD,edgeRDD)\n",
    "\n",
    "Your code should return an RDD containing:\n",
    "\n",
    "Array((2,1), (3,2), (5,2))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaee3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.149:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1670800381956)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "case class Demographics(age: Int,gender: Char, income: Double)\n",
    "case class Person(name: String,demographics: Demographics)\n",
    "case class Connection(strength: Int,msgProbability: Double)\n",
    "\n",
    "\n",
    "//Example users and connections\n",
    "\n",
    "val users = Array(\n",
    "(1L, Person(\"Alice\",Demographics(28,'F',150000.0))),\n",
    "(2L, Person(\"Bob\",Demographics(27,'M',50000.0))),\n",
    "(3L, Person(\"Charlie\",Demographics(65,'M',250000.0))),\n",
    "(4L, Person(\"David\",Demographics(42,'M',750000.0))),\n",
    "(5L, Person(\"Ed\",Demographics(55,'M',25000.0))),\n",
    "(6L, Person(\"Fran\",Demographics(50,'F',3150000.0))),\n",
    "(7L, Person(\"Jack\",Demographics(17,'M',5000.0))),\n",
    "(8L, Person(\"Jill\",Demographics(16,'F',1000.0)))\n",
    ")\n",
    "\n",
    "val connections = Array(\n",
    "Edge(2L, 1L, Connection(7,.2)),\n",
    "Edge(2L, 4L, Connection(2,.7)),\n",
    "Edge(3L, 2L, Connection(4,.31)),\n",
    "Edge(3L, 6L, Connection(3,.22)),\n",
    "Edge(4L, 1L, Connection(1,.12)),\n",
    "Edge(5L, 2L, Connection(2,.45)),\n",
    "Edge(5L, 3L, Connection(8,.91))\n",
    ")\n",
    "\n",
    "val vertexRDD: RDD[(Long, Person)] = sc.parallelize(users)\n",
    "val edgeRDD: RDD[Edge[Connection]] = sc.parallelize(connections)\n",
    "\n",
    "val social_graph = Graph(vertexRDD,edgeRDD)\n",
    "\n",
    "val over21_friends = social_graph.aggregateMessages[Int](\n",
    "    triplet => {\n",
    "    if (triplet.dstAttr.demographics.age >= 21)\n",
    "       {\n",
    "      triplet.sendToSrc(1);\n",
    "    }\n",
    "  },\n",
    "  (a, b) => (a+b)).map(t => t._1).collect\n",
    "\n",
    "val result = social_graph.aggregateMessages[Int](\n",
    "  triplet => {\n",
    "    if (triplet.srcAttr.demographics.age > 21\n",
    "      && triplet.srcAttr.demographics.income > 20000\n",
    "      && triplet.attr.strength >= 3\n",
    "      && (over21_friends contains triplet.srcId) )\n",
    "      {\n",
    "      triplet.sendToDst(1);\n",
    "    }\n",
    "  },\n",
    "  (a, b) => (a+b)\n",
    ")\n",
    "result.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10282ab",
   "metadata": {},
   "source": [
    "<h1>SAMPLE QUESTION 4</h1>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f37b9631",
   "metadata": {},
   "source": [
    "A social network contains some information about people  in a Demographics object and some information about their relationships in a Connection object. The relationship information contains information on the strength of their relationship (strength attribute) and the probability that a message received by a person will be shared with a connection (the msgProbability attribute). \n",
    "\n",
    "You have been hired by an advertising company to explore this data. The goal of your research is to explore useful paths in this graph. In particular, the company wants you to calculate the highest probability path from a given person to every other person in the graph. \n",
    "\n",
    "Write a function get_max_path that takes two arguments, the social network graph and the starting vertex id, and returns a GraphX VertexRDD. The returned VertexRDD should contain vertexids and probability pairs. Each probability should reflect the probability of a message getting to the vertex from the start vertex along a single path with the highest probability. The probability associated with a path is the product of the probabilities on all the edges on that path. For example, if vertex 1 to vertex 2 has a probability of 0.3. And vertex 2 to vertex 3 has a probability of 0.4. Then the probability that a message from vertex 1 will arrive at vertex 3 via vertex 2 is 0.3 * 0.4 or 0.12.\n",
    "\n",
    "Use the data below to test out your code. The vertex RDD returned by the function  should contain:\n",
    "\n",
    "//For the function call get_max_path(social_graph,5).collect\n",
    "//The returned value should be\n",
    "Array[(org.apache.spark.graphx.VertexId, Double)] = Array((1,0.09000000000000001), (2,0.45), (3,0.91), (4,0.315), (5,1.0), (6,0.20020000000000002), (7,0.0), (8,0.0))\n",
    "\n",
    "//CODE: You need to:\n",
    "//add the argument types for the get_max_path function\n",
    "//create the initial graph for pregel\n",
    "// write the vertex program\n",
    "// write the send message function\n",
    "// write the merge message functions\n",
    "// parameterize the call to the pregel function appropriately\n",
    "// return the vertices of the solution graph\n",
    "\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "case class PersonData(age: Int,gender: Char, income: Double)\n",
    "case class Client(name: String,data: PersonData)\n",
    "case class Relationship(strength: Int,msgProbability: Double)\n",
    "\n",
    "\n",
    "//Example users and connections\n",
    "\n",
    "val people = Array(\n",
    "  (1L, Client(\"Alice\",PersonData(28,'F',150000.0))),\n",
    "  (2L, Client(\"Bob\",PersonData(27,'M',50000.0))),\n",
    "  (3L, Client(\"Charlie\",PersonData(65,'M',250000.0))),\n",
    "  (4L, Client(\"David\",PersonData(42,'M',750000.0))),\n",
    "  (5L, Client(\"Ed\",PersonData(55,'M',25000.0))),\n",
    "  (6L, Client(\"Fran\",PersonData(50,'F',3150000.0))),\n",
    "  (7L, Client(\"Jack\",PersonData(17,'M',5000.0))),\n",
    "  (8L, Client(\"Jill\",PersonData(16,'F',1000.0)))\n",
    ")\n",
    "\n",
    "val relationships = Array(\n",
    "  Edge(2L, 1L, Relationship(7,.2)),\n",
    "  Edge(2L, 4L, Relationship(2,.7)),\n",
    "  Edge(3L, 2L, Relationship(4,.31)),\n",
    "  Edge(3L, 6L, Relationship(3,.22)),\n",
    "  Edge(4L, 1L, Relationship(1,.12)),\n",
    "  Edge(5L, 2L, Relationship(2,.45)),\n",
    "  Edge(5L, 3L, Relationship(8,.91))\n",
    ")\n",
    "\n",
    "val vertexRDD: RDD[(Long, Client)] = sc.parallelize(people)\n",
    "val edgeRDD: RDD[Edge[Relationship]] = sc.parallelize(relationships)\n",
    "\n",
    "val social_graph = Graph(vertexRDD,edgeRDD)\n",
    "\n",
    "//Fill in the types for social_graph and sourceId\n",
    "def get_max_path(social_graph: ,sourceId: ) = {\n",
    "    val initialGraph = //Initialize the graph\n",
    "    val vertexProgram = //Write the vertexProgram\n",
    "    val sendMsg = //Write the sendMsg function\n",
    "    val mrgMsg = //Write the mrgMsg function\n",
    "    \n",
    "\n",
    "    //Add the arguments for pregel\n",
    "    val maxPath = initialGraph.pregel()()\n",
    "    //return the vertices\n",
    "}\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83debb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "case class PersonData(age: Int,gender: Char, income: Double)\n",
    "case class Client(name: String,data: PersonData)\n",
    "case class Relationship(strength: Int,msgProbability: Double)\n",
    "\n",
    "\n",
    "//Example users and connections\n",
    "\n",
    "val people = Array(\n",
    "  (1L, Client(\"Alice\",PersonData(28,'F',150000.0))),\n",
    "  (2L, Client(\"Bob\",PersonData(27,'M',50000.0))),\n",
    "  (3L, Client(\"Charlie\",PersonData(65,'M',250000.0))),\n",
    "  (4L, Client(\"David\",PersonData(42,'M',750000.0))),\n",
    "  (5L, Client(\"Ed\",PersonData(55,'M',25000.0))),\n",
    "  (6L, Client(\"Fran\",PersonData(50,'F',3150000.0))),\n",
    "  (7L, Client(\"Jack\",PersonData(17,'M',5000.0))),\n",
    "  (8L, Client(\"Jill\",PersonData(16,'F',1000.0)))\n",
    ")\n",
    "\n",
    "val relationships = Array(\n",
    "  Edge(2L, 1L, Relationship(7,.2)),\n",
    "  Edge(2L, 4L, Relationship(2,.7)),\n",
    "  Edge(3L, 2L, Relationship(4,.31)),\n",
    "  Edge(3L, 6L, Relationship(3,.22)),\n",
    "  Edge(4L, 1L, Relationship(1,.12)),\n",
    "  Edge(5L, 2L, Relationship(2,.45)),\n",
    "  Edge(5L, 3L, Relationship(8,.91))\n",
    ")\n",
    "\n",
    "val vertexRDD: RDD[(Long, Client)] = sc.parallelize(people)\n",
    "val edgeRDD: RDD[Edge[Relationship]] = sc.parallelize(relationships)\n",
    "\n",
    "val social_graph = Graph(vertexRDD,edgeRDD)\n",
    "\n",
    "//Fill in the types for social_graph and sourceId\n",
    "def get_max_path(social_graph: Graph[Client,Relationship],sourceId: VertexId) = {\n",
    "    val initialGraph = social_graph.mapVertices((id,_) => if (id == sourceId) 1.0 else 0.0)\n",
    "    val vertexProgram = (id: VertexId, prob: Double, newProb: Double) => math.max(prob, newProb)\n",
    "    val sendMsg = (triplet:EdgeTriplet[Double,Relationship]) => {  // Send Message\n",
    "        val edgeProb = triplet.attr.msgProbability\n",
    "        if (triplet.srcAttr == 0.0) {\n",
    "          Iterator.empty\n",
    "        } else if (triplet.srcAttr*edgeProb > triplet.dstAttr) {\n",
    "          Iterator((triplet.dstId,triplet.srcAttr*edgeProb))\n",
    "        } else {\n",
    "          Iterator.empty\n",
    "        }\n",
    "      }\n",
    "    val mrgMsg = (a: Double, b: Double) => math.max(a, b)\n",
    "    \n",
    "    val maxPath = initialGraph.pregel(0.0,3)(vertexProgram,sendMsg,mrgMsg)\n",
    "    maxPath.vertices\n",
    "}\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7aa423",
   "metadata": {},
   "source": [
    "<h1>SAMPLE QUESTION 5</h1>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbc6bd09",
   "metadata": {},
   "source": [
    "A movie rating company has built a collaborative filtering model that appears to return good results on a testing dataset. The company has run a model on the training dataset and has come up with some initial results on a testing dataset but wants to compare the results against a baseline. The baseline they decide upon is to use the average rating for each movie as the predicted rating for any unseen movie for a user.\n",
    "\n",
    "Given the following dataframes:\n",
    "\n",
    "train_df.printSchema\n",
    "root\n",
    "|-- user_id: integer (nullable = false)\n",
    "|-- movie_id: integer (nullable = false)\n",
    "|-- rating: double (nullable = false)\n",
    "\n",
    "test_df.printSchema\n",
    "root\n",
    "|-- user_id: integer (nullable = false)\n",
    "|-- movie_id: integer (nullable = false)\n",
    "|-- rating: double (nullable = false)\n",
    "\n",
    "Write the code that returns the rmse for the baseline prediction described above. The rough steps you need to follow are:\n",
    "\n",
    "create a new dataframe with two columns, movie_id and avg_rating\n",
    "use these ratings as the prediction for each row in the test_df (use the dataframe join function). \n",
    "use RegressionEvaluator to get the rmse for the baseline model.\n",
    "DOCUMENTATION:\n",
    "\n",
    "RegressionEvaluator (Links to an external site.) (https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/ml/evaluation/RegressionEvaluator.html (Links to an external site.))\n",
    "\n",
    "For example, given train_df and test_df (assume they are the same!)\n",
    "\n",
    "user_id\tmovie_id\trating\n",
    "1\t1\t3.2\n",
    "1\t2\t4.3\n",
    "2\t4\t1.9\n",
    "2\t2\t3.3\n",
    "2\t1\t4.1\n",
    "3\t15\t4.5\n",
    "3\t2\t4.3\n",
    " \n",
    "\n",
    "Steps 1 should return a df with the following average ratings\n",
    "\n",
    " \n",
    "\n",
    "movie_id\tavg_rating\n",
    "1\t3.65\n",
    "15\t4.5\n",
    "4\t1.9\n",
    "2\t3.9666666666\n",
    " \n",
    "\n",
    "and step 2 should use that df to add a prediction column to the original dataframe (the baseline predictions)\n",
    "\n",
    " \n",
    "\n",
    "user_id\tmovie_id\trating\tprediction\n",
    "1\t1\t3.2\t3.65\n",
    "1\t2\t4.3\t3.966666666\n",
    "2\t4\t1.9\t1.9\n",
    "2\t2\t3.3\t3.966666666\n",
    "2\t1\t4.1\t3.65\n",
    "3\t15\t4.5\t4.5\n",
    "3\t2\t4.3\t3.966666666\n",
    " \n",
    "\n",
    "Finally, using RegressionEvaluator, the reported rmse should be:\n",
    "\n",
    "0.3912738658474881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cf84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.createDataFrame(Seq(\n",
    "(1,1,3.2),\n",
    "(1,2,4.3),\n",
    "(2,4,1.9),\n",
    "(2,2,3.3),\n",
    "(2,1,4.1),\n",
    "(3,15,4.5),\n",
    "(3,2,4.3)))\n",
    ".toDF(\"user_id\",\"movie_id\",\"rating\")\n",
    "val train_df = df\n",
    "val test_df = df\n",
    "val avg_df = train_df.groupBy(\"movie_id\").avg(\"rating\")\n",
    "val new_df = test_df.join(avg_df,Seq(\"movie_id\")).withColumnRenamed(\"avg(rating)\",\"prediction\")\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "val evaluator = new RegressionEvaluator()\n",
    ".setMetricName(\"rmse\")\n",
    ".setLabelCol(\"rating\")\n",
    ".setPredictionCol(\"prediction\")\n",
    "\n",
    "val rmse = evaluator.evaluate(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f18c77c",
   "metadata": {},
   "source": [
    "<h1>SAMPLE QUESTION 6</h1>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e90d4c6b",
   "metadata": {},
   "source": [
    "A Spark DataFrame containing information about restaurants  has the following schema:\n",
    "\n",
    "root\n",
    " |-- name: string (nullable = true)\n",
    " |-- reviews: struct (nullable = true)\n",
    " |    |-- count: integer (nullable = false)\n",
    " |    |-- rating: double (nullable = false)\n",
    " |-- serves: struct (nullable = true)\n",
    " |    |-- alcohol: boolean (nullable = false)\n",
    " |    |-- vegetarian: boolean (nullable = false)\n",
    "Using a Spark SQL UDF, add a column score to this dataframe that scores each restaurant according to the following criteria:\n",
    "\n",
    "1. If the restaurant serves alcohol, it gets 1 point\n",
    "\n",
    "2. If the restaurant serves vegetarian food, it gets 1 point\n",
    "\n",
    "3. Half of the restaurant rating is added to the score\n",
    "\n",
    "For example, if the restaurant serves alcohol, does not cater to vegetarians, and has an average rating of 3, the score for the restaurant is 1 + 0 + 1.5 or 2.5. \n",
    "\n",
    "Example, for the data frame generated by the following code:\n",
    "\n",
    "val dataRDD = spark.sparkContext.makeRDD(\n",
    "\"\"\"[{\"name\":\"Le Monde\",\"reviews\":{\"count\":14,\"rating\":3.2},\"serves\":{\"alcohol\":true,\"vegetarian\":false}} ,\n",
    "{\"name\":\"Junzi Kitchen\",\"reviews\":{\"count\":7,\"rating\":4.5},\"serves\":{\"alcohol\":false,\"vegetarian\":true}},\n",
    "{\"name\":\"Atlas Kitchen\",\"reviews\":{\"count\":9,\"rating\":2.9},\"serves\":{\"alcohol\":true,\"vegetarian\":true}}]\"\"\":: Nil)\n",
    "val df = spark.read.json(dataRDD)\n",
    "\n",
    "df.show\n",
    "+-------------+---------+-------------+\n",
    "|         name|  reviews|       serves|\n",
    "+-------------+---------+-------------+\n",
    "|     Le Monde|[14, 3.2]|[true, false]|\n",
    "|Junzi Kitchen| [7, 4.5]|[false, true]|\n",
    "|Atlas Kitchen| [9, 2.9]| [true, true]|\n",
    "+-------------+---------+-------------+\n",
    "You should get the resulting new dataframe:\n",
    "\n",
    "+-------------+---------+-------------+-----+\n",
    "|         name|  reviews|       serves|score|\n",
    "+-------------+---------+-------------+-----+\n",
    "|     Le Monde|[14, 3.2]|[true, false]|  2.6|\n",
    "|Junzi Kitchen| [7, 4.5]|[false, true]| 3.25|\n",
    "|Atlas Kitchen| [9, 2.9]| [true, true]| 3.45|\n",
    "+-------------+---------+-------------+-----+\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val dataRDD = spark.sparkContext.makeRDD(\n",
    "\"\"\"[{\"name\":\"Le Monde\",\"reviews\":{\"count\":14,\"rating\":3.2},\"serves\":{\"alcohol\":true,\"vegetarian\":false}} ,\n",
    "{\"name\":\"Junzi Kitchen\",\"reviews\":{\"count\":7,\"rating\":4.5},\"serves\":{\"alcohol\":false,\"vegetarian\":true}},\n",
    "{\"name\":\"Atlas Kitchen\",\"reviews\":{\"count\":9,\"rating\":2.9},\"serves\":{\"alcohol\":true,\"vegetarian\":true}}]\"\"\":: Nil)\n",
    "val df = spark.read.json(dataRDD)\n",
    "\n",
    "import org.apache.spark.sql.functions.udf\n",
    "def score(alc: Boolean, vg: Boolean, ra: Double) = {\n",
    "    var score = 0.0\n",
    "    if (alc) score=1\n",
    "    if (vg) score=score+1\n",
    "    score+ra/2.0\n",
    "    \n",
    "}\n",
    "val score_udf = udf(score _)\n",
    "\n",
    "val df2 = df.withColumn(\"score\",score_udf($\"serves.alcohol\",$\"serves.vegetarian\",$\"reviews.rating\"))\n",
    "\n",
    "df2.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62acaa5d",
   "metadata": {},
   "source": [
    "<h1>SAMPLE QUESTION 7</h1>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72efd292",
   "metadata": {},
   "source": [
    "A hedge fund is using historical pricing data to build a new trading model. As a part of the feature engineering process, you've been tasked to do the following:\n",
    "\n",
    "1. Create two new columns, \"ma8\" and \"ma13\", containing the trailing 8 day and 13 day moving averages of the price. You must use the Window function (see below) to create rolling windows to calculate the trailing moving averages. The moving average for a day should include the price for that day\n",
    "2. Create a new column \"diff\" that contains ma8 - ma13\n",
    "3. Create a new column \"binary\" that contains 1 if the diff is > 0 and 0 otherwise. You must use a UDF to do this.\n",
    "4. Use the Spark feature transformer <a href=\"https://spark.apache.org/docs/2.4.7/ml-features#quantilediscretizer\">Quantile Discretizer</a> to decile each diff into a column named \"decile\"\n",
    "\n",
    "Window function:\n",
    "\n",
    "Spark SQL contains a Window function in its sql.expressions library. The Window function can be applied on a dataframe to create a rolling window that can be used to compute moving averages. See <a href=\"https://sparkbyexamples.com/spark/spark-sql-window-functions/\">https://sparkbyexamples.com/spark/spark-sql-window-functions/</a> for additional details or walk through the example below.\n",
    "\n",
    "val df = sc.parallelize(Array((1,2,3),(2,3,4),(3,4,5),\n",
    "                              (4,5,6),(5,6,7),(6,7,8),\n",
    "                             (7,8,9),(9,10,11))).toDF(\"c1\",\"c2\",\"c3\")\n",
    "//For row 5, value in df_ma will be the average of rows 2,3,4.                            \n",
    "val df_ma = df.withColumn(\"ma\",avg(df(\"c1\")).over(Window.orderBy(\"c2\").rowsBetween(-3,-1)))\n",
    "df_ma.show\n",
    "\n",
    "The file, AAPL.csv,  Download AAPL.csv,contains sample data. The file, aapl_result.csv  Download aapl_result.csvcontains what your code should output (as a dataframe). The following code reads the file into a dataframe df (assuming that the file is in the same directory as your notebook):\n",
    "\n",
    "val df = spark.read.option(\"header\",\"true\").option(\"inferschema\",\"true\").csv(\"AAPL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6257bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.56.160.213:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1671142485698)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/15 17:14:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:14:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:14:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:14:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:14:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:14:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:14:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:14:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:14:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:14:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+-------------------+--------+-------------------+-------------------+----------------------+-----+-------+\n",
      "|date               |price   |ma8                |ma13               |diff                  |label|deciles|\n",
      "+-------------------+--------+-------------------+-------------------+----------------------+-----+-------+\n",
      "|1980-12-12 00:00:00|0.1006  |0.1006             |0.1006             |0.0                   |0.0  |4.0    |\n",
      "|1980-12-15 00:00:00|0.095352|0.09797600000000001|0.09797600000000001|0.0                   |0.0  |4.0    |\n",
      "|1980-12-16 00:00:00|0.088353|0.09476833333333334|0.09476833333333334|0.0                   |0.0  |4.0    |\n",
      "|1980-12-17 00:00:00|0.09054 |0.09371125000000001|0.09371125000000001|0.0                   |0.0  |4.0    |\n",
      "|1980-12-18 00:00:00|0.093165|0.093602           |0.093602           |0.0                   |0.0  |4.0    |\n",
      "|1980-12-19 00:00:00|0.098851|0.09447683333333334|0.09447683333333334|0.0                   |0.0  |4.0    |\n",
      "|1980-12-22 00:00:00|0.103662|0.09578900000000001|0.09578900000000001|0.0                   |0.0  |4.0    |\n",
      "|1980-12-23 00:00:00|0.108036|0.09731987500000001|0.09731987500000001|0.0                   |0.0  |4.0    |\n",
      "|1980-12-24 00:00:00|0.113722|0.09896012500000001|0.09914233333333335|-1.822083333333363E-4 |0.0  |4.0    |\n",
      "|1980-12-26 00:00:00|0.124219|0.1025685          |0.10165000000000002|9.184999999999888E-4  |1.0  |5.0    |\n",
      "|1980-12-29 00:00:00|0.125969|0.1072705          |0.10386081818181819|0.0034096818181818117 |1.0  |6.0    |\n",
      "|1980-12-30 00:00:00|0.122907|0.111316375        |0.10544800000000003|0.005868374999999967  |1.0  |6.0    |\n",
      "|1980-12-31 00:00:00|0.119408|0.11459674999999998|0.10652184615384618|0.008074903846153808  |1.0  |6.0    |\n",
      "|1981-01-02 00:00:00|0.12072 |0.11733037499999999|0.10806953846153847|0.00926083653846152   |1.0  |7.0    |\n",
      "|1981-01-05 00:00:00|0.118096|0.119134625        |0.10981907692307692|0.009315548076923072  |1.0  |7.0    |\n",
      "|1981-01-06 00:00:00|0.112847|0.119736           |0.11170323076923075|0.008032769230769246  |1.0  |6.0    |\n",
      "|1981-01-07 00:00:00|0.108036|0.11902525         |0.1130490769230769 |0.005976173076923094  |1.0  |6.0    |\n",
      "|1981-01-08 00:00:00|0.105849|0.116729           |0.11402476923076922|0.002704230769230778  |1.0  |5.0    |\n",
      "|1981-01-09 00:00:00|0.111535|0.11492474999999999|0.11500046153846152|-7.571153846153145E-5 |0.0  |4.0    |\n",
      "|1981-01-12 00:00:00|0.11066 |0.113393875        |0.11553876923076922|-0.0021448942307692176|0.0  |3.0    |\n",
      "|1981-01-13 00:00:00|0.106724|0.11180837499999999|0.11543784615384614|-0.0036294711538461533|0.0  |2.0    |\n",
      "|1981-01-14 00:00:00|0.107161|0.1101135          |0.11493315384615385|-0.0048196538461538485|0.0  |2.0    |\n",
      "|1981-01-15 00:00:00|0.109348|0.10902            |0.11378923076923077|-0.004769230769230762 |0.0  |2.0    |\n",
      "|1981-01-16 00:00:00|0.108473|0.10847325         |0.11244338461538463|-0.003970134615384632 |0.0  |2.0    |\n",
      "|1981-01-19 00:00:00|0.115034|0.109348           |0.11183776923076925|-0.0024897692307692537|0.0  |3.0    |\n",
      "|1981-01-20 00:00:00|0.111535|0.11005875000000001|0.11123215384615384|-0.001173403846153831 |0.0  |3.0    |\n",
      "|1981-01-21 00:00:00|0.113722|0.11033212499999999|0.11069384615384617|-3.617211538461812E-4 |0.0  |4.0    |\n",
      "|1981-01-22 00:00:00|0.115034|0.11087887499999999|0.11045830769230772|4.2056730769227113E-4 |1.0  |4.0    |\n",
      "|1981-01-23 00:00:00|0.114596|0.11186287499999999|0.11059284615384615|0.001270028846153834  |1.0  |5.0    |\n",
      "|1981-01-26 00:00:00|0.112847|0.11257362500000001|0.11096292307692307|0.0016107019230769404 |1.0  |5.0    |\n",
      "|1981-01-27 00:00:00|0.111972|0.112901625        |0.11143392307692307|0.0014677019230769361 |1.0  |5.0    |\n",
      "|1981-01-28 00:00:00|0.108473|0.112901625        |0.11119838461538459|0.0017032403846154176 |1.0  |5.0    |\n",
      "|1981-01-29 00:00:00|0.104537|0.11158950000000001|0.1107273846153846 |8.621153846154056E-4  |1.0  |5.0    |\n",
      "|1981-01-30 00:00:00|0.098851|0.110004           |0.11012176923076923|-1.1776923076922685E-4|0.0  |4.0    |\n",
      "|1981-02-02 00:00:00|0.093165|0.107434375        |0.10904515384615386|-0.0016107788461538625|0.0  |3.0    |\n",
      "|1981-02-03 00:00:00|0.096664|0.10513812500000001|0.10806946153846156|-0.002931336538461546 |0.0  |3.0    |\n",
      "|1981-02-04 00:00:00|0.100163|0.103334           |0.10743023076923078|-0.0040962307692307826|0.0  |2.0    |\n",
      "|1981-02-05 00:00:00|0.100163|0.1017485          |0.10628630769230771|-0.004537807692307702 |0.0  |2.0    |\n",
      "|1981-02-06 00:00:00|0.1006  |0.100327           |0.10544515384615386|-0.005118153846153856 |0.0  |2.0    |\n",
      "|1981-02-09 00:00:00|0.095352|0.09868687500000001|0.10403207692307694|-0.005345201923076928 |0.0  |2.0    |\n",
      "|1981-02-10 00:00:00|0.095352|0.09753875         |0.10251807692307696|-0.004979326923076968 |0.0  |2.0    |\n",
      "|1981-02-11 00:00:00|0.09229 |0.096718625        |0.10080223076923077|-0.0040836057692307665|0.0  |2.0    |\n",
      "|1981-02-12 00:00:00|0.091415|0.096499875        |0.0991536153846154 |-0.0026537403846153967|0.0  |3.0    |\n",
      "|1981-02-13 00:00:00|0.089228|0.095570375        |0.09740407692307693|-0.0018337019230769275|0.0  |3.0    |\n",
      "|1981-02-17 00:00:00|0.091415|0.094476875        |0.09609192307692307|-0.001615048076923073 |0.0  |3.0    |\n",
      "|1981-02-18 00:00:00|0.095352|0.0938755          |0.09538538461538462|-0.0015098846153846213|0.0  |3.0    |\n",
      "|1981-02-19 00:00:00|0.089665|0.092508625        |0.09467876923076925|-0.0021701442307692498|0.0  |3.0    |\n",
      "|1981-02-20 00:00:00|0.084854|0.091196375        |0.09403946153846156|-0.0028430865384615617|0.0  |3.0    |\n",
      "|1981-02-23 00:00:00|0.086166|0.09004812499999999|0.09323192307692309|-0.0031837980769230945|0.0  |2.0    |\n",
      "|1981-02-24 00:00:00|0.083105|0.08889999999999999|0.09191976923076924|-0.0030197692307692425|0.0  |2.0    |\n",
      "|1981-02-25 00:00:00|0.088353|0.08851724999999999|0.09101130769230768|-0.0024940576923076913|0.0  |3.0    |\n",
      "|1981-02-26 00:00:00|0.089665|0.08857187500000001|0.09017015384615384|-0.001598278846153836 |0.0  |3.0    |\n",
      "|1981-02-27 00:00:00|0.092727|0.08873587499999999|0.08996823076923076|-0.0012323557692307668|0.0  |3.0    |\n",
      "|1981-03-02 00:00:00|0.093165|0.0884625          |0.08979999999999998|-0.0013374999999999776|0.0  |3.0    |\n",
      "|1981-03-03 00:00:00|0.091853|0.08873599999999998|0.08976638461538461|-0.0010303846153846274|0.0  |3.0    |\n",
      "|1981-03-04 00:00:00|0.090977|0.089501375        |0.0897326923076923 |-2.3131730769231085E-4|0.0  |4.0    |\n",
      "|1981-03-05 00:00:00|0.09054 |0.09004812499999999|0.0898336153846154 |2.1450961538459212E-4 |1.0  |4.0    |\n",
      "|1981-03-06 00:00:00|0.089665|0.090868125        |0.08969899999999997|0.0011691250000000208 |1.0  |5.0    |\n",
      "|1981-03-09 00:00:00|0.082667|0.090157375        |0.08872323076923078|0.0014341442307692215 |1.0  |5.0    |\n",
      "|1981-03-10 00:00:00|0.07873 |0.0887905          |0.0878820769230769 |9.084230769230983E-4  |1.0  |5.0    |\n",
      "|1981-03-11 00:00:00|0.075669|0.08665824999999999|0.08717553846153846|-5.172884615384649E-4 |0.0  |4.0    |\n",
      "|1981-03-12 00:00:00|0.07873 |0.084853875        |0.08660353846153845|-0.0017496634615384588|0.0  |3.0    |\n",
      "|1981-03-13 00:00:00|0.077856|0.08310424999999999|0.08619976923076922|-0.003095519230769228 |0.0  |2.0    |\n",
      "|1981-03-16 00:00:00|0.080918|0.081846875        |0.08562784615384615|-0.003780971153846152 |0.0  |2.0    |\n",
      "|1981-03-17 00:00:00|0.084854|0.08113612499999999|0.08525776923076922|-0.004121644230769231 |0.0  |2.0    |\n",
      "|1981-03-18 00:00:00|0.090103|0.081190875        |0.08505592307692308|-0.003865048076923089 |0.0  |2.0    |\n",
      "|1981-03-19 00:00:00|0.089228|0.082011           |0.08475307692307693|-0.0027420769230769304|0.0  |3.0    |\n",
      "|1981-03-20 00:00:00|0.090103|0.083432625        |0.08461846153846155|-0.0011858365384615488|0.0  |3.0    |\n",
      "|1981-03-23 00:00:00|0.093602|0.08567425         |0.08482038461538462|8.538653846153765E-4  |1.0  |5.0    |\n",
      "|1981-03-24 00:00:00|0.093165|0.087478625        |0.08502230769230769|0.0024563173076923156 |1.0  |5.0    |\n",
      "|1981-03-25 00:00:00|0.091415|0.08917350000000002|0.08515692307692307|0.004016576923076942  |1.0  |6.0    |\n",
      "|1981-03-26 00:00:00|0.089665|0.09026687500000001|0.08569523076923076|0.004571644230769251  |1.0  |6.0    |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|1981-03-27 00:00:00|0.086604|0.090485625        |0.08630092307692308|0.0041847019230769195 |1.0  |6.0    |\n",
      "|1981-03-30 00:00:00|0.086604|0.09004825000000001|0.08714207692307692|0.002906173076923091  |1.0  |5.0    |\n",
      "|1981-03-31 00:00:00|0.085729|0.089610875        |0.08768046153846153|0.00193041346153848   |1.0  |5.0    |\n",
      "|1981-04-01 00:00:00|0.084854|0.08895475         |0.08821876923076924|7.359807692307596E-4  |1.0  |4.0    |\n",
      "|1981-04-02 00:00:00|0.09229 |0.08879075         |0.08909353846153847|-3.027884615384724E-4 |0.0  |4.0    |\n",
      "|1981-04-03 00:00:00|0.092727|0.088736           |0.08969915384615385|-9.631538461538497E-4 |0.0  |3.0    |\n",
      "|1981-04-06 00:00:00|0.090977|0.08868125         |0.08976638461538464|-0.0010851346153846336|0.0  |3.0    |\n",
      "|1981-04-07 00:00:00|0.090103|0.088736           |0.08983369230769232|-0.0010976923076923273|0.0  |3.0    |\n",
      "|1981-04-08 00:00:00|0.094477|0.08972012500000001|0.09017015384615383|-4.500288461538188E-4 |0.0  |4.0    |\n",
      "|1981-04-09 00:00:00|0.096226|0.09092287500000001|0.090372           |5.508750000000201E-4  |1.0  |4.0    |\n",
      "|1981-04-10 00:00:00|0.097539|0.092399125        |0.09070846153846154|0.0016906634615384553 |1.0  |5.0    |\n",
      "|1981-04-13 00:00:00|0.097539|0.09398475         |0.09117953846153846|0.002805211538461541  |1.0  |5.0    |\n",
      "|1981-04-14 00:00:00|0.097539|0.09464087500000003|0.09178523076923077|0.0028556442307692553 |1.0  |5.0    |\n",
      "|1981-04-15 00:00:00|0.092727|0.09464087500000001|0.09225623076923078|0.0023846442307692284 |1.0  |5.0    |\n",
      "|1981-04-16 00:00:00|0.087478|0.0942035          |0.09232346153846155|0.001880038461538447  |1.0  |5.0    |\n",
      "|1981-04-20 00:00:00|0.090103|0.0942035          |0.09265992307692308|0.0015435769230769114 |1.0  |5.0    |\n",
      "|1981-04-21 00:00:00|0.096226|0.09442212500000001|0.0935346923076923 |8.874326923077047E-4  |1.0  |5.0    |\n",
      "|1981-04-22 00:00:00|0.099725|0.0948595          |0.09410661538461541|7.528846153845858E-4  |1.0  |4.0    |\n",
      "|1981-04-23 00:00:00|0.10235 |0.095460875        |0.09484684615384616|6.14028846153844E-4   |1.0  |4.0    |\n",
      "|1981-04-24 00:00:00|0.101475|0.09595287499999999|0.09565438461538461|2.984903846153797E-4  |1.0  |4.0    |\n",
      "|1981-04-27 00:00:00|0.1006  |0.0963355          |0.09646184615384615|-1.2634615384614356E-4|0.0  |4.0    |\n",
      "|1981-04-28 00:00:00|0.098851|0.097101           |0.0967983076923077 |3.0269230769230937E-4 |1.0  |4.0    |\n",
      "|1981-04-29 00:00:00|0.097539|0.098358625        |0.0968993076923077 |0.0014593173076923038 |1.0  |5.0    |\n",
      "|1981-04-30 00:00:00|0.099288|0.09950675         |0.09703384615384616|0.00247290384615384   |1.0  |5.0    |\n",
      "|1981-05-01 00:00:00|0.099288|0.09988950000000002|0.09716838461538463|0.0027211153846153913 |1.0  |5.0    |\n",
      "|1981-05-04 00:00:00|0.098851|0.09978025000000001|0.0972693076923077 |0.002510942307692318  |1.0  |5.0    |\n",
      "|1981-05-05 00:00:00|0.098413|0.099288125        |0.09770669230769233|0.0015814326923076771 |1.0  |5.0    |\n",
      "|1981-05-06 00:00:00|0.095789|0.098577375        |0.09834600000000002|2.3137499999997813E-4 |1.0  |4.0    |\n",
      "+-------------------+--------+-------------------+-------------------+----------------------+-----+-------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.functions.udf\n",
       "df: org.apache.spark.sql.DataFrame = [date: timestamp, price: double]\n",
       "set_label: (v: Double)Double\n",
       "label_udf: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$3887/0x000000080175b040@a296009,DoubleType,List(Some(class[value[0]: double])),Some(class[value[0]: double]),None,false,true)\n",
       "df_new: org.apache.spark.sql.DataFrame = [date: timestamp, price: double ... 4 more fields]\n",
       "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
       "discretizer: org.apache.spark.ml.feature.QuantileDiscretizer = quantileDiscretizer_6e9a68e5f983\n",
       "result: org.apache.spark.sql.DataFrame = [date: timestamp, price: double ... 5 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "val df = spark.read.option(\"header\",\"true\").option(\"inferschema\",\"true\").csv(\"AAPL.csv\")\n",
    "def set_label(v: Double):Double = if (v>0) 1 else 0\n",
    "val label_udf = udf(set_label _)\n",
    "val df_new = df.withColumn(\"ma8\", avg(df(\"price\")).over( Window.orderBy(\"date\").rowsBetween(-7,0)))\n",
    "    .withColumn(\"ma13\", avg(df(\"price\")).over( Window.orderBy(\"date\").rowsBetween(-12,0)))\n",
    "    .withColumn(\"diff\",$\"ma8\"-$\"ma13\")\n",
    "    .withColumn(\"label\",label_udf($\"diff\"))\n",
    "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
    "\n",
    "\n",
    "val discretizer = new QuantileDiscretizer()\n",
    "  .setInputCol(\"diff\")\n",
    "  .setOutputCol(\"deciles\")\n",
    "  .setNumBuckets(10)\n",
    "\n",
    "val result = discretizer.fit(df_new).transform(df_new)\n",
    "result.show(100,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb4f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
