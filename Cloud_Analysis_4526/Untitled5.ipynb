{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02255d56",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "36: error: not enough arguments for method collect: (pf: PartialFunction[String,B])(implicit bf: scala.collection.generic.CanBuildFrom[Array[String],B,That])That.",
     "output_type": "error",
     "traceback": [
      "<console>:36: error: not enough arguments for method collect: (pf: PartialFunction[String,B])(implicit bf: scala.collection.generic.CanBuildFrom[Array[String],B,That])That.",
      "Unspecified value parameter pf.",
      "       s_p.map(rdd=>getWords(rdd.toLowerCase()).collect())",
      "                                                       ^",
      ""
     ]
    }
   ],
   "source": [
    "def getWords(text: String): Array[String] = {\n",
    "    //FILL THIS IN\n",
    "    text.split(\" \").map(a=>a.toArray.filter(b=>b.isLetter)).map(c=>c.mkString)\n",
    "}\n",
    "\n",
    "val s = Array(\"Jack, the man\\nWoolly the mammoth!\",\"Jack, the man\\nWoolly the mammoth!\")\n",
    "val s_p = sc.parallelize(s)\n",
    "s_p.map(rdd=>getWords(rdd.toLowerCase())).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cf9b5cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "25: error: value contains is not a member of Char",
     "output_type": "error",
     "traceback": [
      "<console>:25: error: value contains is not a member of Char",
      "       val contains_example = \"The world cup fifa is in Qatar this year\".exists(c=>c.contains(filters))",
      "                                                                                     ^",
      ""
     ]
    }
   ],
   "source": [
    "val filters = Array(\"fifa\", \"qatar\", \"soccer\")\n",
    "val contains_example = \"The world cup fifa is in Qatar this year\".exists(c=>c.contains(filters)\n",
    "                                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2eb0994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contains_example: Boolean = true\n",
       "exists_example_true: Boolean = true\n",
       "exists_example_false: Boolean = false\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val contains_example = \"world\\ncup\".contains(\"world\")\n",
    "val exists_example_true = Array(1,3,5,7,8,13).exists(v => v%2==0)\n",
    "val exists_example_false = Array(1,3,5,7,9,13).exists(v => v%2==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44f2ad13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Boolean = true\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The world cup is in Qatar this year\".exists(v => v=='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3aa107f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filters: Array[String] = Array(cup, football)\n",
       "a: Array[String] =\n",
       "Array(I watch world Cup, I watch world, world\n",
       "cup)\n",
       "a_p: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[14] at parallelize at <console>:30\n",
       "res24: Array[String] =\n",
       "Array(I watch world Cup, world\n",
       "cup)\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filters = Array(\"cup\",\"football\")\n",
    "// val keys = \"cup football\"\n",
    "val a = Array(\"I watch world Cup\",\"I watch world\",\"world\\ncup\")\n",
    "val a_p = sc.parallelize(a)\n",
    "a_p.filter(b=>filters.exists(filter => b.toLowerCase().contains(filter))).collect()\n",
    "// filters.exists(filter => a.contains(filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b34b674a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: Boolean = true\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"world\\ncup\".toLowerCase().contains(\"cup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c29a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@3a847789\n",
       "lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@7637d6fe\n",
       "words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@24307ba3\n",
       "wcount: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.WindowedDStream@91edb87\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "val words = lines.flatMap(l => l.split(\" \"))\n",
    "\n",
    "//Total words in the microbatch (10 seconds)\n",
    "//This will print every 10 seconds\n",
    "\n",
    "\n",
    "//Total number of words in every 30 second window sliding every 20 seconds\n",
    "//The print is at each point that the window slides (20, 40, 60 ,80,....)\n",
    "//This will print at 20 (total words in 20 seconds since this is the first slide)\n",
    "//at 40 seconds (total words in the last 3 10 second intervals)\n",
    "//at 60 seconds, etc.\n",
    "val wcount = words.window(Seconds(30),Seconds(20))\n",
    "wcount.foreachRDD((rdd,time) => print(time.toString.takeRight(10),\": Total  \",rdd.collect()(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0358884f",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "155: error: value flatten is not a member of org.apache.spark.streaming.dstream.DStream[(Int, Any)]",
     "output_type": "error",
     "traceback": [
      "<console>:155: error: value flatten is not a member of org.apache.spark.streaming.dstream.DStream[(Int, Any)]",
      "       val b = sentiment.flatten",
      "                         ^",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import org.apache.spark.rdd.PairRDDFunctions._\n",
    "import scala.io.Source\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "val filters = Source.fromFile(\"fifa2022_words.txt\").getLines.toArray\n",
    "\n",
    "val sentimentFilePath = \"AFINN-111.txt\"\n",
    "\n",
    "val wordSentiments = sc.textFile(sentimentFilePath).map { line => \n",
    "    (line.split(\"\\t\")(0),line.split(\"\\t\")(1).toInt)\n",
    "}.cache()\n",
    "\n",
    "def getWords(text: String): Array[String] = {\n",
    "    text.split(\" \").map(a=>a.toArray.filter(b=>b.isLetter)).map(c=>c.mkString)\n",
    "}\n",
    "\n",
    "val ssc = new StreamingContext(sc,Seconds(10.toLong))\n",
    "val text = ssc.socketTextStream(\"localhost\", 4444)\n",
    "\n",
    "//filter tweets only keeping world cup related ones\n",
    "//Use transform to work on each rdd (text is a DStream object, not an RDD)\n",
    "//filter using exists and contains\n",
    "//Also, convert the text to lowercase (all the keywords are in lowercase)\n",
    "val filteredText = text.transform(\n",
    "                    rdd => rdd.filter( //rdd\n",
    "                        a => filters.exists( //element\n",
    "                            key => a.toLowerCase().contains(key))))\n",
    "\n",
    "//convert all tweets in filteredText into a single array of words (think flatMap)\n",
    "val words = filteredText.flatMap(getWords(_))\n",
    "\n",
    "//get sentiments\n",
    "//we need to convert each word into a pair (word, 1) to count the number of words\n",
    "//apply transform to join each rdd with the wordSentiment rdd using fullOuterJoin\n",
    "//Use match to convert Option to a new paired rdd (count, 1*sentiment)\n",
    "val sentiment = words.map(word => (word, 1))\n",
    "                  .transform{\n",
    "                      rdd => rdd.fullOuterJoin(wordSentiments)\n",
    "                          .flatMap(pair => pair match {\n",
    "                                    case (word,(Some(count),None)) => Some(count,None)\n",
    "                                    case (word,(None,senti)) => None\n",
    "                                    case (word,(Some(count),Some(senti))) => Some(count,1*senti)\n",
    "                                    // case (_) => None\n",
    "                                    }\n",
    "                              )\n",
    "                            }\n",
    "// val b = sentiment.flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c84e721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1670109820000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1670109830000 ms\n",
      "-------------------------------------------\n",
      "(1,-2)\n",
      "(1,None)\n",
      "(1,None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.setLogLevel(\"ERROR\")\n",
    "// words.print()\n",
    "sentiment.print()\n",
    "ssc.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "393daf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 18:23:52 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82909f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t: String = 1670107490000 ms\n",
       "res19: String = 7490000\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t = \"1670107490000 ms\"\n",
    "t.dropRight(3).takeRight(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cddd7f9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "85: error: not found: value average",
     "output_type": "error",
     "traceback": [
      "<console>:85: error: not found: value average",
      "       average(1.2,1.2)",
      "       ^",
      ""
     ]
    }
   ],
   "source": [
    "average(1.2,1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b274635c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: org.apache.spark.rdd.RDD[Some[(Int, Any)]] = ParallelCollectionRDD[173] at parallelize at <console>:96\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = sc.parallelize(Array(Some(1,-2),Some(1,None),Some(1,None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b379672",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "98: error: Cannot prove that (Int, Any) <:< Option[B].",
     "output_type": "error",
     "traceback": [
      "<console>:98: error: Cannot prove that (Int, Any) <:< Option[B].",
      "       a.map(_.flatten)",
      "               ^",
      ""
     ]
    }
   ],
   "source": [
    "a.flatMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e932740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b: Array[(Int, Any)] = Array((1,-2), (1,None), (1,None))\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val b = a.flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb335b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res32: Any = None\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b(1)._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c832be6",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "98: error: value _2 is not a member of Some[(Int, Any)]",
     "output_type": "error",
     "traceback": [
      "<console>:98: error: value _2 is not a member of Some[(Int, Any)]",
      "       a(1)._2",
      "            ^",
      ""
     ]
    }
   ],
   "source": [
    "a(1)._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a6d6106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p: Array[(Double, Double)] = Array((0.0,-2.0), (0.0,0.0), (0.0,0.0))\n",
       "a: org.apache.spark.rdd.RDD[(Double, Double)] = ParallelCollectionRDD[5] at parallelize at <console>:27\n",
       "res11: Double = -2.0\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val p = Array((0.0,-2.0),(0.0,0.0),(0.0,0.0))\n",
    "val a = sc.parallelize(p)\n",
    "a.map(t=>t._2).reduce((a,b) => a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61e67d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
