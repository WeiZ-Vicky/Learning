{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b44948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0582480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.56.160.213:4045\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1671144469089)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "data: Seq[(String, Int)] = List((Megan,200000), (Gao,450000), (Antonio,120000))\n",
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:25\n",
       "df_from_data: org.apache.spark.sql.DataFrame = [Name: string, Income: int]\n",
       "df_from_rdd: org.apache.spark.sql.DataFrame = [Name: string, Income: int]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq((\"Megan\",200000),(\"Gao\",450000),(\"Antonio\",120000))//ordered,immutable collection\n",
    "val rdd = sc.parallelize(data)\n",
    "val df_from_data = data.toDF(\"Name\",\"Income\")\n",
    "val df_from_rdd = rdd.toDF(\"Name\",\"Income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c20f6d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: Array[(String, Int)] = Array((Megan,200000), (Gao,450000), (Antonio,120000))\n",
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[1] at parallelize at <console>:27\n",
       "df_from_rdd: org.apache.spark.sql.DataFrame = [Name: string, Income: int]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Array((\"Megan\",200000),(\"Gao\",450000),(\"Antonio\",120000))//ordered,immutable collection\n",
    "val rdd = sc.parallelize(data)\n",
    "// val df_from_data = data.toDF(\"Name\",\"Income\")\n",
    "val df_from_rdd = rdd.toDF(\"Name\",\"Income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d13f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [name: string, salary: int]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read //Jupyter automatically creates a sparksession with the identifer \"spark\"\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferschema\",\"true\")//\"Michael\" is a string, 3000 is an interger\n",
    "   .csv(\"employees.csv\")\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f1c1bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- positions: struct (nullable = true)\n",
      " |    |-- NYC: long (nullable = true)\n",
      " |    |-- Palo Alto: long (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [name: string, positions: struct<NYC: bigint, Palo Alto: bigint> ... 1 more field]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read //Jupyter automatically creates a sparksession with the identifer \"spark\"\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferschema\",\"true\")//\"Michael\" is a string, 3000 is an interger\n",
    "   .json(\"employees_singleline.json\")\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "069ad833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true),StructField(positions,StructType(StructField(NYC,LongType,true),StructField(Palo Alto,LongType,true)),true),StructField(salary,LongType,true))\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2584f56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "data: Array[Array[Any]] = Array(Array(Megan, 200000), Array(Gao, 450000), Array(Antonio, 120000))\n",
       "rdd: org.apache.spark.rdd.RDD[Array[Any]] = ParallelCollectionRDD[45] at parallelize at <console>:44\n",
       "rddRow: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[46] at map at <console>:45\n",
       "res11: Array[org.apache.spark.sql.Row] = Array([Megan,200000], [Gao,450000], [Antonio,120000])\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "val data = Array(Array(\"Megan\",200000),Array(\"Gao\",450000),Array(\"Antonio\",120000))\n",
    "val rdd = sc.parallelize(data)\n",
    "val rddRow = rdd.map(a=>Row(a(0),a(1)))\n",
    "rddRow.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82f61f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "data: Seq[(String, Int)] = List((Megan,200000), (Gao,450000), (Antonio,120000))\n",
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[56] at parallelize at <console>:63\n",
       "rddRow: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[57] at map at <console>:65\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "val data = Seq((\"Megan\",200000),(\"Gao\",450000),(\"Antonio\",120000))\n",
    "val rdd = sc.parallelize(data)\n",
    "// Row.fromSeq(Seq(value1, value2, ...))\n",
    "val rddRow = rdd.map(a=>Row(a._1,a._2))\n",
    "// rddRow.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa58c662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res25: Array[org.apache.spark.sql.Row] = Array([Megan,200000], [Gao,450000], [Antonio,120000])\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddRow.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fa60abe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "68: error: value getString is not a member of Array[org.apache.spark.sql.Row]",
     "output_type": "error",
     "traceback": [
      "<console>:68: error: value getString is not a member of Array[org.apache.spark.sql.Row]",
      "       rddRow.collect.getString(0)",
      "                      ^",
      ""
     ]
    }
   ],
   "source": [
    "rddRow.collect.getString(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b29b438e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res53: Array[org.apache.spark.sql.Row] = Array([Megan,200000], [Gao,450000], [Antonio,120000])\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddRow.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "878d5179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types._\n",
       "df: org.apache.spark.sql.DataFrame = [name: string, income: int]\n"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "val df = spark.createDataFrame(rddRow,\n",
    "                                StructType(\n",
    "                                     StructField(\"name\",StringType) ::\n",
    "                                     StructField(\"income\",IntegerType) ::\n",
    "                                     Nil)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "eac1c9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "struct: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true),StructField(income,IntegerType,true))\n"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val struct =\n",
    "   StructType(\n",
    "     StructField(\"name\",StringType) ::\n",
    "     StructField(\"income\",IntegerType) ::\n",
    "     Nil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "59f99cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- income: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4e7416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|income|\n",
      "+-------+------+\n",
      "|  Megan|200000|\n",
      "|    Gao|450000|\n",
      "|Antonio|120000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "35019abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res55: Array[org.apache.spark.sql.Row] = Array([Megan,200000], [Gao,450000], [Antonio,120000])\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aae07eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l: List[(String, Int)] = List((Alice,1))\n",
       "res20: Array[org.apache.spark.sql.Row] = Array([Alice,1])\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List((\"Alice\", 1))\n",
    "spark.createDataFrame(l).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b21aa09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: List[Int] = List(1)\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = List(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1916e8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Name|percentage|\n",
      "+----+----------+\n",
      "|John|     100.0|\n",
      "|Jill|      80.0|\n",
      "|John|      30.0|\n",
      "|Jill|      90.0|\n",
      "+----+----------+\n",
      "\n",
      "+----+----------+\n",
      "|Name|avg(Score)|\n",
      "+----+----------+\n",
      "|John|       6.5|\n",
      "|Jill|       8.5|\n",
      "+----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x: Array[(String, String, Int)] = Array((John,Q1,10), (Jill,Q1,8), (John,Q2,3), (Jill,Q2,9))\n",
       "rdd: org.apache.spark.rdd.RDD[(String, String, Int)] = ParallelCollectionRDD[112] at parallelize at <console>:75\n",
       "df: org.apache.spark.sql.DataFrame = [Name: string, Quiz: string ... 1 more field]\n",
       "percentages: org.apache.spark.sql.DataFrame = [Name: string, percentage: double]\n",
       "grouped: org.apache.spark.sql.RelationalGroupedDataset = RelationalGroupedDataset: [grouping expressions: [Name: string], value: [Name: string, Quiz: string ... 1 more field], type: ]\n"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = Array((\"John\",\"Q1\",10),\n",
    "              (\"Jill\",\"Q1\",8),\n",
    "              (\"John\",\"Q2\",3),\n",
    "              (\"Jill\",\"Q2\",9))\n",
    "val rdd = sc.parallelize(x)\n",
    "val df = rdd.toDF(\"Name\",\"Quiz\",\"Score\")\n",
    "// val names = df.select(\"Name\")\n",
    "// names.show\n",
    "// val scores = df.select(\"Name\",\"Score\")\n",
    "// scores.show\n",
    "// val percentages = df.select($\"Name\",$\"Score\"*100.0/10.0)//$ -> trade \"Score\" as column. when you need one, you need to do every one. \n",
    "// percentages.show\n",
    "val percentages = df.select($\"Name\",$\"Score\"*100.0/10.0 as \"percentage\")\n",
    "percentages.show\n",
    "val grouped = df.groupBy(\"Name\")\n",
    "grouped.mean(\"Score\").show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c5c2d9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Name|sum(Score)|\n",
      "+----+----------+\n",
      "|John|        13|\n",
      "|Jill|        17|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped.sum(\"Score\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8bd465d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|Name|Quiz|Score|\n",
      "+----+----+-----+\n",
      "|John|  Q1|   10|\n",
      "|Jill|  Q2|    9|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "a_grades: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Name: string, Quiz: string ... 1 more field]\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a_grades = df.filter($\"Score\">=9.0)\n",
    "a_grades.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0157a57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+\n",
      "|Name|percentage|grade|\n",
      "+----+----------+-----+\n",
      "|John|     100.0|    A|\n",
      "|Jill|      80.0|    B|\n",
      "|John|      30.0|    F|\n",
      "|Jill|      90.0|    A|\n",
      "+----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "grades: org.apache.spark.sql.DataFrame = [Name: string, percentage: double ... 1 more field]\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val grades = percentages.withColumn(\"grade\",\n",
    "                      when(col(\"percentage\")>=90.0,\"A\")\n",
    "                      .when(col(\"percentage\")>=80.0,\"B\")\n",
    "                        .otherwise( \"F\"))\n",
    "grades.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "26e9cb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Name|percentage|\n",
      "+----+----------+\n",
      "|John|     100.0|\n",
      "|Jill|      80.0|\n",
      "|John|      30.0|\n",
      "|Jill|      90.0|\n",
      "+----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.udf\n",
       "x: Array[(String, String, Int)] = Array((John,Q1,10), (Jill,Q1,8), (John,Q2,3), (Jill,Q2,9))\n",
       "rdd: org.apache.spark.rdd.RDD[(String, String, Int)] = ParallelCollectionRDD[149] at parallelize at <console>:87\n",
       "df: org.apache.spark.sql.DataFrame = [Name: string, Quiz: string ... 1 more field]\n",
       "percentage: org.apache.spark.sql.DataFrame = [Name: string, percentage: double]\n",
       "grader: (x: Double)String\n",
       "grader_udf: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$5062/0x0000000801bfe840@74ef3153,StringType,List(Some(class[value[0]: double])),Some(class[value[0]: string]),None,true,true)\n",
       "a: org.apache.spark.sql.DataFrame = [Name: string, percentage: double ... 1 more field]\n"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "val x = Array((\"John\",\"Q1\",10),\n",
    "              (\"Jill\",\"Q1\",8),\n",
    "              (\"John\",\"Q2\",3),\n",
    "              (\"Jill\",\"Q2\",9))\n",
    "val rdd = sc.parallelize(x)\n",
    "val df = rdd.toDF(\"Name\",\"Quiz\",\"Score\")\n",
    "val percentage = df.select($\"Name\",$\"Score\"*100.0/10.0 as \"percentage\")\n",
    "percentage.show\n",
    "def grader(x:Double):String = {\n",
    "    if (x >= 90.0) \"A\"\n",
    "    else if (x >= 80.0) \"B\"\n",
    "    else \"F\"\n",
    "}\n",
    "val grader_udf = udf(grader _)\n",
    "val a = percentage.withColumn(\"grade_\",grader_udf($\"percentage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a2310567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "|Name|percentage|grade_|\n",
      "+----+----------+------+\n",
      "|John|     100.0|     A|\n",
      "|Jill|      80.0|     B|\n",
      "|John|      30.0|     F|\n",
      "|Jill|      90.0|     A|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b1c80dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res72: org.apache.spark.sql.DataFrame = [Name: string, percentage: double]\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b7655920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|Name|Quiz|Score|\n",
      "+----+----+-----+\n",
      "|John|  Q1|100.0|\n",
      "|Jill|  Q1| 80.0|\n",
      "|John|  Q2| 30.0|\n",
      "|Jill|  Q2| 90.0|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x: Array[(String, String, Double)] = Array((John,Q1,100.0), (Jill,Q1,80.0), (John,Q2,30.0), (Jill,Q2,90.0))\n",
       "rdd: org.apache.spark.rdd.RDD[(String, String, Double)] = ParallelCollectionRDD[138] at parallelize at <console>:82\n",
       "df: org.apache.spark.sql.DataFrame = [Name: string, Quiz: string ... 1 more field]\n"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = Array((\"John\",\"Q1\",100.0),\n",
    "              (\"Jill\",\"Q1\",80.0),\n",
    "              (\"John\",\"Q2\",30.0),\n",
    "              (\"Jill\",\"Q2\",90.0))\n",
    "val rdd = sc.parallelize(x)\n",
    "val df = rdd.toDF(\"Name\",\"Quiz\",\"Score\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "30a2ee9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grader: (x: Double)String\n",
       "grader_udf: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$5059/0x0000000801a6b840@707ee2c8,StringType,List(Some(class[value[0]: double])),Some(class[value[0]: string]),None,true,true)\n",
       "a: org.apache.spark.sql.DataFrame = [Name: string, Quiz: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader(x:Double):String = {\n",
    "    if (x >= 90.0) \"A\"\n",
    "    else if (x >= 80.0) \"B\"\n",
    "    else \"F\"\n",
    "}\n",
    "val grader_udf = udf(grader _)\n",
    "val a = df.withColumn(\"grade\",grader_udf($\"Score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6ebd5f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+-----+\n",
      "|Name|Quiz|Score|grade|\n",
      "+----+----+-----+-----+\n",
      "|John|  Q1|100.0|    A|\n",
      "|Jill|  Q1| 80.0|    B|\n",
      "|John|  Q2| 30.0|    F|\n",
      "|Jill|  Q2| 90.0|    A|\n",
      "+----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "55a5d53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Quiz: string (nullable = true)\n",
      " |-- Score: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c61cd349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- percentage: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentages.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9b6b7f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|Name|Quiz|Score|\n",
      "+----+----+-----+\n",
      "|John|  Q1|   10|\n",
      "|Jill|  Q1|    8|\n",
      "|John|  Q2|    3|\n",
      "|Jill|  Q2|    9|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "95a3ebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|avg(Score)|avg(Score)|\n",
      "+----------+----------+\n",
      "|       7.5|       7.5|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(mean(\"Score\"),avg(\"Score\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "46e08609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|first(Score)|avg(Score)|\n",
      "+------------+----------+\n",
      "|          10|       7.5|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(first(\"Score\"),avg(\"Score\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a65eb055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|first(Score)|avg(Score)|\n",
      "+------------+----------+\n",
      "|          10|       7.5|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(first(\"Score\"),avg(\"Score\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "08101472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.MutableAggregationBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8af7c2ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "83: error: not found: value MutableAggregationBuffer",
     "output_type": "error",
     "traceback": [
      "<console>:83: error: not found: value MutableAggregationBuffer",
      "       MutableAggregationBuffer()",
      "       ^",
      ""
     ]
    }
   ],
   "source": [
    "MutableAggregationBuffer()\n",
    "// mutable_array_example += 5.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "bcea989c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: List[Any] = List(a, 1)\n"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = (\"a\"::1::Nil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c885e9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res111: Any = a\n"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1fe53ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res112: Any = 1\n"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4e3f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.56.162.227:4045\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1671218144539)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Created_Date: timestamp, Closed_Date: timestamp ... 9 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "   .csv(\"nyc_311_2022_clean.csv\")\n",
    "    .withColumnRenamed(\"Created Date\",\"Created_Date\")\n",
    "    .withColumnRenamed(\"Closed Date\",\"Closed_Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "882bc12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "res4: org.apache.spark.sql.types.StructType = StructType(StructField(inputColumn,DoubleType,true))\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "StructType(StructField(\"inputColumn\", DoubleType) ::\n",
    "                                             //StructField(\"inputColumn\", StringType)\n",
    "                                             Nil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "794e1175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.sql.Column = processing_days\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df(\"processing_days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "719bf062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.Column = 1.0\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82674dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/16 14:41:28 WARN SimpleFunctionRegistry: The function conditionalaverage replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{Row, SparkSession}\n",
       "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
       "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
       "import org.apache.spark.sql.types._\n",
       "defined object ConditionalAverage\n",
       "res12: org.apache.spark.sql.expressions.UserDefinedAggregateFunction = ConditionalAverage$@558994d8\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Necessary imports\n",
    "import org.apache.spark.sql.{Row, SparkSession}\n",
    "import org.apache.spark.sql.expressions.MutableAggregationBuffer\n",
    "import org.apache.spark.sql.expressions.UserDefinedAggregateFunction\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "//We'll define a scala program that calculates this average\n",
    "//Scala programs are contained in an object\n",
    "object ConditionalAverage extends UserDefinedAggregateFunction {\n",
    "    //Start with a schema for the input to the function\n",
    "    //In this case, a column and the type of the column\n",
    "    def inputSchema: StructType = StructType(StructField(\"inputColumn\", DoubleType) :: \n",
    "                                             StructField(\"threshold\",DoubleType) :: Nil)\n",
    "\n",
    "    // Set up an aggregation buffer. This specified the schema for the data that will be updated by each case\n",
    "    // Since we're calculating an average, we need to count elements and sum elements\n",
    "    def bufferSchema: StructType = {\n",
    "        StructType(StructField(\"sum\", DoubleType) :: StructField(\"count\", LongType) :: Nil)\n",
    "    }\n",
    "    \n",
    "    //Define the type of the value returned by the function\n",
    "    def dataType: DataType = DoubleType\n",
    "    \n",
    "    // Whether this function always returns the same output on the identical input\n",
    "    def deterministic: Boolean = true\n",
    "    \n",
    "    //Initialize the aggregation buffer\n",
    "    def initialize(buffer: MutableAggregationBuffer): Unit = {\n",
    "        buffer(0) = 0.0\n",
    "        buffer(1) = 0L\n",
    "    }\n",
    "    \n",
    "    // Update procedure. What to do with each case\n",
    "    def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n",
    "        if (!input.isNullAt(0)) {\n",
    "            if (input.getDouble(0) > input.getDouble(1)) {\n",
    "                buffer(0) = buffer.getDouble(0) + input.getDouble(0)\n",
    "                buffer(1) = buffer.getLong(1) + 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Merges two aggregation buffers and stores the updated buffer values back to `buffer1`\n",
    "    def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n",
    "        buffer1(0) = buffer1.getDouble(0) + buffer2.getDouble(0)\n",
    "        buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)\n",
    "    }\n",
    "  // Calculates the final result\n",
    "    def evaluate(buffer: Row): Double = buffer.getDouble(0) / buffer.getLong(1)\n",
    "}\n",
    "\n",
    "// Register the function to access it\n",
    "spark.udf.register(\"ConditionalAverage\", ConditionalAverage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08aba62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|conditionalaverage$(processing_days)|\n",
      "+------------------------------------+\n",
      "|                  19.811219247322303|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(ConditionalAverage(df(\"processing_days\"))).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64f978f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|conditionalaverage$(processing_days, 1.0)|\n",
      "+-----------------------------------------+\n",
      "|                       19.811219247322303|\n",
      "+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(ConditionalAverage($\"processing_days\",lit(1.0))).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2716fb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+\n",
      "|   processing_days|1.0|\n",
      "+------------------+---+\n",
      "| 5.882638888888889|1.0|\n",
      "| 4.070833333333334|1.0|\n",
      "|1.3104166666666668|1.0|\n",
      "| 4.272916666666666|1.0|\n",
      "|1.5930555555555554|1.0|\n",
      "| 2.272222222222222|1.0|\n",
      "| 5.195138888888889|1.0|\n",
      "|2.1868055555555554|1.0|\n",
      "|2.4805555555555556|1.0|\n",
      "|2.5652777777777778|1.0|\n",
      "| 4.317361111111111|1.0|\n",
      "|1.2472222222222222|1.0|\n",
      "| 3.395138888888889|1.0|\n",
      "| 5.979166666666667|1.0|\n",
      "|1.3180555555555555|1.0|\n",
      "| 3.870833333333333|1.0|\n",
      "|1.5958333333333332|1.0|\n",
      "| 9.120138888888889|1.0|\n",
      "|3.5493055555555557|1.0|\n",
      "|               2.0|1.0|\n",
      "+------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select($\"processing_days\",lit(1.0)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af52a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter($\"processing_days\">1.0).select(mean(\"processing_days\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "93d406ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Created_Date: timestamp (nullable = true)\n",
      " |-- Closed_Date: timestamp (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Agency Name: string (nullable = true)\n",
      " |-- Complaint Type: string (nullable = true)\n",
      " |-- Incident Zip: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- processing_time: string (nullable = true)\n",
      " |-- processing_days: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "45690e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res119: org.apache.spark.sql.DataFrame = [processing_days: double]\n"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"processing_days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b244345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: String = NYPD\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = \"NYPD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da89eee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: String = \"NYPD\"\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = \"\"\"\"NYPD\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fff283c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: String =\n",
       "\"aa\n",
       "aa\n",
       "\"\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = \"\"\"aa\n",
    "aa\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "355aa227",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: unclosed string literal",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: unclosed string literal",
      "       val x = \"a",
      "               ^",
      "<console>:3: error: unclosed string literal",
      "       a\"",
      "         ^",
      ""
     ]
    }
   ],
   "source": [
    "val x = \"a\n",
    "a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d743bc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.56.162.227:4045\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1671222736925)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vertexArray: Array[(Int, String, Int)] = Array((1,Alice,28), (2,Bob,27), (3,Charlie,65), (4,David,42), (5,Ed,55), (6,Fran,50), (7,Qing,27), (8,Sarika,78), (9,Olafson,17), (10,Birgit,33))\n",
       "edgeArray: Array[(Int, Int, Int)] = Array((2,1,7), (1,2,13), (2,4,2), (3,2,4), (3,6,3), (4,1,1), (5,2,2), (5,3,8), (5,6,3), (7,8,14), (7,9,2), (8,10,8), (9,10,6))\n",
       "vertex_df: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]\n",
       "edge_df: org.apache.spark.sql.DataFrame = [src: int, dst: int ... 1 more field]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vertexArray = Array(\n",
    "  (1, \"Alice\", 28),\n",
    "  (2, \"Bob\", 27),\n",
    "  (3, \"Charlie\", 65),\n",
    "  (4, \"David\", 42),\n",
    "  (5, \"Ed\", 55),\n",
    "  (6, \"Fran\", 50),\n",
    "    (7, \"Qing\",27),\n",
    "    (8, \"Sarika\",78),\n",
    "    (9, \"Olafson\",17),\n",
    "    (10, \"Birgit\",33)\n",
    ")\n",
    "\n",
    "val edgeArray = Array(\n",
    "  (2, 1, 7),\n",
    "  (1, 2, 13),\n",
    "  (2, 4, 2),\n",
    "  (3, 2, 4),\n",
    "  (3, 6, 3),\n",
    "  (4, 1, 1),\n",
    "  (5, 2, 2),\n",
    "  (5, 3, 8),\n",
    "  (5, 6, 3),\n",
    "    (7, 8, 14),\n",
    "    (7, 9, 2),\n",
    "    (8, 10, 8),\n",
    "    (9, 10, 6)\n",
    ")\n",
    "\n",
    "val vertex_df = spark.createDataFrame(vertexArray).toDF(\"id\",\"name\",\"age\")\n",
    "val edge_df = spark.createDataFrame(edgeArray).toDF(\"src\",\"dst\",\"attr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bcb3c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 28|\n",
      "|  2|    Bob| 27|\n",
      "|  3|Charlie| 65|\n",
      "|  4|  David| 42|\n",
      "|  5|     Ed| 55|\n",
      "|  6|   Fran| 50|\n",
      "|  7|   Qing| 27|\n",
      "|  8| Sarika| 78|\n",
      "|  9|Olafson| 17|\n",
      "| 10| Birgit| 33|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vertex_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769c9f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| _1|     _2| _3|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 28|\n",
      "|  2|    Bob| 27|\n",
      "|  3|Charlie| 65|\n",
      "|  4|  David| 42|\n",
      "|  5|     Ed| 55|\n",
      "|  6|   Fran| 50|\n",
      "|  7|   Qing| 27|\n",
      "|  8| Sarika| 78|\n",
      "|  9|Olafson| 17|\n",
      "| 10| Birgit| 33|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(vertexArray).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc664c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: Array[String] = Array(John,33,245233.234, Jill,23,132987.22, Qing,54,782344.22, Rahul,50,389223,54)\n",
       "d: Array[(String, String, String)] = Array((John,33,245233.234), (Jill,23,132987.22), (Qing,54,782344.22), (Rahul,50,389223))\n",
       "res15: Class[_ <: (String, String, String)] = class scala.Tuple3\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Array(\"John,33,245233.234\",\"Jill,23,132987.22\",\"Qing,54,782344.22\",\"Rahul,50,389223,54\")\n",
    "val d = data.map(a=>(a.split(\",\")(0),a.split(\",\")(1),a.split(\",\")(2)))\n",
    "\n",
    "// spark.createDataFrame(d).toDF(\"name\",\"age\",\"income\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c3d3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d: Array[Array[String]] = Array(Array(John, 33, 245233.234), Array(Jill, 23, 132987.22), Array(Qing, 54, 782344.22), Array(Rahul, 50, 389223, 54))\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = data.map(a=>a.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2f6596b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertexArray: Array[(Int, String, Int)] = Array((1,Alice,28), (2,Bob,27), (3,Charlie,65), (4,David,42), (5,Ed,55), (6,Fran,50), (7,Qing,27), (8,Sarika,78), (9,Olafson,17), (10,Birgit,33))\n",
       "rdd: org.apache.spark.rdd.RDD[(Int, String, Int)] = ParallelCollectionRDD[3] at parallelize at <console>:38\n",
       "df: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vertexArray = Array(\n",
    "  (1, \"Alice\", 28),\n",
    "  (2, \"Bob\", 27),\n",
    "  (3, \"Charlie\", 65),\n",
    "  (4, \"David\", 42),\n",
    "  (5, \"Ed\", 55),\n",
    "  (6, \"Fran\", 50),\n",
    "    (7, \"Qing\",27),\n",
    "    (8, \"Sarika\",78),\n",
    "    (9, \"Olafson\",17),\n",
    "    (10, \"Birgit\",33)\n",
    ")\n",
    "val rdd = sc.parallelize(vertexArray)\n",
    "val df = rdd.toDF(\"id\",\"name\",\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd9aab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 28|\n",
      "|  2|    Bob| 27|\n",
      "|  3|Charlie| 65|\n",
      "|  4|  David| 42|\n",
      "|  5|     Ed| 55|\n",
      "|  6|   Fran| 50|\n",
      "|  7|   Qing| 27|\n",
      "|  8| Sarika| 78|\n",
      "|  9|Olafson| 17|\n",
      "| 10| Birgit| 33|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f176439",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.packages= [\"graphframes:graphframes:0.8.2-spark3.2-s_2.12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56bdfaf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.graphframes._\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.graphframes._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d30309d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertexArray: Array[(Int, String, Int)] = Array((1,Alice,28), (2,Bob,27), (3,Charlie,65), (4,David,42), (5,Ed,55), (6,Fran,50), (7,Qing,27), (8,Sarika,78), (9,Olafson,17), (10,Birgit,33))\n",
       "edgeArray: Array[(Int, Int, Int)] = Array((2,1,7), (1,2,13), (2,4,2), (3,2,4), (3,6,3), (4,1,1), (5,2,2), (5,3,8), (5,6,3), (7,8,14), (7,9,2), (8,10,8), (9,10,6))\n",
       "vertex_df: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]\n",
       "edge_df: org.apache.spark.sql.DataFrame = [src: int, dst: int ... 1 more field]\n",
       "g: org.graphframes.GraphFrame = GraphFrame(v:[id: int, name: string ... 1 more field], e:[src: int, dst: int ... 1 more field])\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vertexArray = Array(\n",
    "  (1, \"Alice\", 28),\n",
    "  (2, \"Bob\", 27),\n",
    "  (3, \"Charlie\", 65),\n",
    "  (4, \"David\", 42),\n",
    "  (5, \"Ed\", 55),\n",
    "  (6, \"Fran\", 50),\n",
    "    (7, \"Qing\",27),\n",
    "    (8, \"Sarika\",78),\n",
    "    (9, \"Olafson\",17),\n",
    "    (10, \"Birgit\",33)\n",
    ")\n",
    "\n",
    "val edgeArray = Array(\n",
    "  (2, 1, 7),\n",
    "//     (2, 2, 1),\n",
    "  (1, 2, 13),\n",
    "  (2, 4, 2),\n",
    "  (3, 2, 4),\n",
    "  (3, 6, 3),\n",
    "  (4, 1, 1),\n",
    "  (5, 2, 2),\n",
    "  (5, 3, 8),\n",
    "  (5, 6, 3),\n",
    "    (7, 8, 14),\n",
    "    (7, 9, 2),\n",
    "    (8, 10, 8),\n",
    "    (9, 10, 6)\n",
    ")\n",
    "\n",
    "val vertex_df = spark.createDataFrame(vertexArray).toDF(\"id\",\"name\",\"age\")\n",
    "val edge_df = spark.createDataFrame(edgeArray).toDF(\"src\",\"dst\",\"attr\")\n",
    "\n",
    "val g = GraphFrame(vertex_df, edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be37bd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------------+----------+\n",
      "|             a|         e|             b|        e2|\n",
      "+--------------+----------+--------------+----------+\n",
      "|{1, Alice, 28}|{1, 2, 13}|  {2, Bob, 27}| {2, 1, 7}|\n",
      "|  {2, Bob, 27}| {2, 1, 7}|{1, Alice, 28}|{1, 2, 13}|\n",
      "+--------------+----------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.find(\"(a)-[e]->(b); (b)-[e2]->(a)\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5181e185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n",
      "|               a|               b|\n",
      "+----------------+----------------+\n",
      "|  {1, Alice, 28}|    {2, Bob, 27}|\n",
      "|    {2, Bob, 27}|  {4, David, 42}|\n",
      "|    {2, Bob, 27}|  {1, Alice, 28}|\n",
      "|{3, Charlie, 65}|   {6, Fran, 50}|\n",
      "|{3, Charlie, 65}|    {2, Bob, 27}|\n",
      "|  {4, David, 42}|  {1, Alice, 28}|\n",
      "|     {5, Ed, 55}|   {6, Fran, 50}|\n",
      "|     {5, Ed, 55}|{3, Charlie, 65}|\n",
      "|     {5, Ed, 55}|    {2, Bob, 27}|\n",
      "|   {7, Qing, 27}|{9, Olafson, 17}|\n",
      "|   {7, Qing, 27}| {8, Sarika, 78}|\n",
      "| {8, Sarika, 78}|{10, Birgit, 33}|\n",
      "|{9, Olafson, 17}|{10, Birgit, 33}|\n",
      "+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.find(\"(a)-[]->(b)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb640e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|src|dst|swap|\n",
      "+---+---+----+\n",
      "|  2|  1|null|\n",
      "|  1|  1|   2|\n",
      "|  2|  2|   4|\n",
      "|  3|  2|null|\n",
      "|  3|  3|   6|\n",
      "|  4|  1|null|\n",
      "|  5|  2|null|\n",
      "|  5|  3|null|\n",
      "|  5|  5|   6|\n",
      "|  7|  7|   8|\n",
      "|  7|  7|   9|\n",
      "|  8|  8|  10|\n",
      "|  9|  9|  10|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.find(\"(a)-[]->(b)\") //Get all nodes in a df\n",
    "    .select($\"a.id\" as \"src\" ,$\"b.id\" as \"dst\" ) \n",
    ".withColumn(\"swap\",when(col(\"src\")<col(\"dst\"),col(\"dst\")))\n",
    ".withColumn(\"dst\",\n",
    "                when(col(\"swap\").isNotNull,col(\"src\"))\n",
    "                .otherwise(col(\"dst\")))\n",
    ".show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c9fdb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name| name|\n",
      "+----+-----+\n",
      "|  Ed|David|\n",
      "|  Ed|Alice|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.find(\"(a)-[ab]->(b);(b)-[bc]->(c);(c)-[cd]->(d)\")\n",
    "    .filter(\"a.id != b.id\")\n",
    "    .filter(\"a.id != c.id\")\n",
    "    .filter(\"a.id != d.id\")\n",
    "    .filter(\"b.id != c.id\")\n",
    "    .filter(\"b.id != d.id\")\n",
    "    .filter(\"c.id != d.id\")\n",
    "    .select(($\"ab.attr\"+$\"bc.attr\"+$\"cd.attr\").as(\"s\"),$\"a.name\",$\"d.name\")\n",
    "    .filter($\"s\">8)\n",
    "    .drop($\"s\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63cf68f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "condition: org.apache.spark.sql.Column = (((0 + ab[attr]) + bc[attr]) + cd[attr])\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// def sumLikes(total:Column, likes:Column):Column = {total+likes}\n",
    "val condition = Seq(\"ab\",\"bc\",\"cd\")\n",
    "    .foldLeft(lit(0))((total,a) => total+col(a)(\"attr\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6430d6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name| name|\n",
      "+----+-----+\n",
      "|  Ed|David|\n",
      "|  Ed|Alice|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.find(\"(a)-[ab]->(b);(b)-[bc]->(c);(c)-[cd]->(d)\")\n",
    "    .filter(\"a.id != b.id\")\n",
    "    .filter(\"a.id != c.id\")\n",
    "    .filter(\"a.id != d.id\")\n",
    "    .filter(\"b.id != c.id\")\n",
    "    .filter(\"b.id != d.id\")\n",
    "    .filter(\"c.id != d.id\")\n",
    "    .where(condition>8)\n",
    "    .select(\"a.name\",\"d.name\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1dea6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res30: scala.collection.immutable.IndexedSeq[String] = Vector((n1)-[e1]->(n2), (n2)-[e2]->(n3), (n3)-[e3]->(n4))\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 to 3).map(i => s\"(n$i)-[e$i]->(n${i + 1})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1aa94f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  3|Charlie| 65|\n",
      "|  4|  David| 42|\n",
      "|  5|     Ed| 55|\n",
      "|  6|   Fran| 50|\n",
      "|  8| Sarika| 78|\n",
      "| 10| Birgit| 33|\n",
      "+---+-------+---+\n",
      "\n",
      "+---+---+----+\n",
      "|src|dst|attr|\n",
      "+---+---+----+\n",
      "|  3|  6|   3|\n",
      "|  5|  3|   8|\n",
      "|  5|  6|   3|\n",
      "|  8| 10|   8|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.filterVertices(\"age > 30\").vertices.show\n",
    "g.filterVertices(\"age > 30\").edges.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a74ce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 28|\n",
      "|  2|    Bob| 27|\n",
      "|  3|Charlie| 65|\n",
      "|  4|  David| 42|\n",
      "|  5|     Ed| 55|\n",
      "|  6|   Fran| 50|\n",
      "|  7|   Qing| 27|\n",
      "|  8| Sarika| 78|\n",
      "|  9|Olafson| 17|\n",
      "| 10| Birgit| 33|\n",
      "+---+-------+---+\n",
      "\n",
      "+---+---+----+\n",
      "|src|dst|attr|\n",
      "+---+---+----+\n",
      "|  2|  1|   7|\n",
      "|  1|  2|  13|\n",
      "|  3|  2|   4|\n",
      "|  3|  6|   3|\n",
      "|  5|  3|   8|\n",
      "|  5|  6|   3|\n",
      "|  7|  8|  14|\n",
      "|  8| 10|   8|\n",
      "|  9| 10|   6|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.filterEdges(\"attr>2\").vertices.show\n",
    "g.filterEdges(\"attr>2\").edges.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b82deca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  3|Charlie| 65|\n",
      "|  5|     Ed| 55|\n",
      "|  5|     Ed| 55|\n",
      "|  5|     Ed| 55|\n",
      "|  2|    Bob| 27|\n",
      "|  2|    Bob| 27|\n",
      "|  3|Charlie| 65|\n",
      "|  3|Charlie| 65|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [a: struct<id: int, name: string ... 1 more field>, ab: struct<src: int, dst: int ... 1 more field> ... 5 more fields]\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = g.find(\"(a)-[ab]->(b);(b)-[bc]->(c);(c)-[cd]->(d)\")\n",
    "    .filter(\"a.id != b.id\")\n",
    "    .filter(\"a.id != c.id\")\n",
    "    .filter(\"a.id != d.id\")\n",
    "    .filter(\"b.id != c.id\")\n",
    "    .filter(\"b.id != d.id\")\n",
    "    .filter(\"c.id != d.id\")\n",
    "df.select(\"a.*\").union(df.select(\"b.*\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1cc7fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.graphframes._\n",
       "import org.graphframes.lib.AggregateMessages\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.graphframes._\n",
    "import org.graphframes.lib.AggregateMessages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d5b971b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertexArray: Array[(Int, String, Int)] = Array((1,Alice,28), (2,Bob,27), (3,Charlie,65), (4,David,42), (5,Ed,55), (6,Fran,50), (7,Qing,27), (8,Sarika,78), (9,Olafson,17), (10,Birgit,33))\n",
       "edgeArray: Array[(Int, Int, Int)] = Array((2,1,7), (1,2,13), (2,4,2), (3,2,4), (3,6,3), (4,1,1), (5,2,2), (5,3,8), (5,6,3), (7,8,14), (7,9,2), (8,10,8), (9,10,6))\n",
       "vertex_df: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]\n",
       "edge_df: org.apache.spark.sql.DataFrame = [src: int, dst: int ... 1 more field]\n",
       "g: org.graphframes.GraphFrame = GraphFrame(v:[id: int, name: string ... 1 more field], e:[src: int, dst: int ... 1 more field])\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vertexArray = Array(\n",
    "  (1, \"Alice\", 28),\n",
    "  (2, \"Bob\", 27),\n",
    "  (3, \"Charlie\", 65),\n",
    "  (4, \"David\", 42),\n",
    "  (5, \"Ed\", 55),\n",
    "  (6, \"Fran\", 50),\n",
    "    (7, \"Qing\",27),\n",
    "    (8, \"Sarika\",78),\n",
    "    (9, \"Olafson\",17),\n",
    "    (10, \"Birgit\",33)\n",
    ")\n",
    "\n",
    "val edgeArray = Array(\n",
    "  (2, 1, 7),\n",
    "  (1, 2, 13),\n",
    "  (2, 4, 2),\n",
    "  (3, 2, 4),\n",
    "  (3, 6, 3),\n",
    "  (4, 1, 1),\n",
    "  (5, 2, 2),\n",
    "  (5, 3, 8),\n",
    "  (5, 6, 3),\n",
    "    (7, 8, 14),\n",
    "    (7, 9, 2),\n",
    "    (8, 10, 8),\n",
    "    (9, 10, 6)\n",
    ")\n",
    "\n",
    "val vertex_df = spark.createDataFrame(vertexArray).toDF(\"id\",\"name\",\"age\")\n",
    "val edge_df = spark.createDataFrame(edgeArray).toDF(\"src\",\"dst\",\"attr\")\n",
    "\n",
    "val g = GraphFrame(vertex_df, edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "148124fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|alllikes|\n",
      "+---+--------+\n",
      "|  1|       8|\n",
      "|  4|       2|\n",
      "|  2|      19|\n",
      "|  6|       6|\n",
      "|  3|       8|\n",
      "|  9|       2|\n",
      "|  8|      14|\n",
      "| 10|      14|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.aggregateMessages\n",
    "    .sendToDst(AggregateMessages.edge(\"attr\")) //Send the edge attr to value to the destination\n",
    "    .agg(sum(AggregateMessages.msg).as(\"alllikes\")).show //Aggregate messages by summing them up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6694d762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.graphframes.lib.AggregateMessages = org.graphframes.lib.AggregateMessages@567e5b96\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.aggregateMessages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba45165a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96d0d8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertexArray: Array[(Int, String, Int)] = Array((1,Alice,28), (2,Bob,27), (3,Charlie,65), (4,David,42), (5,Ed,55), (6,Fran,50), (7,Qing,27), (8,Sarika,78), (9,Olafson,17), (10,Birgit,33))\n",
       "edgeArray: Array[(Int, Int, Int)] = Array((2,1,7), (1,2,13), (2,4,2), (3,2,4), (3,6,3), (4,1,1), (5,2,2), (5,3,8), (5,6,3), (7,8,14), (7,9,2), (8,10,8), (9,10,6))\n",
       "vertexArrayX: Array[(Long, (String, Int))] = Array((1,(Alice,28)), (2,(Bob,27)), (3,(Charlie,65)), (4,(David,42)), (5,(Ed,55)), (6,(Fran,50)), (7,(Qing,27)), (8,(Sarika,78)), (9,(Olafson,17)), (10,(Birgit,33)))\n",
       "edgeArrayX: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(2,1,7), Edge(1,2,13), Edge(2,4,2), Edge(3,2,4), Edge(3,6,3), Edge(4,1,1), Edge(5,2,2), Edge(5,3,8), Edge(5,6,3), Edge(7,8,14), Edge(7,9,2), Edge(8,10,8), Edge(9,10,6))\n",
       "ve...\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vertexArray = Array(\n",
    "  (1, \"Alice\", 28),\n",
    "  (2, \"Bob\", 27),\n",
    "  (3, \"Charlie\", 65),\n",
    "  (4, \"David\", 42),\n",
    "  (5, \"Ed\", 55),\n",
    "  (6, \"Fran\", 50),\n",
    "    (7, \"Qing\",27),\n",
    "    (8, \"Sarika\",78),\n",
    "    (9, \"Olafson\",17),\n",
    "    (10, \"Birgit\",33)\n",
    ")\n",
    "\n",
    "val edgeArray = Array(\n",
    "  (2, 1, 7),\n",
    "  (1, 2, 13),\n",
    "  (2, 4, 2),\n",
    "  (3, 2, 4),\n",
    "  (3, 6, 3),\n",
    "  (4, 1, 1),\n",
    "  (5, 2, 2),\n",
    "  (5, 3, 8),\n",
    "  (5, 6, 3),\n",
    "    (7, 8, 14),\n",
    "    (7, 9, 2),\n",
    "    (8, 10, 8),\n",
    "    (9, 10, 6)\n",
    ")\n",
    "val vertexArrayX = vertexArray.map(r => (r._1.toLong,(r._2,r._3)))//(v.idL,v.attr)\n",
    "val edgeArrayX = edgeArray.map(r => Edge(r._1.toLong,r._2.toLong,r._3))//(srcL,detL,attr)\n",
    "// val vertexRDD = sc.parallelize(vertexArrayX)\n",
    "val vertexRDD: RDD[(Long, (String, Int))] = sc.parallelize(vertexArrayX)//Long is required\n",
    "val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArrayX)//attr type\n",
    "\n",
    "val graph: Graph[(String, Int), Int] = Graph(vertexRDD, edgeRDD)//vertex attr type, edge attr type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a289814c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.rdd.RDD[(Long, (String, Int))] = ParallelCollectionRDD[9] at parallelize at <console>:72\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertexRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0737b18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.graphframes.GraphFrame = GraphFrame(v:[id: int, name: string ... 1 more field], e:[src: int, dst: int ... 1 more field])\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edc0097a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[28] at rdd at <console>:41\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.vertices.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ab0ef01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Array[(org.apache.spark.graphx.VertexId, (String, Int))] = Array((8,(Sarika,78)), (1,(Alice,28)), (9,(Olafson,17)), (10,(Birgit,33)), (2,(Bob,27)), (3,(Charlie,65)), (4,(David,42)), (5,(Ed,55)), (6,(Fran,50)), (7,(Qing,27)))\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12e8ff1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_incoming_likes: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[36] at RDD at VertexRDD.scala:57\n",
       "res12: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((8,14), (1,8), (9,2), (10,14), (2,19), (3,8), (4,2), (6,6))\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val total_incoming_likes = graph.aggregateMessages[Int](ec => ec.sendToDst(ec.attr),(x,y) => x+y)\n",
    "//work on triplets,x ->first message\n",
    "total_incoming_likes.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fba1657a",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "40: error: missing parameter type",
     "output_type": "error",
     "traceback": [
      "<console>:40: error: missing parameter type",
      "Error occurred in an application involving default arguments.",
      "       val total_incoming_likes = graph.aggregateMessages(ec => ec.sendToDst(ec.attr),(x,y) => x+y)",
      "                                                          ^",
      "<console>:40: error: missing parameter type",
      "Error occurred in an application involving default arguments.",
      "       val total_incoming_likes = graph.aggregateMessages(ec => ec.sendToDst(ec.attr),(x,y) => x+y)",
      "                                                                                       ^",
      "<console>:40: error: missing parameter type",
      "Error occurred in an application involving default arguments.",
      "       val total_incoming_likes = graph.aggregateMessages(ec => ec.sendToDst(ec.attr),(x,y) => x+y)",
      "                                                                                         ^",
      ""
     ]
    }
   ],
   "source": [
    "val total_incoming_likes = graph.aggregateMessages(ec => ec.sendToDst(ec.attr),(x,y) => x+y)\n",
    "//work on triplets,x ->first message\n",
    "total_incoming_likes.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9195e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "651e8b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@2a6e0244\n",
       "lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@20f7adff\n",
       "words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@35c15dd7\n",
       "pairs: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.MappedDStream@1eb5d7ef\n",
       "wordCounts: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.ShuffledDStream@44938a5f\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc,Seconds(10.toLong))\n",
    "\n",
    "//NOTE: lines, words, pairs, wordCounts are all DStream objects, not RDDs!\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "val words = lines.flatMap(line => line.split(\" \"))\n",
    "val pairs = words.map(word => (word, 1))\n",
    "val wordCounts = pairs.reduceByKey((x, y) => x + y)\n",
    "wordCounts.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b452663d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1671408530000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1671408540000 ms\n",
      "-------------------------------------------\n",
      "(aa,2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e97c069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/18 19:09:02 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ac4834d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateFunction: (Seq[Int], Option[Int]) => Option[Int] = $Lambda$5834/0x0000000801ac8840@4c849d1\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val updateFunction = (nv: Seq[Int], rc: Option[Int]) => { //nv = new value; rc = running count\n",
    "    val uc = rc.getOrElse(0) + nv.sum  //If rc does not exist, set it to 0, otherwise use the existing value\n",
    "    Option(uc) //return Some(uc)  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d146833f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.149:4042\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1671557841888)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: scala.collection.immutable.IndexedSeq[Int] = Vector(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0 to 9).map(i=>i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "19d573c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertexArray: Array[(Int, Int)] = Array((1,1), (2,2), (3,3), (4,4), (5,5), (6,6), (7,7), (8,8))\n",
       "edgeArray: Array[(Int, Int)] = Array((1,3), (2,3), (2,4), (4,5), (3,5), (5,6), (6,7), (6,8), (7,8))\n",
       "vertex_df: org.apache.spark.sql.DataFrame = [id: int, v_desc: int]\n",
       "edge_df: org.apache.spark.sql.DataFrame = [src: int, dst: int]\n",
       "g: org.graphframes.GraphFrame = GraphFrame(v:[id: int, v_desc: int], e:[src: int, dst: int])\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val vertexArray = Array(\n",
    "  (1,1),\n",
    "  (2,2),\n",
    "  (3,3),\n",
    "  (4,4),\n",
    "  (5,5),\n",
    "  (6,6),\n",
    "    (7,7),\n",
    "    (8,8)\n",
    ")\n",
    "\n",
    "\n",
    "val edgeArray = Array(\n",
    "  (1, 3),\n",
    "  (2, 3),\n",
    "  (2, 4),\n",
    "  (4, 5),\n",
    "  (3, 5),\n",
    "  (5, 6),\n",
    "  (6, 7),\n",
    "  (6, 8),\n",
    "  (7, 8)\n",
    ")\n",
    "\n",
    "val vertex_df = spark.createDataFrame(vertexArray).toDF(\"id\",\"v_desc\")\n",
    "val edge_df = spark.createDataFrame(edgeArray).toDF(\"src\",\"dst\")\n",
    "\n",
    "val g = GraphFrame(vertex_df, edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "12b65444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getShortestPath: (g: org.graphframes.GraphFrame, i: Int, j: Int)Array[Int]\n"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getShortestPath(g: GraphFrame,i: Int, j: Int) = {\n",
    "    val path_df = g.bfs.fromExpr(s\"id=$i\").toExpr(s\"id=$j\").run() \n",
    "    if (path_df.count > 0) {\n",
    "        val cols = path_df.columns.filter(n=>n.contains(\"v\")).map(n=>col(n+\".id\"))\n",
    "        val a = path_df.select(cols:_*).rdd.collect()(0).toSeq.toArray.map(e => e.toString.toInt)\n",
    "        a\n",
    "    }\n",
    "    else Array[Int]()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "14ff53fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_df: org.apache.spark.sql.DataFrame = [from: struct<id: int, v_desc: int>, e0: struct<src: int, dst: int> ... 7 more fields]\n",
       "cols: Array[org.apache.spark.sql.Column] = Array(v1.id, v2.id, v3.id)\n",
       "a: Array[Int] = Array(3, 5, 6)\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path_df = g.bfs.fromExpr(s\"id=1\").toExpr(s\"id=8\").run() \n",
    "val cols = path_df.columns.filter(n=>n.contains(\"v\")).map(n=>col(n+\".id\"))\n",
    "val a = path_df.select(cols:_*).rdd.collect()(0).toSeq.toArray.map(e => e.toString.toInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "56960347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res96: Array[Int] = Array()\n"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getShortestPath(g,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fce2fe41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: Array[Int] = Array(1, 2)\n",
       "res94: List[Array[Int]] = List(Array(1, 2), Array(2, 3))\n"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = Array(1,2)\n",
    "a::List(Array(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed9c77c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getAllPairs: (nums: Seq[Int])Seq[(Int, Int)]\n",
       "res25: Array[(Int, Int)] = Array((1,2), (1,3), (2,1), (2,3), (3,1), (3,2))\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getAllPairs(nums: Seq[Int]) =\n",
    "    nums.flatMap(x => nums.map(y => (x,y))).filter(p=>p._1 != p._2)\n",
    "getAllPairs((1 to 3)).toArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8e6bad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n1,+,n2)(n1,+,n3)(n2,+,n3)"
     ]
    }
   ],
   "source": [
    "(1 to 3).map(i => s\"n$i\").combinations(2).foreach(x=>print(x.toSeq(0).toString,\"+\",x.toSeq(1).toString))\n",
    "// .map ( x => x.mkString(\" \"))\n",
    "// .flatMap(_.combinations(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "43e1e69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res64: String = Vector(n1, n2)\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 to 3).map(i => s\"n$i\").combinations(2).toSeq(0).toString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9af580ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res31: Option[Int] = Some(0)\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updateFunction(Seq(),updateFunction(Seq(),None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04203305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indices: scala.collection.immutable.Range.Inclusive = Range 0 to 9\n",
       "res5: scala.collection.immutable.Range = Range 0 to 8\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indices = 0 to 10-1\n",
    "indices.slice(0,indices.length-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "791403be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Int = 9\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.length - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a439e8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: String = aaa\n",
       "res8: String = aa\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = \"aaa\"\n",
    "a.slice(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c464e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
