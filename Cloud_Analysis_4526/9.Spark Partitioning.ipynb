{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c29035",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:50px\">Partitioning</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4039b518",
   "metadata": {},
   "source": [
    "<li>Spark automatically partitions data. </li>\n",
    "<li>If using hadoop,  Spark uses the HDFS partition size for each partition (default = 512MB)</li>\n",
    "<li>Otherwise, the default is usually  = the number of cores on the machine</li>\n",
    "<li>But, partitioning is controllable by the user</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb8e5d",
   "metadata": {},
   "source": [
    "<li><span style=\"color:red\">Partition size too big</span>: you lose the benefits of working on data in parallel\n",
    "<li><span style=\"color:red\">Partition size too small</span>: the overhead of managing partitions may become too expensive\n",
    "<li>So, try to get close to <span style=\"color:red\">just right</span> by understanding the structure of your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96889b",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Creating partitions</span>\n",
    "<p>\n",
    "<li>sc.parallelize partitions the data into multiple (usually number of cores) partitions</li>\n",
    "<li>on OSX, the command <span style=\"color:red\">sysctl -n hw.ncpu</span> will tell you how many cores your machine has</li>\n",
    "<li>In the example below, each partition will be saved in a separate file on the disk</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a0e66d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.149:4044\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1666124313972)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "grades: Array[(String, String)] = Array((John,A), (Jack,B+), (Jill,C), (Qing,A+), (Mahesh,A), (Thierry,B+))\n",
       "grades_RDD: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[0] at parallelize at <console>:27\n",
       "size: Int = 8\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val grades = Array((\"John\",\"A\") , (\"Jack\",\"B+\"), (\"Jill\",\"C\"),\n",
    "                   (\"Qing\",\"A+\"),(\"Mahesh\",\"A\"),(\"Thierry\",\"B+\"))\n",
    "\n",
    "val grades_RDD = sc.parallelize(grades)\n",
    "val size = grades_RDD.partitions.size\n",
    "\n",
    "grades_RDD.saveAsTextFile(\"grades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed519cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 grades/_SUCCESS\r\n",
      "       0 grades/part-00000\r\n",
      "       1 grades/part-00001\r\n",
      "       1 grades/part-00002\r\n",
      "       1 grades/part-00003\r\n",
      "       0 grades/part-00004\r\n",
      "       1 grades/part-00005\r\n",
      "       1 grades/part-00006\r\n",
      "       1 grades/part-00007\r\n",
      "       6 total\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wc -l grades/* //word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68815d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(John,A)\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat grades/part-00001 //Concatenate files and print on the standard output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189b211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: grades: No such file or directory\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -r grades \n",
    "// remove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a9dcf",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Purposeful partitioning</span>\n",
    "<p>\n",
    "<li>If you have some knowledge about the distribution of your data, you can guide partitioning</li>\n",
    "<li>Spark allows for user driven partitioning on PairRDDs</li>\n",
    "<li>Spark provides two partitioning mechanisms</li>\n",
    "<ol>\n",
    "    <li><span style=\"color:red\">Hash Partitioners</span>: Creates key, value pairs by hashing on the key</li>\n",
    "    <li><span style=\"color:red\">Range Partitioners</span>: Sequentially allocates data across the partitions</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987df12",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">hash partitioning</span>\n",
    "<p>\n",
    "        <li>decide on the number of partitions</li>\n",
    "        <li>hash keys to partition </li>\n",
    "        <li>allocate key,value pairs to the correct partition</li>\n",
    "        <li>The partitioning step is a transformation, not an action</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e9a004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NYC_Data_Path: String = nyc_311_2022_clean.csv\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NYC_Data_Path = \"nyc_311_2022_clean.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4a4b1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Int = 49\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(NYC_Data_Path).getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5595112c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\n",
       "hash_data: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[16] at partitionBy at <console>:31\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val hash_data = sc.textFile(NYC_Data_Path)\n",
    "                        .mapPartitionsWithIndex{ (idx,iter) => if (idx==0) iter.drop(1) else iter}\n",
    "                        .map(l=>l.split(\",\"))\n",
    "                        .map(t => (t(2),t(10).toDouble))\n",
    "                        .partitionBy(new HashPartitioner(10)) //Partition the data into 10 hashed sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be51fd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Int = 10\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_data.getNumPartitions\n",
    "//the 49 parition will not occur since there are transformations in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e492af25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[17] at mapPartitions at <console>:28\n",
       "res5: Array[Int] = Array(1494658, 583954, 0, 332647, 11897, 4435357, 61183, 236898, 1679035, 0)\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cat = hash_data.mapPartitions(iter => Iterator(iter.length)) //number of pairs in each partition\n",
    "cat.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd439d7c",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Examine the partitions</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cbbba3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[10] at mapPartitions at <console>:25\n",
       "res4: Array[Int] = Array(1494658, 583954, 0, 332647, 11897, 4435357, 61183, 236898, 1679035, 0)\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cat = hash_data.mapPartitions(iter => Iterator(iter.length)) //number of pairs in each partition\n",
    "cat.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "962714b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r nyc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fede6f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_data.saveAsTextFile(\"nyc_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9ebb6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4435356\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!grep \"NYPD\" nyc_data/part-00005 | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35e8cc",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">range partitioning</span>\n",
    "<p>\n",
    "<li>Find the minimum and maximum values of the keys</li>\n",
    "<li>divide the difference by the number of partitions to get partition key range sizes</li>\n",
    "<li>set partition boundaries using the key range size</li>\n",
    "<li>Range Partitions are transformations, not actions</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "249bcec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.RangePartitioner\n",
       "agency_time_map: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[64] at map at <console>:50\n",
       "range_data: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[67] at partitionBy at <console>:51\n",
       "cat: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[68] at mapPartitions at <console>:52\n",
       "res10: Array[Int] = Array(1315468, 1775422, 11897, 1233935, 57511, 4435356, 6040)\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.RangePartitioner\n",
    "val agency_time_map = sc.textFile(NYC_Data_Path)\n",
    "                        .mapPartitionsWithIndex{ (idx,iter) => if (idx==0) iter.drop(1) else iter}\n",
    "                        .map(l=>l.split(\",\"))\n",
    "                        .map(t => (t(2),t(10).toDouble))\n",
    "val range_data = agency_time_map.partitionBy(new RangePartitioner(10,agency_time_map))\n",
    "val cat = range_data.mapPartitions(iter => Iterator(iter.length))\n",
    "cat.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daac94de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Int = 7\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f3970b",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">hash partitioner vs range partitioner</span>\n",
    "<p>\n",
    "<li>When the data is ordered, range partitioners are preferred</li>\n",
    "<li>Range Partitioners need a distribution and typically make two passes (more expensive)</li>\n",
    "<li>If the distribution is unstable, then range partitioning can be problematic</li>\n",
    "<li>Let's look at an example</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ae01f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\n",
       "import org.apache.spark.RangePartitioner\n",
       "rdd1: org.apache.spark.rdd.RDD[(Long, Int)] = MapPartitionsRDD[71] at map at <console>:46\n",
       "rdd2: org.apache.spark.rdd.RDD[(Long, Int)] = MapPartitionsRDD[74] at map at <console>:47\n",
       "hash_data1: org.apache.spark.rdd.RDD[(Long, Int)] = ShuffledRDD[75] at partitionBy at <console>:48\n",
       "hash_data2: org.apache.spark.rdd.RDD[(Long, Int)] = ShuffledRDD[76] at partitionBy at <console>:49\n",
       "h1: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[77] at mapPartitions at <console>:50\n",
       "h2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[78] at mapPartitions at <console>:51\n",
       "r_partitioner: org.apache.spark.RangePartitioner[Long,Int] = org.apache.spark.RangePartitioner@b2cf1b58\n",
       "ran_data1: org.apache.spark.rdd.RDD[(Long, Int)] = Shu...\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "import org.apache.spark.RangePartitioner\n",
    "val rdd1 = sc.range(1,1000).map(x => (x,1))\n",
    "val rdd2 = sc.range(900,1900).map(x=>(x,1))\n",
    "\n",
    "val hash_data1 = rdd1.partitionBy(new HashPartitioner(10))\n",
    "val hash_data2 = rdd2.partitionBy(new HashPartitioner(10))\n",
    "val h1 = hash_data1.mapPartitions(iter => Iterator(iter.length))\n",
    "val h2 = hash_data2.mapPartitions(iter => Iterator(iter.length))\n",
    "\n",
    "\n",
    "val r_partitioner = new RangePartitioner(10,rdd1)\n",
    "val ran_data1 = rdd1.partitionBy(r_partitioner)\n",
    "val ran_data2 = rdd2.partitionBy(r_partitioner)\n",
    "val r1 = ran_data1.mapPartitions(iter => Iterator(iter.length))\n",
    "val r2 = ran_data2.mapPartitions(iter => Iterator(iter.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36127673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2_partitioner: org.apache.spark.RangePartitioner[Long,Int] = org.apache.spark.RangePartitioner@f303e653\n",
       "ran_data22: org.apache.spark.rdd.RDD[(Long, Int)] = ShuffledRDD[90] at partitionBy at <console>:50\n",
       "r22: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[91] at mapPartitions at <console>:51\n",
       "res17: Array[Int] = Array(104, 98, 94, 107, 95, 96, 106, 96, 107, 97)\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r2_partitioner = new RangePartitioner(10,rdd2)\n",
    "val ran_data22 = rdd2.partitionBy(r2_partitioner)\n",
    "val r22 = ran_data22.mapPartitions(iter => Iterator(iter.length))\n",
    "r22.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77926ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Array[Int] = Array(99, 100, 100, 100, 100, 100, 100, 100, 100, 100)\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1.collect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94b8879f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Array[Int] = Array(100, 100, 100, 100, 100, 100, 100, 100, 100, 100)\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2.collect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8dd14555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Array[Int] = Array(101, 100, 102, 103, 96, 91, 107, 96, 98, 105)\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.collect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28a1ca23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 1000)\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c185e39",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Calibrating a Spark model</span>\n",
    "<p>\n",
    "<li>A spark context comes with a handy UI for evaluating the performance of a Spark model</li>\n",
    "<li>http://localhost:4040 (usually)</li>\n",
    "<li>Let's see how well the different partitions perform on our data when computing average processing time for each agency</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceeb403",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Average processing time analysis</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428f7cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "combiner: Double => (Int, Double) = $Lambda$2326/0x0000000800e8e840@5774109\n",
       "merger: ((Int, Double), Double) => (Int, Double) = $Lambda$2327/0x0000000800e8d840@14eed62e\n",
       "mergeAndCombiner: ((Int, Double), (Int, Double)) => (Int, Double) = $Lambda$2328/0x0000000800e8d040@1ed65bc5\n",
       "getAvgFunction: ((String, (Int, Double))) => (String, Double) = $Lambda$2329/0x0000000800e8c040@389b2be2\n",
       "agency_time_map: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[4] at map at <console>:45\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//All transformations. So no processing cost\n",
    "\n",
    "val combiner = (x: Double) => (1,x)\n",
    "val merger = (x: (Int, Double),y: Double) => {\n",
    "    val (c,acc) = x\n",
    "    (c+1, acc + y)\n",
    "}\n",
    "val mergeAndCombiner = (x1: (Int, Double), x2: (Int, Double)) => {\n",
    "    val (c1, acc1) = x1\n",
    "    val (c2, acc2) = x2\n",
    "    (c1+c2,acc1+acc2)\n",
    "}\n",
    "val getAvgFunction = (x: (String, (Int, Double))) => {\n",
    "    val (identifier, (count,total)) = x\n",
    "    (identifier,total/count)\n",
    "}\n",
    "\n",
    "val agency_time_map = sc.textFile(NYC_Data_Path)\n",
    "                        .mapPartitionsWithIndex{ (idx,iter) => if (idx==0) iter.drop(1) else iter}\n",
    "                        .map(l=>l.split(\",\"))\n",
    "                        .map(t => (t(2),t(10).toDouble))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee40f4f",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Hash partitioner</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5edda7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\n",
       "hash_data: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[5] at partitionBy at <console>:28\n",
       "h_avg_times: Array[(String, Double)] = Array((DOITT,28.392176800309258), (DSNY,6.984900241530284), (MAYORâS OFFICE OF SPECIAL ENFORCEMENT,10.993121710573053), (DCA,3.1134426905912482), (DOE,43.447434186637715), (DPR,65.99335925229805), (HPD,13.242378653308426), (EDC,56.00096076391685), (DOHMH,15.397117888616057), (DOF,19.762443628704336), (OSE,0.12648533950617283), (TLC,53.68203580195772), (DEP,5.0066760309900165), (FDNY,402.1443981481481), (DOT,14.494687185115032), (OFFICE OF TECHNOLOGY AND INNOVATION,0.7780439814814815), (DFTA,13.390725308641976), (NYPD,0.3395727580072987), (DOB,39.106442592864965), (DHS,1.2574107460376571))\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val hash_data = agency_time_map.partitionBy(new HashPartitioner(2))\n",
    "val h_avg_times = hash_data\n",
    "                    .combineByKey(combiner,merger,mergeAndCombiner)\n",
    "                    .map(t => (t._1,t._2._2.toDouble/t._2._1))\n",
    "                    .collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068aea7c",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Range partitioner</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0972e05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.RangePartitioner\n",
       "r_partitioner: org.apache.spark.RangePartitioner[String,Double] = org.apache.spark.RangePartitioner@474db71\n",
       "r_avg_times: Array[(String, Double)] = Array((DOITT,28.392176800309258), (DSNY,6.984900241530284), (DOF,19.762443628704336), (MAYORâS OFFICE OF SPECIAL ENFORCEMENT,10.993121710573053), (DEP,5.0066760309900165), (DCA,3.1134426905912482), (FDNY,402.1443981481481), (DOT,14.494687185115032), (DOE,43.447434186637715), (DPR,65.99335925229805), (DFTA,13.390725308641976), (NYPD,0.3395727580072987), (HPD,13.242378653308426), (EDC,56.00096076391685), (DOB,39.106442592864965), (DOHMH,15.397117888616057), (DHS,1.2574107460376571), (OSE,0.12648533950617283), (TLC,53.68203580195772), (OFFICE OF TECHNOLOGY AND INNOVATION,0.7780439814814815))\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.RangePartitioner\n",
    "val r_partitioner = new RangePartitioner(2,agency_time_map)\n",
    "val r_avg_times = agency_time_map.partitionBy(r_partitioner)\n",
    "                .combineByKey(combiner,merger,mergeAndCombiner)\n",
    "                    .map(t => (t._1,t._2._2.toDouble/t._2._1))\n",
    "                    .collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c09e58",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Custom partitioner</span>\n",
    "<li>Spark allows you to write your own partitioner</li>\n",
    "<li>Only on (key,value) pairs</li>\n",
    "<li>Hard to come up with a good use case because you need to know the key values in advance</li>\n",
    "<li>but, consider our nyc data. We know what the various departments are so we know the keys</li>\n",
    "<li>Let's build a partitioner that will partition based on the length of the agency name</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaae02b",
   "metadata": {},
   "source": [
    "<li>First, we need to decide on the number of partitions</li>\n",
    "<li>Then, we'll define a max key length and a min key length</li>\n",
    "<li>Finally partition the data using the number of partitions, the max and the min key lengths</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "492f3812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[(String, Double)] = Array((DOITT,28.392176800309258), (DSNY,6.984900241530284), (MAYORâS OFFICE OF SPECIAL ENFORCEMENT,10.993121710573053), (DCA,3.1134426905912482), (DOE,43.447434186637715), (DPR,65.99335925229805), (HPD,13.242378653308426), (EDC,56.00096076391685), (DOHMH,15.397117888616057), (DOF,19.762443628704336), (OSE,0.12648533950617283), (TLC,53.68203580195772), (DEP,5.0066760309900165), (FDNY,402.1443981481481), (DOT,14.494687185115032), (OFFICE OF TECHNOLOGY AND INNOVATION,0.7780439814814815), (DFTA,13.390725308641976), (NYPD,0.3395727580072987), (DOB,39.106442592864965), (DHS,1.2574107460376571))\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_avg_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7fe8df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "max_len: Int = 39\n",
       "min_len: Int = 3\n",
       "keys: Array[String] = Array(DOITT, DSNY, MAYORâS OFFICE OF SPECIAL ENFORCEMENT, DCA, DOE, DPR, HPD, EDC, DOHMH, DOF, OSE, TLC, DEP, FDNY, DOT, OFFICE OF TECHNOLOGY AND INNOVATION, DFTA, NYPD, DOB, DHS)\n",
       "key_lengths: Array[Int] = Array(5, 4, 39, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 4, 3, 35, 4, 4, 3, 3)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//max and min key lengths\n",
    "val max_len = h_avg_times.map(t=>t._1.length).max\n",
    "val min_len = h_avg_times.map(t=>t._1.length).min\n",
    "val keys = h_avg_times.map(t=>t._1)\n",
    "val key_lengths = h_avg_times.map(t=>t._1.length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7047a97a",
   "metadata": {},
   "source": [
    "<li>Since the two ultra long keys are outliers, we'll reset max to 6</li>\n",
    "<li>Note that we'll be using for loops and vars in this example!</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac013f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "max_len: Int = 6\n",
       "min_len: Int = 3\n",
       "n: Int = 3\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val max_len=6\n",
    "val min_len=3\n",
    "val n = 3//the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f461f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: custom: No such file or directory\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -r custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82bf661e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class KeyLenPartitioner\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val max_len=6\n",
    "val min_len=3\n",
    "val n = 3//the number of partitions\n",
    "\n",
    "class KeyLenPartitioner(num:Int,max_len:Int,min_len:Int) extends org.apache.spark.Partitioner{\n",
    "    override def numPartitions: Int = num //This is necessary!\n",
    "\n",
    "    override def getPartition(key: Any): Int = {\n",
    "        import scala.util.control.Breaks._\n",
    "        val partition_increment = (max_len-min_len)/num //Note that this will be an Int\n",
    "        val key_length = key.toString.size\n",
    "        var partition = 0\n",
    "        breakable {\n",
    "            for(i<-num-1 to 0 by -1) {\n",
    "                if (key_length >= min_len + partition_increment*num) {\n",
    "                    partition = partition + 1\n",
    "                    break\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "        partition\n",
    "    }\n",
    "\n",
    " }\n",
    "\n",
    "\n",
    "\n",
    "agency_time_map\n",
    "    .partitionBy(new KeyLenPartitioner(n,max_len,min_len))\n",
    "    .saveAsTextFile(\"custom\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ad41801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 custom/_SUCCESS\n",
      "\n",
      " 8778116 custom/part-00000\n",
      "   57513 custom/part-00001\n",
      "       0 custom/part-00002\n",
      " 8835629 total\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wc -l custom/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f59eacf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.math.sqrt\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math.sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c4403af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Double = 5.000000000000001\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/03 04:28:24 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 680521 ms exceeds timeout 120000 ms\n",
      "22/10/03 04:28:24 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "2.23606797749979 * 2.23606797749979"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0f0d456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Double = 42.99999999999999\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt(43)*sqrt(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed07db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
