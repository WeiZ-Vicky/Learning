{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">Apache Spark</span>\n",
    "<br><br>\n",
    "<div class=\"list\">\n",
    "<span>A <b>framework</b> for <b>cluster computing</b></span>\n",
    "<ul>\n",
    "    <li>becoming a standard for \"big data\" analytics</li>\n",
    "    <li>provides services for streaming analytics</li>\n",
    "    <li>provides services for graph analytics</li>\n",
    "    <li>provides services for machine learning</li>\n",
    "    <li>provides services for in-memory data querying</li>\n",
    "</ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"list\">\n",
    "<li>Spark is written in <span style=\"color:blue\">Scala</span></li>\n",
    "<li>Can be compiled on JVM(Java Virtual Machine)</li>\n",
    "<li>Has APIs for Scala, Python, Java, and R</li>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classnotes:\n",
    "\n",
    "Advantage of Scala over Python:\n",
    "- strongly typed\n",
    "- immutable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">A Spark Program</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.11.104:4041\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1675373514373)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[String] = shakespeare.txt MapPartitionsRDD[1] at textFile at <console>:24\n",
       "relevant_lines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <console>:27\n",
       "result: Long = 52\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = sc.textFile(\"shakespeare.txt\")\n",
    "// spark function\n",
    "// text is scala variable contains spark object\n",
    "val relevant_lines = text.filter(l => l.contains(\"Music\"))\n",
    "val result = relevant_lines.count()//scala object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<img src=\"spark_context.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:x-large\">Spark context</span>\n",
    "<div class=\"list\">\n",
    "<li>Spark context encapsulates the connection between the application and the cluster</li>\n",
    "    <li>It handles job distribution, broadcasting, creating in-memory datasets (distributed), etc.</li> \n",
    "<li>Jupyter, spark-shell both automatically create the environment for us</li>\n",
    "<li>Only one spark context can be active at a time on  a Java Virtual Machine (Spark runs as a JVM) - two or more would be very confusing - why?</li>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@1493a28c\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: String = spylon-kernel\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: String = local[*]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<img src=\"spark_environment.png\">\n",
    "<p>\n",
    "</p>\n",
    "<h3>Notes</h3>\n",
    "<li>Each application gets its own environment</li>\n",
    "<li>The environment runs from the point the application starts until it terminates</li>\n",
    "<li><b>Each application is, thus, isolated from other applications!</b></li>\n",
    "<ul>\n",
    "    <li>Each application schedules its own tasks</li>\n",
    "    <li>Each application runs in its own JVM</li>\n",
    "    <li>However, data cannot be shared between applications (but can be shared through an external file system)</li>\n",
    "</ul>\n",
    "<li>The cluster manager can be Spark's own cluster manager or some other cluster manager (YARN, Kubernetes). I.e., Spark's driver program can talk to non-spark cluster managers</li>\n",
    "<li>The driver program is network addressable. This makes it possible for its workers to send messages to the driver through the lifetime of the application without requiring physical connectivity</li>\n",
    "<li>However, to ensure quick turnaround, spark drivers and workers are typically on the same local network and use local network addressing to communicate</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Lazy Spark</span>\n",
    "<br><br>\n",
    "<li>Spark is intrinically lazy. Nothing is evaluated unless there is an action step</li>\n",
    "<li>In our program, count() is the evaluation step</li>\n",
    "<li>For example, the following code does not give an error</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[String] = file_does_not_exist MapPartitionsRDD[7] at textFile at <console>:26\n",
       "relevant_lines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[8] at filter at <console>:27\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//lazy steps\n",
    "val text = sc.textFile(\"file_does_not_exist\") //did not check whether the resource exists or not\n",
    "val relevant_lines = text.filter(l => l.contains(\"Music\"))\n",
    "//val result = relevant_lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>But this code does</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.hadoop.mapred.InvalidInputException",
     "evalue": " Input path does not exist: file:/Users/weizhou/Columbia Coursework/Cloud Analysis 4526/file_does_not_exist",
     "output_type": "error",
     "traceback": [
      "org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/weizhou/Columbia Coursework/Cloud Analysis 4526/file_does_not_exist",
      "  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)",
      "  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)",
      "  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)",
      "  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)",
      "  at org.apache.spark.rdd.RDD.count(RDD.scala:1274)",
      "  ... 38 elided",
      "Caused by: java.io.IOException: Input path does not exist: file:/Users/weizhou/Columbia Coursework/Cloud Analysis 4526/file_does_not_exist",
      "  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)",
      "  ... 54 more",
      ""
     ]
    }
   ],
   "source": [
    "val text = sc.textFile(\"file_does_not_exist\")\n",
    "val relevant_lines = text.filter(l => l.contains(\"Music\"))\n",
    "val result = relevant_lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[String] = shakespeare.txt MapPartitionsRDD[13] at textFile at <console>:26\n",
       "relevant_lines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[14] at filter at <console>:27\n",
       "result: Long = 52\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = sc.textFile(\"shakespeare.txt\")\n",
    "val relevant_lines = text.filter(l => l.contains(\"Music\"))\n",
    "val result = relevant_lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Resilient Distributed Datasets</span>\n",
    "<br><p>\n",
    "A Resilient Distributed Dataset (RDD) is the primary data abstraction used in Spark\n",
    "    <p>\n",
    "<br>RDDs are:\n",
    "<li>immutable (read only)</li>\n",
    "<li>resilient (fault tolerant) </li>\n",
    "<li>distributed (spread on multiple nodes) </li>\n",
    "<p>\n",
    "    RDDs allow for low level (programming level) operations on data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Properties of RDDs</span>\n",
    "<br><p>\n",
    "<li>immutable, resilient, distributed (see above)</li>\n",
    "<li><b>in-memory</b> as far as possible, an RDD is maintained in memory for faster computation</li>\n",
    "<li><b>cached</b> if space is not available, the RDD is stored on disk in a memory cache</li>\n",
    "<li><b>lazy evaluation</b> RDDs are not evaluated unless an action step is encountered</li>\n",
    "<li><b>parallel computation</b> when evaluation is necessary, it is done in parallel on each partition</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[String] = shakespeare.txt MapPartitionsRDD[9] at textFile at <console>:24\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = sc.textFile(\"shakespeare.txt\")\n",
    "//val text1 = sc.textFile(\"shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Partitioning</span>\n",
    "<br><br>\n",
    "<li>Notice that the data type of the shakepeare text rdd is MapPartitionsRDD</li>\n",
    "<li>Spark automatically partitions the data and, when a task needs to be run, allocates jobs to each partition to run on its data</li>\n",
    "<li>Partitions can be set by the programmer or by default (usually based on the number of cores)</li>\n",
    "<li>The number of partitions is a trade-off between communication overhead (master nodes to worker nodes) and processing time within a partition (a function of the size of the data in that node)</li>\n",
    "<li>Note that you can have multiple partitions running on a single core</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Cores on a mac</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw.physicalcpu: 8\r\n",
      "hw.logicalcpu: 8\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sysctl hw.physicalcpu hw.logicalcpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Cores on windows</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://support.microsoft.com/en-us/help/4026757/windows-10-find-out-how-many-cores-your-processor-has"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">partitions are maintained in an array of rdds</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.rdd.RDD[String] = shakespeare.txt MapPartitionsRDD[16] at textFile at <console>:24\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[org.apache.spark.Partition] = Array(org.apache.spark.rdd.HadoopPartition@4b9, org.apache.spark.rdd.HadoopPartition@4ba)\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Int = 2\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.partitions.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Setting the number of partitions</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[String] = shakespeare.txt MapPartitionsRDD[6] at textFile at <console>:25\n",
       "np: Int = 5\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = sc.textFile(\"shakespeare.txt\",5)\n",
    "val np = text.partitions.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Int = 5\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[String] = Array(\"The Project Gutenberg EBook of The Complete Works of William Shakespeare, by \", William Shakespeare, \"\", This eBook is for the use of anyone anywhere at no cost and with, almost no restrictions whatsoever.  You may copy it, give it away or, re-use it under the terms of the Project Gutenberg License included, with this eBook or online at www.gutenberg.org, \"\", ** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **, **     Please follow the copyright guidelines in this file.     **, \"\", Title: The Complete Works of William Shakespeare, \"\", Author: William Shakespeare, \"\", Posting Date: September 1, 2011 [EBook #100], Release Date: January, 1994, \"\", Language: English, \"\", \"\", *** START OF THIS PROJECT GUTENBERG EBOOK COMPLETE WORKS--WILLIAM SHAKESP...\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Array[String] = Array(\"The Project Gutenberg EBook of The Complete Works of William Shakespeare, by \", William Shakespeare, \"\", This eBook is for the use of anyone anywhere at no cost and with, almost no restrictions whatsoever.  You may copy it, give it away or, re-use it under the terms of the Project Gutenberg License included, with this eBook or online at www.gutenberg.org, \"\", ** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **, **     Please follow the copyright guidelines in this file.     **, \"\", Title: The Complete Works of William Shakespeare, \"\", Author: William Shakespeare, \"\", Posting Date: September 1, 2011 [EBook #100], Release Date: January, 1994, \"\", Language: English, \"\", \"\", *** START OF THIS PROJECT GUTENBERG EBOOK COMPLETE WORKS--WILLIAM SHAKES...\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Creating partitions using parallelize</span>\n",
    "<li><span style=\"color:green\">parallelize</span> can be used to partition any data object</li>\n",
    "<li>as with sc.textFile, you can specify the number of partitions</li>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[Char] = ParallelCollectionRDD[7] at parallelize at <console>:25\n",
       "np: Int = 8\n",
       "res8: Array[Char] = Array(s, h, a, k, e, s, p, e, a, r, e, ., t, x, t)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = sc.parallelize(\"shakespeare.txt\")\n",
    "val np = text.getNumPartitions//number of CPUs\n",
    "\n",
    "text.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Int = 8\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.partitions.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "names: Array[String] = Array(John, Qing, Vladimir, Audrey, Baskin, Robbins)\n",
       "n: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[18] at parallelize at <console>:27\n",
       "res15: Array[String] = Array(John, Qing, Vladimir, Audrey, Baskin, Robbins)\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val names = Array(\"John\",\"Qing\",\"Vladimir\",\"Audrey\",\"Baskin\",\"Robbins\")\n",
    "val n = sc.parallelize(names)\n",
    "n.getNumPartitions\n",
    "n.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[Char] = ParallelCollectionRDD[21] at parallelize at <console>:25\n",
       "np: Int = 25\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = sc.parallelize(\"shakespeare.txt\",25)\n",
    "val np = text.getNumPartitions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"list\">\n",
    "<li><span style=\"color:blue\">sc.textFile</span> uses hadoop's block size as a guide and a default minimum partitions as a floor for the number of partitions</li>\n",
    "<li><span style=\"color:blue\">sc.parallelize</span> uses a default number of partitions \n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "print(sc.defaultMinPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<p>\n",
    "<span style=\"color:green;font-size:xx-large\">RDD operations</span>\n",
    "<li>RDDs are lazy</li>\n",
    "<li>Two kinds of operations are allowed</li>\n",
    "<ol><li>transformations: a function that produces a new RDD from an existing RDD</li>\n",
    "    <li>actions: a function that returns actual values (not RDDs) from an RDD</li>\n",
    "    </ol>\n",
    "    <li>The set of transformations and their order associated with an RDD is called its <b>lineage</b></li>\n",
    "<br><br>\n",
    "<li><a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\">transformations</a> </li>\n",
    "<li><a htef=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions\">actions</a></li>\n",
    "<li><b>Important!</b> Transformations and actions may look like Scala functions but they are Spark RDD operations!</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lineage.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">lineage of relevant_lines</span>\n",
    "<p>\n",
    "    <li>The RDD attribute <span style=\"color:blue\">toDebugString</span> returns the lineage of an RDD</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[String] = shakespeare.txt MapPartitionsRDD[10] at textFile at <console>:26\n",
       "relevant_lines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at filter at <console>:27\n",
       "result: Long = 52\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = sc.textFile(\"shakespeare.txt\")\n",
    "val relevant_lines = text.filter(l => l.contains(\"Music\"))\n",
    "val result = relevant_lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: String =\n",
       "(2) MapPartitionsRDD[11] at filter at <console>:27 []\n",
       " |  shakespeare.txt MapPartitionsRDD[10] at textFile at <console>:26 []\n",
       " |  shakespeare.txt HadoopRDD[9] at textFile at <console>:26 []\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_lines.toDebugString\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">RDD actions</span>\n",
    "<li><span style=\"color:red\">.toDebugString</span>: returns the lineage of an RDD\n",
    "<li><span style=\"color:red\">.count:</span> counts the number of elements in the RDD\n",
    "<li><span style=\"color:red\">.first</span>: returns the first element of the RDD\n",
    "<li><span style=\"color:red\">.take(n)</span>: returns the first n elements in an RDD\n",
    "<li><span style=\"color:red\">.takeOrdered(n)(function)</span>: returns the first n according to the ordering function\n",
    "<li><span style=\"color:red\">.collect</span>: collects all the data resulting from the series of operations in one node. Use carefully, because the data must fit in the master node memory!\n",
    "<li><span style=\"color:red\">.foreach</span>: applies a function to each element in an RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[String] = shakespeare.txt MapPartitionsRDD[13] at textFile at <console>:26\n",
       "relevant_lines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[14] at filter at <console>:27\n",
       "result: Long = 52\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = sc.textFile(\"shakespeare.txt\")\n",
    "val relevant_lines = text.filter(l => l.contains(\"Music\"))\n",
    "val result = relevant_lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of The Complete Works of William Shakespeare, by \n",
      "William Shakespeare\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n"
     ]
    }
   ],
   "source": [
    "text.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your written explanation.  The person or entity that provided you with\n",
      "your equipment.\n",
      "your debt. But a good conscience will make any possible\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "r3: Unit = ()\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r3 = text.takeOrdered(3)(Ordering[String].reverse).foreach(println)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    whither wilt?' ROSALIND. Nay, you might keep that check for it, till you met your\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK COMPLETE WORKS--WILLIAM SHAKESPEARE ***\n",
      "*** END OF THIS PROJECT GUTENBERG EBOOK COMPLETE WORKS--WILLIAM SHAKESPEARE ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "r4: Unit = ()\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r4 = text.takeOrdered(3)(Ordering[Double].reverse.on(x => 1.0 * x.length)).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r1: String = \"The Project Gutenberg EBook of The Complete Works of William Shakespeare, by \"\n",
       "r2: Array[String] = Array(\"The Project Gutenberg EBook of The Complete Works of William Shakespeare, by \", William Shakespeare, \"\")\n",
       "r3: Array[String] = Array(your written explanation.  The person or entity that provided you with, your equipment., your debt. But a good conscience will make any possible)\n",
       "r4: Array[String] = Array(\"    whither wilt?' ROSALIND. Nay, you might keep that check for it, till you met your\", *** START OF THIS PROJECT GUTENBERG EBOOK COMPLETE WORKS--WILLIAM SHAKESPEARE ***, *** END OF THIS PROJECT GUTENBERG EBOOK COMPLETE WORKS--WILLIAM SHAKESPEARE ***)\n",
       "r5: Array[String] = Array(\"    whither wilt?' ROSALIND. Nay, you might keep that check for it, till you met your\", *** ST...\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r1 = text.first\n",
    "val r2 = text.take(3)\n",
    "val r3 = text.takeOrdered(3)(Ordering[String].reverse)\n",
    "val r4 = text.takeOrdered(3)(Ordering[Double].reverse.on(x => 1.0*x.length))\n",
    "val r5 = text.takeOrdered(3)(Ordering[Double].on(x => 1.0/x.length))\n",
    "val r6 = text.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your written explanation.  The person or entity that provided you with\n",
      "your equipment.\n",
      "your debt. But a good conscience will make any possible\n",
      "you- but, indeed, to pray for the Queen.\n",
      "you!) can copy and distribute it in the United States without permission\n",
      "you must, at no additional cost, fee or expense to the user, provide a\n",
      "you good night.\n",
      "written explanation to the person you received the work from.  If you\n",
      "works.  See paragraph 1.E below.\n",
      "works.\n"
     ]
    }
   ],
   "source": [
    "text.takeOrdered(10)(Ordering[String].reverse).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Try this: Extend our simple program</span>\n",
    "<li>limit results to lines that are less than 30 characters in length and contain the word Music or music</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ALL. The music, ho!\n",
      "    provided this music?\n",
      "[followed by Musicians].\n",
      "    Music.\n",
      "    Come, some music!\n",
      "    Music do I hear?\n",
      "    music.\n",
      "    Like music.\n",
      "  Three Musicians.\n",
      "    Held current music too.\n",
      "    What music is this?\n",
      "    As howling after music.\n",
      "  JULIA. That will be music.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text: org.apache.spark.rdd.RDD[String] = shakespeare.txt MapPartitionsRDD[50] at textFile at <console>:26\n",
       "relevant_lines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[51] at filter at <console>:27\n",
       "result: Long = 13\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = sc.textFile(\"shakespeare.txt\")//scala string//spark API textFile\n",
    "val relevant_lines = text.filter(l => l.length<30 & (l.contains(\"Music\") | l.contains(\"music\"))) //scala doing the filter\n",
    "val result = relevant_lines.count()\n",
    "relevant_lines.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//if in python kernel\n",
    "text = sc.textFile(\"shakespeare.txt\")\n",
    "relevant_lines = text.filter(lambda l: l.length<30 and (l.contains(\"Music\") or l.contains(\"music\")))\n",
    "print(result = relevant_lines.count())"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
