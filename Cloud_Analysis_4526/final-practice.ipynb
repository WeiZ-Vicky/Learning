{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ff79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.packages= [\"graphframes:graphframes:0.8.2-spark3.2-s_2.12\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f81cd7",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb12f14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.149:4043\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1671562624389)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|document_id|tfidfVec                                                                                                                                                                                     |\n",
      "+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|doc 1      |(12,[1,2,3,4,5,6,7,8,9,11],[0.0,0.0,0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644])|\n",
      "|doc 2      |(12,[0,1,2,10],[1.6218604324326575,0.0,0.0,0.4054651081081644])                                                                                                                              |\n",
      "+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "doc1: (String, String) =\n",
       "(doc 1,\"\n",
       "Columbia University is a large university in New York.\n",
       "It has many schools including Columbia College, Engineering School, Law School, and Business School.\n",
       "It was established in 1754\n",
       "\")\n",
       "doc2: (String, String) =\n",
       "(doc 2,\"\n",
       "Operations Research is a department in the Engineering School of Columbia University.\n",
       "Operations Research was established in 1919.\n",
       "Operations Research has a BS major and offers many MS degrees.\n",
       "Graduates of Operations Research get good jobs and have a very happy life.\n",
       "\")\n",
       "both_uc: (w1: String, w2: String)Boolean\n",
       "split_data: (a: String)Array[String]\n",
       "clean_data: (a: String)String\n",
       "replace_entities: (a: Array[String])Array[String]\n",
       "clean_data_udf: org.apache.spark.sql.expressions.UserDefinedFunction = Spar...\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "val doc1 = (\"doc 1\",\"\"\"\n",
    "Columbia University is a large university in New York.\n",
    "It has many schools including Columbia College, Engineering School, Law School, and Business School.\n",
    "It was established in 1754\n",
    "\"\"\")\n",
    "val doc2 = (\"doc 2\",\"\"\"\n",
    "Operations Research is a department in the Engineering School of Columbia University.\n",
    "Operations Research was established in 1919.\n",
    "Operations Research has a BS major and offers many MS degrees.\n",
    "Graduates of Operations Research get good jobs and have a very happy life.\n",
    "\"\"\")\n",
    "\n",
    "def both_uc(w1: String,w2: String): Boolean = \n",
    "{\n",
    "    w1(0).isUpper & w2(0).isUpper\n",
    "}\n",
    "\n",
    "def split_data(a: String): Array[String] = a.split(\"\\\\s+\")\n",
    "\n",
    "def clean_data(a: String): String = \n",
    "{\n",
    "//     a.filter(a=>a != ','  & a != '.' & a != '\\n').trim() //not works because filter \\n will concate words together\n",
    "    a.replace(\"\\n\",\" \").replace(\".\",\" \").replace(\",\",\" \").replace(\"  \",\" \").trim()\n",
    "}\n",
    "\n",
    "\n",
    "def replace_entities(a: Array[String]):Array[String] = {\n",
    "    val indices = 0 to a.length-1\n",
    "    indices.slice(0,indices.length-1) //not include indices.length-1\n",
    "    .flatMap(i => \n",
    "         if (both_uc(a(i),a(i+1))) Some(a(i)+a(i+1))\n",
    "         else None)\n",
    "    .toArray\n",
    "}\n",
    "\n",
    "val clean_data_udf = udf(clean_data _)\n",
    "val split_data_udf = udf(split_data _)\n",
    "val replace_entities_udf = udf(replace_entities _)\n",
    "\n",
    "def make_df(a: Seq[(String,String)]): DataFrame = {\n",
    "    sc.parallelize(a)\n",
    "        .toDF(\"document_id\",\"document_text\")\n",
    "        .withColumn(\"cleaned_text\",clean_data_udf($\"document_text\"))\n",
    "        .withColumn(\"document_terms\",split_data_udf($\"cleaned_text\"))\n",
    "        .withColumn(\"entity_terms\",replace_entities_udf($\"document_terms\"))\n",
    "}\n",
    "\n",
    "val df = make_df(Array(doc1,doc2))\n",
    "\n",
    "//Using CountVectorizer, generate term_freqs column\n",
    "import org.apache.spark.ml.feature.CountVectorizer\n",
    "val countVectorizer = new CountVectorizer()\n",
    "    .setInputCol(\"entity_terms\")\n",
    "    .setOutputCol(\"term_freqs\")\n",
    "    .setVocabSize(20)\n",
    "\n",
    "val vocabModel = countVectorizer.fit(df)\n",
    "val freqs = vocabModel.transform(df)\n",
    "\n",
    "//Using IDF get the tfidfVec\n",
    "import org.apache.spark.ml.feature.IDF\n",
    "\n",
    "val idf = new IDF().setInputCol(\"term_freqs\").setOutputCol(\"tfidfVec\")\n",
    "val idfModel = idf.fit(freqs)\n",
    "val idfMatrix = idfModel.transform(freqs).select(\"document_id\",\"tfidfVec\")\n",
    "\n",
    "idfMatrix.show(false) //The Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c214d",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892c5932",
   "metadata": {},
   "source": [
    "# note: GraphFrame, bfs, CONS!, mapValues, getOrElse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a31fbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.graphframes._\n",
       "vertexArray: Array[(Int, Int)] = Array((1,1), (2,2), (3,3), (4,4), (5,5), (6,6), (7,7), (8,8))\n",
       "edgeArray: Array[(Int, Int)] = Array((1,3), (2,3), (2,4), (4,5), (3,5), (5,6), (6,7), (6,8), (7,8))\n",
       "vertex_df: org.apache.spark.sql.DataFrame = [id: int, v_desc: int]\n",
       "edge_df: org.apache.spark.sql.DataFrame = [src: int, dst: int]\n",
       "g: org.graphframes.GraphFrame = GraphFrame(v:[id: int, v_desc: int], e:[src: int, dst: int])\n",
       "getAllVertexPairs: (g: org.graphframes.GraphFrame)Array[(Int, Int)]\n",
       "getShortestPath: (g: org.graphframes.GraphFrame, i: Int, j: Int)Array[Int]\n",
       "getAllShortestPaths: (g: org.graphframes.GraphFrame)List[Array[Int]]\n",
       "getBetweenessCentrality: (g: org.graphframes.GraphFrame)org.apache.spark...\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.graphframes._\n",
    "\n",
    "\n",
    "val vertexArray = Array(\n",
    "  (1,1),\n",
    "  (2,2),\n",
    "  (3,3),\n",
    "  (4,4),\n",
    "  (5,5),\n",
    "  (6,6),\n",
    "    (7,7),\n",
    "    (8,8)\n",
    ")\n",
    "\n",
    "\n",
    "val edgeArray = Array(\n",
    "  (1, 3),\n",
    "  (2, 3),\n",
    "  (2, 4),\n",
    "  (4, 5),\n",
    "  (3, 5),\n",
    "  (5, 6),\n",
    "  (6, 7),\n",
    "  (6, 8),\n",
    "  (7, 8)\n",
    ")\n",
    "\n",
    "val vertex_df = spark.createDataFrame(vertexArray).toDF(\"id\",\"v_desc\")\n",
    "val edge_df = spark.createDataFrame(edgeArray).toDF(\"src\",\"dst\")\n",
    "\n",
    "val g = GraphFrame(vertex_df, edge_df)\n",
    "\n",
    "//Function to get all vertex pairs. This is written for you\n",
    "def getAllVertexPairs(g: GraphFrame): Array[(Int,Int)] = {\n",
    "    def getAllPairs(nums: Seq[Int]) =\n",
    "        nums.flatMap(x => nums.map(y => (x,y))).filter(p=>p._1 != p._2)\n",
    "\n",
    "    val col_vals = g.vertices.select(\"id\").map(_.getInt(0)).collect.toSeq.toArray\n",
    "    val all_vertex_pairs = getAllPairs(col_vals).toArray\n",
    "    all_vertex_pairs\n",
    "}\n",
    "\n",
    "//Function to get the shortest path between two vertices. This is also already written\n",
    "//for you\n",
    "//Note that this uses the bfs algorithm. So it will take some time to run and should\n",
    "//not be run on large graphs!\n",
    "\n",
    "//:_* a special notation that tells the compiler to pass each element as its own argument, rather than all of it as a single argument.\n",
    "\n",
    "def getShortestPath(g: GraphFrame,i: Int, j: Int) = {\n",
    "    val path_df = g.bfs.fromExpr(s\"id=$i\").toExpr(s\"id=$j\").run() \n",
    "    if (path_df.count > 0) {\n",
    "        val cols = path_df.columns.filter(n=>n.contains(\"v\")).map(n=>col(n+\".id\"))\n",
    "        val a = path_df.select(cols:_*).rdd.collect()(0).toSeq.toArray.map(e => e.toString.toInt)\n",
    "        a\n",
    "    }\n",
    "    else Array[Int]()\n",
    "}\n",
    "\n",
    "def getAllShortestPaths(g: GraphFrame):List[Array[Int]]  = {\n",
    "    def loop(a: Array[(Int,Int)]):List[Array[Int]] = {\n",
    "        if (a.length == 0) List[Array[Int]]()\n",
    "        else {\n",
    "            val sp = getShortestPath(g,a(0)._1,a(0)._2)\n",
    "            if (a.length == 1)\n",
    "                List(sp)\n",
    "            else sp ::loop(a.slice(1,a.length))\n",
    "        }\n",
    "    }\n",
    "    val all_vertex_pairs = getAllVertexPairs(g)\n",
    "    loop(all_vertex_pairs)\n",
    "}\n",
    "\n",
    "def getBetweenessCentrality(g: GraphFrame) = {\n",
    "    //get all shortest paths removing empty paths\n",
    "    val all_shortest_paths = getAllShortestPaths(g).filter(p=>p.length>0)\n",
    "    val vertices = g.vertices.select(\"id\").rdd.map(v=>v(0).toString.toInt)\n",
    "    val denominator = vertices.count * (vertices.count - 1)\n",
    "    vertices.map(v => all_shortest_paths.flatten.groupBy(identity).mapValues(_.size).getOrElse(v,0)*1.0/denominator)\n",
    "}\n",
    "\n",
    "//Result\n",
    "val b = getBetweenessCentrality(g)\n",
    "b.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd1e7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[Double] = Array(0.0, 0.0, 0.10714285714285714, 0.03571428571428571, 0.21428571428571427, 0.17857142857142858, 0.0, 0.0)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40547414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: List[Int] = List(3, 3, 5, 3, 5, 6, 3, 5, 6, 3, 4, 5, 4, 5, 6, 4, 5, 6, 5, 5, 6, 5, 6, 5, 5, 6, 5, 6, 6, 6)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getAllShortestPaths(g).filter(p=>p.length>0).flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "347f558d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: Array[Int] = Array(1, 1, 3, 2, 1, 4, 4, 4)\n",
       "res6: scala.collection.immutable.Map[Int,Array[Int]] = Map(1 -> Array(1, 1, 1), 3 -> Array(3), 2 -> Array(2), 4 -> Array(4, 4, 4))\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = Array(1,1,3,2,1,4,4,4)\n",
    "x.groupBy(identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1444dd",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc47d5",
   "metadata": {},
   "source": [
    "# GraphX, case class, aggregateMessages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9559784f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n",
       "defined class Demographics\n",
       "defined class Person\n",
       "defined class Connection\n",
       "users: Array[(Long, Person)] = Array((1,Person(Alice,Demographics(28,F,150000.0))), (2,Person(Bob,Demographics(27,M,50000.0))), (3,Person(Charlie,Demographics(65,M,250000.0))), (4,Person(David,Demographics(42,M,750000.0))), (5,Person(Ed,Demographics(55,M,25000.0))), (6,Person(Fran,Demographics(50,F,3150000.0))), (7,Person(Jack,Demographics(17,M,5000.0))), (8,Person(Jill,Demographics(16,F,1000.0))))\n",
       "connections: Array[org.apache.spark.graphx.Edge[Connection]] = Array(Edge(2,1,Connection(7,0.2)), Edge(2,4,Connection(2,0.7)), Edge(3,2,Connection(4,0.31)), Edge(3,6,Connection(3,0.22)), Edge(4,1,Connection(1,0.12)), Edge(5,2,Connection(2,0.45)), Edge(5,3,C...\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx._ \n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "case class Demographics(age: Int,gender: Char, income: Double) \n",
    "case class Person(name: String,demographics: Demographics) \n",
    "case class Connection(strength: Int,msgProbability: Double)\n",
    "\n",
    "val users = Array(\n",
    "(1L, Person(\"Alice\",Demographics(28,'F',150000.0))),\n",
    "(2L, Person(\"Bob\",Demographics(27,'M',50000.0))),\n",
    "(3L, Person(\"Charlie\",Demographics(65,'M',250000.0))),\n",
    "(4L, Person(\"David\",Demographics(42,'M',750000.0))),\n",
    "(5L, Person(\"Ed\",Demographics(55,'M',25000.0))),\n",
    "(6L, Person(\"Fran\",Demographics(50,'F',3150000.0))),\n",
    "(7L, Person(\"Jack\",Demographics(17,'M',5000.0))),\n",
    "(8L, Person(\"Jill\",Demographics(16,'F',1000.0)))\n",
    ")\n",
    "\n",
    "val connections = Array(\n",
    "Edge(2L, 1L, Connection(7,.2)),\n",
    "Edge(2L, 4L, Connection(2,.7)),\n",
    "Edge(3L, 2L, Connection(4,.31)),\n",
    "Edge(3L, 6L, Connection(3,.22)),\n",
    "Edge(4L, 1L, Connection(1,.12)),\n",
    "Edge(5L, 2L, Connection(2,.45)),\n",
    "Edge(5L, 3L, Connection(8,.91))\n",
    ")\n",
    "\n",
    "val vertexRDD: RDD[(Long, Person)] = sc.parallelize(users) \n",
    "val edgeRDD: RDD[Edge[Connection]] = sc.parallelize(connections)\n",
    "\n",
    "val social_graph = Graph(vertexRDD,edgeRDD)\n",
    "\n",
    "val over21_friends = social_graph.aggregateMessages[Int](\n",
    "    triplet => {\n",
    "        if (triplet.dstAttr.demographics.age >= 21)\n",
    "        triplet.sendToSrc(1)\n",
    "    },\n",
    "    (a, b) => (a+b)).map(t =>t._1).collect\n",
    "\n",
    "val result = social_graph.aggregateMessages[Int](\n",
    "triplet => {\n",
    "    if (triplet.srcAttr.demographics.age > 21\n",
    "       && triplet.srcAttr.demographics.income > 20000\n",
    "       && triplet.attr.strength >= 3\n",
    "       && (over21_friends contains triplet.srcId) )\n",
    "    triplet.sendToDst(1)\n",
    "}, (a, b) => (a+b)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6642c212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((1,1), (2,1), (3,1), (6,1))\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925e206f",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23875d9",
   "metadata": {},
   "source": [
    "# Pregel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6805e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.149:4041\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1671401582401)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n",
       "defined class PersonData\n",
       "defined class Client\n",
       "defined class Relationship\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "case class PersonData(age: Int,gender: Char, income: Double)\n",
    "case class Client(name: String,data: PersonData)\n",
    "case class Relationship(strength: Int,msgProbability: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb565b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people: Array[(Long, Client)] = Array((1,Client(Alice,PersonData(28,F,150000.0))), (2,Client(Bob,PersonData(27,M,50000.0))), (3,Client(Charlie,PersonData(65,M,250000.0))), (4,Client(David,PersonData(42,M,750000.0))), (5,Client(Ed,PersonData(55,M,25000.0))), (6,Client(Fran,PersonData(50,F,3150000.0))), (7,Client(Jack,PersonData(17,M,5000.0))), (8,Client(Jill,PersonData(16,F,1000.0))))\n",
       "relationships: Array[org.apache.spark.graphx.Edge[Relationship]] = Array(Edge(2,1,Relationship(7,0.2)), Edge(2,4,Relationship(2,0.7)), Edge(3,2,Relationship(4,0.31)), Edge(3,6,Relationship(3,0.22)), Edge(4,1,Relationship(1,0.12)), Edge(5,2,Relationship(2,0.45)), Edge(5,3,Relationship(8,0.91)))\n",
       "vertexRDD: org.apache.spark.rdd.RDD[(Long, Client)] = ParallelCollectionRDD[0] at parallelize at <console>:55\n",
       "edgeR...\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val people = Array(\n",
    "  (1L, Client(\"Alice\",PersonData(28,'F',150000.0))),\n",
    "  (2L, Client(\"Bob\",PersonData(27,'M',50000.0))),\n",
    "  (3L, Client(\"Charlie\",PersonData(65,'M',250000.0))),\n",
    "  (4L, Client(\"David\",PersonData(42,'M',750000.0))),\n",
    "  (5L, Client(\"Ed\",PersonData(55,'M',25000.0))),\n",
    "  (6L, Client(\"Fran\",PersonData(50,'F',3150000.0))),\n",
    "  (7L, Client(\"Jack\",PersonData(17,'M',5000.0))),\n",
    "  (8L, Client(\"Jill\",PersonData(16,'F',1000.0)))\n",
    ")\n",
    "\n",
    "val relationships = Array(\n",
    "  Edge(2L, 1L, Relationship(7,.2)),\n",
    "  Edge(2L, 4L, Relationship(2,.7)),\n",
    "  Edge(3L, 2L, Relationship(4,.31)),\n",
    "  Edge(3L, 6L, Relationship(3,.22)),\n",
    "  Edge(4L, 1L, Relationship(1,.12)),\n",
    "  Edge(5L, 2L, Relationship(2,.45)),\n",
    "  Edge(5L, 3L, Relationship(8,.91))\n",
    ")\n",
    "\n",
    "val vertexRDD: RDD[(Long, Client)] = sc.parallelize(people)\n",
    "val edgeRDD: RDD[Edge[Relationship]] = sc.parallelize(relationships)\n",
    "\n",
    "val social_graph = Graph(vertexRDD,edgeRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ede8ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_max_path: (social_graph: org.apache.spark.graphx.Graph[Client,Relationship], sourceId: org.apache.spark.graphx.VertexId)org.apache.spark.graphx.VertexRDD[Double]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Fill in the types for social_graph and sourceId\n",
    "def get_max_path(social_graph: Graph[Client,Relationship] ,sourceId: VertexId) = {\n",
    "    val initialGraph = social_graph.mapVertices((id,_) => if (id == sourceId) 1.0 else 0.0)\n",
    "    val vertexProgram = (id: VertexId, prob: Double, newProb: Double) => math.max(prob, newProb)\n",
    "    val sendMsg = (triplet:EdgeTriplet[Double,Relationship]) => {\n",
    "        val edgeProb = triplet.attr.msgProbability\n",
    "        if (triplet.srcAttr == 0.0) {\n",
    "            Iterator.empty\n",
    "        } else if (triplet.srcAttr*edgeProb > triplet.dstAttr) {\n",
    "            Iterator((triplet.dstId,triplet.srcAttr*edgeProb))\n",
    "        } else {\n",
    "            Iterator.empty\n",
    "        }\n",
    "    }\n",
    "    val mrgMsg = (a:Double, b:Double) => math.max(a, b)\n",
    "    //Add the arguments for pregel\n",
    "    val maxPath = initialGraph.pregel(0.0,3)(vertexProgram,sendMsg,mrgMsg)\n",
    "    //return the vertices\n",
    "    maxPath.vertices\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c75326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(org.apache.spark.graphx.VertexId, Double)] = Array((8,0.0), (1,0.09000000000000001), (2,0.45), (3,0.91), (4,0.315), (5,1.0), (6,0.20020000000000002), (7,0.0))\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_max_path(social_graph,5).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a227fb",
   "metadata": {},
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db6517",
   "metadata": {},
   "source": [
    "# dataframe, ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e468d46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [user_id: int, movie_id: int ... 1 more field]\n",
       "train_df: org.apache.spark.sql.DataFrame = [user_id: int, movie_id: int ... 1 more field]\n",
       "test_df: org.apache.spark.sql.DataFrame = [user_id: int, movie_id: int ... 1 more field]\n",
       "avg_df: org.apache.spark.sql.DataFrame = [movie_id: int, avg(rating): double]\n",
       "new_df: org.apache.spark.sql.DataFrame = [movie_id: int, user_id: int ... 2 more fields]\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_fae5d8f0b3c6, metricName=rmse, throughOrigin=false\n",
       "rmse: Double = 0.39127386584748797\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.createDataFrame(Seq(\n",
    "(1,1,3.2),\n",
    "(1,2,4.3),\n",
    "(2,4,1.9),\n",
    "(2,2,3.3),\n",
    "(2,1,4.1),\n",
    "(3,15,4.5),\n",
    "(3,2,4.3)))\n",
    ".toDF(\"user_id\",\"movie_id\",\"rating\")\n",
    "val train_df = df\n",
    "val test_df = df\n",
    "val avg_df = train_df.groupBy(\"movie_id\").avg(\"rating\")\n",
    "val new_df = test_df.join(avg_df, Seq(\"movie_id\")).withColumnRenamed(\"avg(rating)\",\"prediction\")\n",
    "\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "val evaluator = new RegressionEvaluator()\n",
    ".setMetricName(\"rmse\")\n",
    ".setLabelCol(\"rating\")\n",
    ".setPredictionCol(\"prediction\")\n",
    "\n",
    "val rmse = evaluator.evaluate(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f903d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+------------------+\n",
      "|movie_id|user_id|rating|        prediction|\n",
      "+--------+-------+------+------------------+\n",
      "|       1|      2|   4.1|              3.65|\n",
      "|       1|      1|   3.2|              3.65|\n",
      "|       2|      3|   4.3|3.9666666666666663|\n",
      "|       2|      2|   3.3|3.9666666666666663|\n",
      "|       2|      1|   4.3|3.9666666666666663|\n",
      "|       4|      2|   1.9|               1.9|\n",
      "|      15|      3|   4.5|               4.5|\n",
      "+--------+-------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c6d747",
   "metadata": {},
   "source": [
    "# Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2533538",
   "metadata": {},
   "source": [
    "# udf, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f210422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[24309] at makeRDD at <console>:40\n",
       "df: org.apache.spark.sql.DataFrame = [name: string, reviews: struct<count: bigint, rating: double> ... 1 more field]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataRDD = spark.sparkContext.makeRDD(\n",
    "\"\"\"[{\"name\":\"Le Monde\",\"reviews\":{\"count\":14,\"rating\":3.2},\"serves\":{\"alcohol\":true,\"vegetarian\":false}} ,\n",
    "{\"name\":\"Junzi Kitchen\",\"reviews\":{\"count\":7,\"rating\":4.5},\"serves\":{\"alcohol\":false,\"vegetarian\":true}},\n",
    "{\"name\":\"Atlas Kitchen\",\"reviews\":{\"count\":9,\"rating\":2.9},\"serves\":{\"alcohol\":true,\"vegetarian\":true}}]\"\"\":: Nil)\n",
    "val df = spark.read.json(dataRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d5575ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Array[String] =\n",
       "Array([{\"name\":\"Le Monde\",\"reviews\":{\"count\":14,\"rating\":3.2},\"serves\":{\"alcohol\":true,\"vegetarian\":false}} ,\n",
       "{\"name\":\"Junzi Kitchen\",\"reviews\":{\"count\":7,\"rating\":4.5},\"serves\":{\"alcohol\":false,\"vegetarian\":true}},\n",
       "{\"name\":\"Atlas Kitchen\",\"reviews\":{\"count\":9,\"rating\":2.9},\"serves\":{\"alcohol\":true,\"vegetarian\":true}}])\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRDD.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e66b0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------------+\n",
      "|         name|  reviews|       serves|\n",
      "+-------------+---------+-------------+\n",
      "|     Le Monde|{14, 3.2}|{true, false}|\n",
      "|Junzi Kitchen| {7, 4.5}|{false, true}|\n",
      "|Atlas Kitchen| {9, 2.9}| {true, true}|\n",
      "+-------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7859947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.udf\n",
       "score: (alc: Boolean, vg: Boolean, ra: Double)Double\n",
       "score_udf: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$5874/0x0000000801c31840@3c23ca66,DoubleType,List(Some(class[value[0]: boolean]), Some(class[value[0]: boolean]), Some(class[value[0]: double])),Some(class[value[0]: double]),None,false,true)\n",
       "df2: org.apache.spark.sql.DataFrame = [name: string, reviews: struct<count: bigint, rating: double> ... 2 more fields]\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "def score(alc:Boolean, vg: Boolean, ra: Double) = {\n",
    "    var score = 0.0\n",
    "    if (alc) score =1\n",
    "    if (vg) score=score+1\n",
    "    score+ra/2.0\n",
    "}\n",
    "val score_udf = udf(score _)\n",
    "val df2 = df.withColumn(\"score\",score_udf($\"serves.alcohol\",$\"serves.vegetarian\",$\"reviews.rating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac25aaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------------+-----+\n",
      "|         name|  reviews|       serves|score|\n",
      "+-------------+---------+-------------+-----+\n",
      "|     Le Monde|{14, 3.2}|{true, false}|  2.6|\n",
      "|Junzi Kitchen| {7, 4.5}|{false, true}| 3.25|\n",
      "|Atlas Kitchen| {9, 2.9}| {true, true}| 3.45|\n",
      "+-------------+---------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304707c9",
   "metadata": {},
   "source": [
    "# Problem 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea9d644",
   "metadata": {},
   "source": [
    "# read, fit & transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10a64c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/15 17:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:01:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+---+---+---+----+\n",
      "| c1| c2| c3|  ma|\n",
      "+---+---+---+----+\n",
      "|  1|  2|  3|null|\n",
      "|  2|  3|  4| 1.0|\n",
      "|  3|  4|  5| 1.5|\n",
      "|  4|  5|  6| 2.0|\n",
      "|  5|  6|  7| 3.0|\n",
      "|  6|  7|  8| 4.0|\n",
      "|  7|  8|  9| 5.0|\n",
      "|  9| 10| 11| 6.0|\n",
      "+---+---+---+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "df: org.apache.spark.sql.DataFrame = [c1: int, c2: int ... 1 more field]\n",
       "df_ma: org.apache.spark.sql.DataFrame = [c1: int, c2: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "val df = sc.parallelize(Array((1,2,3),(2,3,4),(3,4,5),\n",
    "                              (4,5,6),(5,6,7),(6,7,8),\n",
    "                             (7,8,9),(9,10,11))).toDF(\"c1\",\"c2\",\"c3\")\n",
    "//For row 5, value in df_ma will be the average of rows 2,3,4.                            \n",
    "val df_ma = df.withColumn(\"ma\",avg(df(\"c1\")).over(Window.orderBy(\"c2\").rowsBetween(-3,-1)))\n",
    "df_ma.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d296c7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [date: timestamp, price: double]\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\",\"true\").option(\"inferschema\",\"true\").csv(\"AAPL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b5a9ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/15 17:10:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:10:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:10:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:10:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:10:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:10:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:10:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:10:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:10:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/15 17:10:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+-------------------+--------+-------------------+-------------------+----------------------+-----+-------+\n",
      "|date               |price   |ma8                |ma13               |diff                  |label|deciles|\n",
      "+-------------------+--------+-------------------+-------------------+----------------------+-----+-------+\n",
      "|1980-12-12 00:00:00|0.1006  |0.1006             |0.1006             |0.0                   |0.0  |4.0    |\n",
      "|1980-12-15 00:00:00|0.095352|0.09797600000000001|0.09797600000000001|0.0                   |0.0  |4.0    |\n",
      "|1980-12-16 00:00:00|0.088353|0.09476833333333334|0.09476833333333334|0.0                   |0.0  |4.0    |\n",
      "|1980-12-17 00:00:00|0.09054 |0.09371125000000001|0.09371125000000001|0.0                   |0.0  |4.0    |\n",
      "|1980-12-18 00:00:00|0.093165|0.093602           |0.093602           |0.0                   |0.0  |4.0    |\n",
      "|1980-12-19 00:00:00|0.098851|0.09447683333333334|0.09447683333333334|0.0                   |0.0  |4.0    |\n",
      "|1980-12-22 00:00:00|0.103662|0.09578900000000001|0.09578900000000001|0.0                   |0.0  |4.0    |\n",
      "|1980-12-23 00:00:00|0.108036|0.09731987500000001|0.09731987500000001|0.0                   |0.0  |4.0    |\n",
      "|1980-12-24 00:00:00|0.113722|0.09896012500000001|0.09914233333333335|-1.822083333333363E-4 |0.0  |4.0    |\n",
      "|1980-12-26 00:00:00|0.124219|0.1025685          |0.10165000000000002|9.184999999999888E-4  |1.0  |5.0    |\n",
      "|1980-12-29 00:00:00|0.125969|0.1072705          |0.10386081818181819|0.0034096818181818117 |1.0  |6.0    |\n",
      "|1980-12-30 00:00:00|0.122907|0.111316375        |0.10544800000000003|0.005868374999999967  |1.0  |6.0    |\n",
      "|1980-12-31 00:00:00|0.119408|0.11459674999999998|0.10652184615384618|0.008074903846153808  |1.0  |6.0    |\n",
      "|1981-01-02 00:00:00|0.12072 |0.11733037499999999|0.10806953846153847|0.00926083653846152   |1.0  |7.0    |\n",
      "|1981-01-05 00:00:00|0.118096|0.119134625        |0.10981907692307692|0.009315548076923072  |1.0  |7.0    |\n",
      "|1981-01-06 00:00:00|0.112847|0.119736           |0.11170323076923075|0.008032769230769246  |1.0  |6.0    |\n",
      "|1981-01-07 00:00:00|0.108036|0.11902525         |0.1130490769230769 |0.005976173076923094  |1.0  |6.0    |\n",
      "|1981-01-08 00:00:00|0.105849|0.116729           |0.11402476923076922|0.002704230769230778  |1.0  |5.0    |\n",
      "|1981-01-09 00:00:00|0.111535|0.11492474999999999|0.11500046153846152|-7.571153846153145E-5 |0.0  |4.0    |\n",
      "|1981-01-12 00:00:00|0.11066 |0.113393875        |0.11553876923076922|-0.0021448942307692176|0.0  |3.0    |\n",
      "|1981-01-13 00:00:00|0.106724|0.11180837499999999|0.11543784615384614|-0.0036294711538461533|0.0  |2.0    |\n",
      "|1981-01-14 00:00:00|0.107161|0.1101135          |0.11493315384615385|-0.0048196538461538485|0.0  |2.0    |\n",
      "|1981-01-15 00:00:00|0.109348|0.10902            |0.11378923076923077|-0.004769230769230762 |0.0  |2.0    |\n",
      "|1981-01-16 00:00:00|0.108473|0.10847325         |0.11244338461538463|-0.003970134615384632 |0.0  |2.0    |\n",
      "|1981-01-19 00:00:00|0.115034|0.109348           |0.11183776923076925|-0.0024897692307692537|0.0  |3.0    |\n",
      "|1981-01-20 00:00:00|0.111535|0.11005875000000001|0.11123215384615384|-0.001173403846153831 |0.0  |3.0    |\n",
      "|1981-01-21 00:00:00|0.113722|0.11033212499999999|0.11069384615384617|-3.617211538461812E-4 |0.0  |4.0    |\n",
      "|1981-01-22 00:00:00|0.115034|0.11087887499999999|0.11045830769230772|4.2056730769227113E-4 |1.0  |4.0    |\n",
      "|1981-01-23 00:00:00|0.114596|0.11186287499999999|0.11059284615384615|0.001270028846153834  |1.0  |5.0    |\n",
      "|1981-01-26 00:00:00|0.112847|0.11257362500000001|0.11096292307692307|0.0016107019230769404 |1.0  |5.0    |\n",
      "|1981-01-27 00:00:00|0.111972|0.112901625        |0.11143392307692307|0.0014677019230769361 |1.0  |5.0    |\n",
      "|1981-01-28 00:00:00|0.108473|0.112901625        |0.11119838461538459|0.0017032403846154176 |1.0  |5.0    |\n",
      "|1981-01-29 00:00:00|0.104537|0.11158950000000001|0.1107273846153846 |8.621153846154056E-4  |1.0  |5.0    |\n",
      "|1981-01-30 00:00:00|0.098851|0.110004           |0.11012176923076923|-1.1776923076922685E-4|0.0  |4.0    |\n",
      "|1981-02-02 00:00:00|0.093165|0.107434375        |0.10904515384615386|-0.0016107788461538625|0.0  |3.0    |\n",
      "|1981-02-03 00:00:00|0.096664|0.10513812500000001|0.10806946153846156|-0.002931336538461546 |0.0  |3.0    |\n",
      "|1981-02-04 00:00:00|0.100163|0.103334           |0.10743023076923078|-0.0040962307692307826|0.0  |2.0    |\n",
      "|1981-02-05 00:00:00|0.100163|0.1017485          |0.10628630769230771|-0.004537807692307702 |0.0  |2.0    |\n",
      "|1981-02-06 00:00:00|0.1006  |0.100327           |0.10544515384615386|-0.005118153846153856 |0.0  |2.0    |\n",
      "|1981-02-09 00:00:00|0.095352|0.09868687500000001|0.10403207692307694|-0.005345201923076928 |0.0  |2.0    |\n",
      "|1981-02-10 00:00:00|0.095352|0.09753875         |0.10251807692307696|-0.004979326923076968 |0.0  |2.0    |\n",
      "|1981-02-11 00:00:00|0.09229 |0.096718625        |0.10080223076923077|-0.0040836057692307665|0.0  |2.0    |\n",
      "|1981-02-12 00:00:00|0.091415|0.096499875        |0.0991536153846154 |-0.0026537403846153967|0.0  |3.0    |\n",
      "|1981-02-13 00:00:00|0.089228|0.095570375        |0.09740407692307693|-0.0018337019230769275|0.0  |3.0    |\n",
      "|1981-02-17 00:00:00|0.091415|0.094476875        |0.09609192307692307|-0.001615048076923073 |0.0  |3.0    |\n",
      "|1981-02-18 00:00:00|0.095352|0.0938755          |0.09538538461538462|-0.0015098846153846213|0.0  |3.0    |\n",
      "|1981-02-19 00:00:00|0.089665|0.092508625        |0.09467876923076925|-0.0021701442307692498|0.0  |3.0    |\n",
      "|1981-02-20 00:00:00|0.084854|0.091196375        |0.09403946153846156|-0.0028430865384615617|0.0  |3.0    |\n",
      "|1981-02-23 00:00:00|0.086166|0.09004812499999999|0.09323192307692309|-0.0031837980769230945|0.0  |2.0    |\n",
      "|1981-02-24 00:00:00|0.083105|0.08889999999999999|0.09191976923076924|-0.0030197692307692425|0.0  |2.0    |\n",
      "|1981-02-25 00:00:00|0.088353|0.08851724999999999|0.09101130769230768|-0.0024940576923076913|0.0  |3.0    |\n",
      "|1981-02-26 00:00:00|0.089665|0.08857187500000001|0.09017015384615384|-0.001598278846153836 |0.0  |3.0    |\n",
      "|1981-02-27 00:00:00|0.092727|0.08873587499999999|0.08996823076923076|-0.0012323557692307668|0.0  |3.0    |\n",
      "|1981-03-02 00:00:00|0.093165|0.0884625          |0.08979999999999998|-0.0013374999999999776|0.0  |3.0    |\n",
      "|1981-03-03 00:00:00|0.091853|0.08873599999999998|0.08976638461538461|-0.0010303846153846274|0.0  |3.0    |\n",
      "|1981-03-04 00:00:00|0.090977|0.089501375        |0.0897326923076923 |-2.3131730769231085E-4|0.0  |4.0    |\n",
      "|1981-03-05 00:00:00|0.09054 |0.09004812499999999|0.0898336153846154 |2.1450961538459212E-4 |1.0  |4.0    |\n",
      "|1981-03-06 00:00:00|0.089665|0.090868125        |0.08969899999999997|0.0011691250000000208 |1.0  |5.0    |\n",
      "|1981-03-09 00:00:00|0.082667|0.090157375        |0.08872323076923078|0.0014341442307692215 |1.0  |5.0    |\n",
      "|1981-03-10 00:00:00|0.07873 |0.0887905          |0.0878820769230769 |9.084230769230983E-4  |1.0  |5.0    |\n",
      "|1981-03-11 00:00:00|0.075669|0.08665824999999999|0.08717553846153846|-5.172884615384649E-4 |0.0  |4.0    |\n",
      "|1981-03-12 00:00:00|0.07873 |0.084853875        |0.08660353846153845|-0.0017496634615384588|0.0  |3.0    |\n",
      "|1981-03-13 00:00:00|0.077856|0.08310424999999999|0.08619976923076922|-0.003095519230769228 |0.0  |2.0    |\n",
      "|1981-03-16 00:00:00|0.080918|0.081846875        |0.08562784615384615|-0.003780971153846152 |0.0  |2.0    |\n",
      "|1981-03-17 00:00:00|0.084854|0.08113612499999999|0.08525776923076922|-0.004121644230769231 |0.0  |2.0    |\n",
      "|1981-03-18 00:00:00|0.090103|0.081190875        |0.08505592307692308|-0.003865048076923089 |0.0  |2.0    |\n",
      "|1981-03-19 00:00:00|0.089228|0.082011           |0.08475307692307693|-0.0027420769230769304|0.0  |3.0    |\n",
      "|1981-03-20 00:00:00|0.090103|0.083432625        |0.08461846153846155|-0.0011858365384615488|0.0  |3.0    |\n",
      "|1981-03-23 00:00:00|0.093602|0.08567425         |0.08482038461538462|8.538653846153765E-4  |1.0  |5.0    |\n",
      "|1981-03-24 00:00:00|0.093165|0.087478625        |0.08502230769230769|0.0024563173076923156 |1.0  |5.0    |\n",
      "|1981-03-25 00:00:00|0.091415|0.08917350000000002|0.08515692307692307|0.004016576923076942  |1.0  |6.0    |\n",
      "|1981-03-26 00:00:00|0.089665|0.09026687500000001|0.08569523076923076|0.004571644230769251  |1.0  |6.0    |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|1981-03-27 00:00:00|0.086604|0.090485625        |0.08630092307692308|0.0041847019230769195 |1.0  |6.0    |\n",
      "|1981-03-30 00:00:00|0.086604|0.09004825000000001|0.08714207692307692|0.002906173076923091  |1.0  |5.0    |\n",
      "|1981-03-31 00:00:00|0.085729|0.089610875        |0.08768046153846153|0.00193041346153848   |1.0  |5.0    |\n",
      "|1981-04-01 00:00:00|0.084854|0.08895475         |0.08821876923076924|7.359807692307596E-4  |1.0  |4.0    |\n",
      "|1981-04-02 00:00:00|0.09229 |0.08879075         |0.08909353846153847|-3.027884615384724E-4 |0.0  |4.0    |\n",
      "|1981-04-03 00:00:00|0.092727|0.088736           |0.08969915384615385|-9.631538461538497E-4 |0.0  |3.0    |\n",
      "|1981-04-06 00:00:00|0.090977|0.08868125         |0.08976638461538464|-0.0010851346153846336|0.0  |3.0    |\n",
      "|1981-04-07 00:00:00|0.090103|0.088736           |0.08983369230769232|-0.0010976923076923273|0.0  |3.0    |\n",
      "|1981-04-08 00:00:00|0.094477|0.08972012500000001|0.09017015384615383|-4.500288461538188E-4 |0.0  |4.0    |\n",
      "|1981-04-09 00:00:00|0.096226|0.09092287500000001|0.090372           |5.508750000000201E-4  |1.0  |4.0    |\n",
      "|1981-04-10 00:00:00|0.097539|0.092399125        |0.09070846153846154|0.0016906634615384553 |1.0  |5.0    |\n",
      "|1981-04-13 00:00:00|0.097539|0.09398475         |0.09117953846153846|0.002805211538461541  |1.0  |5.0    |\n",
      "|1981-04-14 00:00:00|0.097539|0.09464087500000003|0.09178523076923077|0.0028556442307692553 |1.0  |5.0    |\n",
      "|1981-04-15 00:00:00|0.092727|0.09464087500000001|0.09225623076923078|0.0023846442307692284 |1.0  |5.0    |\n",
      "|1981-04-16 00:00:00|0.087478|0.0942035          |0.09232346153846155|0.001880038461538447  |1.0  |5.0    |\n",
      "|1981-04-20 00:00:00|0.090103|0.0942035          |0.09265992307692308|0.0015435769230769114 |1.0  |5.0    |\n",
      "|1981-04-21 00:00:00|0.096226|0.09442212500000001|0.0935346923076923 |8.874326923077047E-4  |1.0  |5.0    |\n",
      "|1981-04-22 00:00:00|0.099725|0.0948595          |0.09410661538461541|7.528846153845858E-4  |1.0  |4.0    |\n",
      "|1981-04-23 00:00:00|0.10235 |0.095460875        |0.09484684615384616|6.14028846153844E-4   |1.0  |4.0    |\n",
      "|1981-04-24 00:00:00|0.101475|0.09595287499999999|0.09565438461538461|2.984903846153797E-4  |1.0  |4.0    |\n",
      "|1981-04-27 00:00:00|0.1006  |0.0963355          |0.09646184615384615|-1.2634615384614356E-4|0.0  |4.0    |\n",
      "|1981-04-28 00:00:00|0.098851|0.097101           |0.0967983076923077 |3.0269230769230937E-4 |1.0  |4.0    |\n",
      "|1981-04-29 00:00:00|0.097539|0.098358625        |0.0968993076923077 |0.0014593173076923038 |1.0  |5.0    |\n",
      "|1981-04-30 00:00:00|0.099288|0.09950675         |0.09703384615384616|0.00247290384615384   |1.0  |5.0    |\n",
      "|1981-05-01 00:00:00|0.099288|0.09988950000000002|0.09716838461538463|0.0027211153846153913 |1.0  |5.0    |\n",
      "|1981-05-04 00:00:00|0.098851|0.09978025000000001|0.0972693076923077 |0.002510942307692318  |1.0  |5.0    |\n",
      "|1981-05-05 00:00:00|0.098413|0.099288125        |0.09770669230769233|0.0015814326923076771 |1.0  |5.0    |\n",
      "|1981-05-06 00:00:00|0.095789|0.098577375        |0.09834600000000002|2.3137499999997813E-4 |1.0  |4.0    |\n",
      "+-------------------+--------+-------------------+-------------------+----------------------+-----+-------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.functions.udf\n",
       "df: org.apache.spark.sql.DataFrame = [date: timestamp, price: double]\n",
       "set_label: (v: Double)Double\n",
       "label_udf: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$6407/0x0000000801ba5040@60c61238,DoubleType,List(Some(class[value[0]: double])),Some(class[value[0]: double]),None,false,true)\n",
       "df_new: org.apache.spark.sql.DataFrame = [date: timestamp, price: double ... 4 more fields]\n",
       "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
       "discretizer: org.apache.spark.ml.feature.QuantileDiscretizer = quantileDiscretizer_5aa5150f166b\n",
       "result: org.apache.spark.sql.DataFrame = [date: timestamp, price: double ... 5 more fields]\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "val df = spark.read.option(\"header\",\"true\").option(\"inferschema\",\"true\").csv(\"AAPL.csv\")\n",
    "def set_label(v: Double):Double = if (v>0) 1 else 0\n",
    "val label_udf = udf(set_label _)\n",
    "val df_new = df.withColumn(\"ma8\", avg(df(\"price\")).over(Window.orderBy(\"date\").rowsBetween(-7,0)))\n",
    "    .withColumn(\"ma13\", avg(df(\"price\")).over(Window.orderBy(\"date\").rowsBetween(-12,0)))\n",
    "    .withColumn(\"diff\", $\"ma8\" - $\"ma13\")\n",
    "    .withColumn(\"label\", label_udf($\"diff\"))\n",
    "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
    "\n",
    "val discretizer = new QuantileDiscretizer()\n",
    "    .setInputCol(\"diff\")\n",
    "    .setOutputCol(\"deciles\")\n",
    "    .setNumBuckets(10)\n",
    "\n",
    "val result = discretizer.fit(df_new).transform(df_new)\n",
    "result.show(100,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27d90d3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: illegal start of simple expression",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: illegal start of simple expression",
      "       values = [(0.1,), (0.4,), (1.2,), (1.5,), (float(\"nan\"),), (float(\"nan\"),)]",
      "                ^",
      "<console>:3: error: illegal start of simple expression",
      "       df1 = spark.createDataFrame(values, [\"values\"])",
      "                                           ^",
      ""
     ]
    }
   ],
   "source": [
    "values = [(0.1,), (0.4,), (1.2,), (1.5,), (float(\"nan\"),), (float(\"nan\"),)]\n",
    "df1 = spark.createDataFrame(values, [\"values\"])\n",
    "qds1 = QuantileDiscretizer(inputCol=\"values\", outputCol=\"buckets\")\n",
    "qds1.setNumBuckets(2)\n",
    "\n",
    "qds1.setRelativeError(0.01)\n",
    "\n",
    "qds1.setHandleInvalid(\"error\")\n",
    "\n",
    "qds1.getRelativeError()\n",
    "\n",
    "bucketizer = qds1.fit(df1)\n",
    "qds1.setHandleInvalid(\"keep\").fit(df1).transform(df1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11b2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
