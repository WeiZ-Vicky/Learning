{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">Windowing with DStreams</span>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.log4j.Logger\n",
    "import org.apache.log4j.Level\n",
    "\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "Logger.getLogger(\"akka\").setLevel(Level.OFF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Data distributions may change over time (non-stationary)</li>\n",
    "<li>We might want to overweight recent data and underweight older data</li>\n",
    "<li>Examples:\n",
    "<ul><li>Monitoring frequency of errors in log files\n",
    "<li>A high frequency market making app that uses data from short time windows\n",
    "<li>Monitoring passenger flows into a railway station \n",
    "</ul>\n",
    "    <li>Spark Streaming allows the creation of sliding windows (and, therefore, also fixed windows)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Windows in Spark</span>\n",
    "<br><br>\n",
    "<li>Spark DStream implements sliding windows</li>\n",
    "<li>Each window has a length (for example, 20 seconds)</li>\n",
    "<li>Each window has a \"sliding interval\" (for example, the window is created every 10 seconds)</li>\n",
    "<li>Window length is a multiple of microbatch size</li>\n",
    "<li>sliding interval is a multiple of microbatch size</li>\n",
    "<li>Both can be, at a minimum, the batch size</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">windows, slide intervals, and microbatches</span>\n",
    "<img src=\"streaming_windows.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Window transformations</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li><b>Window(window_length,slide_interval)</b>: Creates a window</li>\n",
    "<li><b>countByWindow(window_length,slide_interval)</b>: returns the number of elements in a window</li>\n",
    "<li><b>countByValueAndWindow(window_length,slide_interval)</b>: Counts the number for each key</li>\n",
    "<li><b>reduceByWindow(function,window_length,slide_interval)</b>: Applies reduce, using the specified function, on the data in the DStreams in the window</li>\n",
    "<li><b>reduceByKeyAndWindow</b>: The key version of reduceByWindow.  </li>\n",
    "<li>Windowed operations require a checkpoint</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>reduceByKeyAndWindow(reduce_function,inverse_function,window_duration,slide_duration)</b>\n",
    "<ul>\n",
    "<li><b>reduce_function</b>: The ordinary reduce function (in our example, add)\n",
    "<li><b>inverse_function</b>: What to do with data that slides out of the window\n",
    "<li><b>window_duration</b>: window size (multiple of batch interval\n",
    "<li><b>slide_duration</b>: the amount the window will slide by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Example: countByWindow</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Microbatch size: 10 seconds</li>\n",
    "<li>Window length: 30 seconds</li>\n",
    "<li>Slide interval: 20 seconds</li>\n",
    "<li>Report the total number of words in each microbatch and in each window</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@490f1dd2\n",
       "lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@74eb4bd8\n",
       "words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@53486503\n",
       "pairs: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.MappedDStream@4faf3938\n",
       "wordCounts: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.ShuffledDStream@7e7f4c92\n",
       "wcount: org.apache.spark.streaming.dstream.DStream[Long] = org.apache.spark.streaming.dstream.MappedDStream@50e46d0e\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "val words = lines.flatMap(l => l.split(\" \"))\n",
    "\n",
    "//Total words in the microbatch (10 seconds)\n",
    "//This will print every 10 seconds\n",
    "words.count.print\n",
    "val pairs = words.map(word => (word, 1))\n",
    "val wordCounts = pairs.reduceByKey((x, y) => x + y)\n",
    "wordCounts.print()\n",
    "\n",
    "//Total number of words in every 30 second window sliding every 20 seconds\n",
    "//The print is at each point that the window slides (20, 40, 60 ,80,....)\n",
    "//This will print at 20 (total words in 20 seconds since this is the first slide)\n",
    "//at 40 seconds (total words in the last 3 10 second intervals)\n",
    "//at 60 seconds, etc.\n",
    "val wcount = words.countByWindow(Seconds(30),Seconds(20))\n",
    "wcount.print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1669757220000 ms\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669757220000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "22/11/29 16:27:01 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/11/29 16:27:01 WARN BlockManager: Block input-0-1669757220800 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/11/29 16:27:02 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/11/29 16:27:02 WARN BlockManager: Block input-0-1669757221800 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/11/29 16:27:02 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/11/29 16:27:02 WARN BlockManager: Block input-0-1669757222400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/11/29 16:27:03 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/11/29 16:27:03 WARN BlockManager: Block input-0-1669757223200 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/11/29 16:27:04 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/11/29 16:27:04 WARN BlockManager: Block input-0-1669757224000 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/11/29 16:27:05 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/11/29 16:27:05 WARN BlockManager: Block input-0-1669757224800 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/11/29 16:27:05 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/11/29 16:27:05 WARN BlockManager: Block input-0-1669757225600 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1669757230000 ms\n",
      "-------------------------------------------\n",
      "7\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669757230000 ms\n",
      "-------------------------------------------\n",
      "(a,3)\n",
      "(b,1)\n",
      "(c,2)\n",
      "(d,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669757230000 ms\n",
      "-------------------------------------------\n",
      "7\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669757240000 ms\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669757240000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669757250000 ms\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669757250000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "22/11/29 16:27:30 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/11/29 16:27:30 WARN BlockManager: Block input-0-1669757250000 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1669757250000 ms\n",
      "-------------------------------------------\n",
      "7\n",
      "\n",
      "22/11/29 16:27:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/11/29 16:27:31 WARN BlockManager: Block input-0-1669757251200 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/11/29 16:27:32 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/11/29 16:27:32 WARN BlockManager: Block input-0-1669757252400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/11/29 16:27:33 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/11/29 16:27:33 WARN BlockManager: Block input-0-1669757253400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/11/29 16:27:34 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/11/29 16:27:34 WARN BlockManager: Block input-0-1669757254400 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1669757260000 ms\n",
      "-------------------------------------------\n",
      "5\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669757260000 ms\n",
      "-------------------------------------------\n",
      "(a,1)\n",
      "(q,1)\n",
      "(s,1)\n",
      "(e,1)\n",
      "(g,1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 16:27:48 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "22/11/29 16:27:48 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/11/29 16:27:48 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/11/29 16:27:48 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "22/11/29 16:27:48 WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>To pretty print, we can add  <span style=\"color:blue\">foreachRDD</span></li>\n",
    "<li><span style=\"color:blue\">foreachRDD</span> is an iterator through a collection of DStream RDDs</li>\n",
    "<li>In this example, we use it for printing</li>\n",
    "<li>Since wcount is an RDD, we need to extract the number of elements from it</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Example: foreachRDD</span>\n",
    "<li>DStream objects consist of RDDs</li>\n",
    "<li>foreachRDD applies a function to each RDD in a DStream object</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://vickyzmbp-2:4042\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1670087311860)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@175d7431\n",
       "lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@67ea2d09\n",
       "words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@59745967\n",
       "wcount: org.apache.spark.streaming.dstream.DStream[Long] = org.apache.spark.streaming.dstream.MappedDStream@1fc78c96\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "val words = lines.flatMap(l => l.split(\" \"))\n",
    "\n",
    "//Total words in the microbatch (10 seconds)\n",
    "//This will print every 10 seconds\n",
    "words.count.print\n",
    "\n",
    "//Total number of words in every 30 second window sliding every 20 seconds\n",
    "//The print is at each point that the window slides (20, 40, 60 ,80,....)\n",
    "//This will print at 20 (total words in 20 seconds since this is the first slide)\n",
    "//at 40 seconds (total words in the last 3 10 second intervals)\n",
    "//at 60 seconds, etc.\n",
    "val wcount = words.countByWindow(Seconds(30),Seconds(20))\n",
    "wcount.foreachRDD((rdd,time) => print(time.toString.takeRight(10),\": Total  \",rdd.collect()(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 12:08:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/12/03 12:08:43 WARN BlockManager: Block input-0-1670087322800 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/12/03 12:08:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/12/03 12:08:47 WARN BlockManager: Block input-0-1670087327000 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/12/03 12:08:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/12/03 12:08:47 WARN BlockManager: Block input-0-1670087327200 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1670087330000 ms\n",
      "-------------------------------------------\n",
      "69\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1670087340000 ms\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "(7340000 ms,: Total  ,69)"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/03 12:09:05 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/12/03 12:09:05 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "22/12/03 12:09:05 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/12/03 12:09:05 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "22/12/03 12:09:05 WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Example: countByValueAndWindow</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Microbatch size: 10 seconds</li>\n",
    "<li>Window length: 30 seconds</li>\n",
    "<li>Slide interval: 20 seconds</li>\n",
    "<li>Calculate the total instances of each word in a window</li>\n",
    "<li>We'll use countByValueAndWindow for this</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@6a5ae9aa\n",
       "lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@65fb7203\n",
       "words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@b05a14f\n",
       "pairs: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.MappedDStream@6f4f7882\n",
       "windowedWordCounts1: org.apache.spark.streaming.dstream.DStream[((String, Int), Long)] = org.apache.spark.streaming.dstream.ReducedWindowedDStream@5265db78\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "val words = lines.flatMap(l => l.split(\" \"))\n",
    "\n",
    "// Construct the (word,1) pairs\n",
    "val pairs = words.map(word => (word, 1))\n",
    "pairs.print()\n",
    "\n",
    "val windowedWordCounts1 = pairs.countByValueAndWindow(Seconds(30),Seconds(20))\n",
    "// val windowedWordCounts1 = words.countByValueAndWindow(Seconds(30),Seconds(20))\n",
    "\n",
    "windowedWordCounts1.print\n",
    "\n",
    "windowedWordCounts1.foreachRDD((rdd,time) => {\n",
    "    println(time.toString.takeRight(15));\n",
    "    rdd.foreach(r => println(r._1._1,r._2))\n",
    "})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1669757840000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669757850000 ms\n",
      "-------------------------------------------\n",
      "(jim,1)\n",
      "(james,1)\n",
      "(nacy,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669757850000 ms\n",
      "-------------------------------------------\n",
      "((james,1),1)\n",
      "((nacy,1),1)\n",
      "((jim,1),1)\n",
      "\n",
      "669757850000 ms\n",
      "(james,1)\n",
      "(nacy,1)\n",
      "(jim,1)\n",
      "-------------------------------------------\n",
      "Time: 1669757860000 ms\n",
      "-------------------------------------------\n",
      "(vicky,1)\n",
      "(jim,1)\n",
      "(james,1)\n",
      "(vicky,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669757870000 ms\n",
      "-------------------------------------------\n",
      "(jim,1)\n",
      "(james,1)\n",
      "(nacy,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669757870000 ms\n",
      "-------------------------------------------\n",
      "((vicky,1),2)\n",
      "((james,1),3)\n",
      "((nacy,1),2)\n",
      "((jim,1),3)\n",
      "\n",
      "669757870000 ms\n",
      "(vicky,2)\n",
      "(nacy,2)\n",
      "(james,3)\n",
      "(jim,3)\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 16:37:52 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Example: reduceByKeyAndWindow</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Microbatch size: 10 seconds</li>\n",
    "<li>Window length: 30 seconds</li>\n",
    "<li>Slide interval: 20 seconds</li>\n",
    "<li>Every time the window slides, some data goes out of the window and some data enters</li>\n",
    "<li>We'll score each word as follows:</li>\n",
    "<ul>\n",
    "    <li>Add the instances of the words that enter to the current total</li>\n",
    "    <li>Subtract half the instances of the words that leave to the current total</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@2013c08\n",
       "lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@6a1b3ba6\n",
       "words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@5599dee\n",
       "pairs: org.apache.spark.streaming.dstream.DStream[(String, Double)] = org.apache.spark.streaming.dstream.MappedDStream@5ada69d0\n",
       "window_data: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.WindowedDStream@51b0efff\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "val words = lines.flatMap(l => l.split(\" \"))\n",
    "val pairs = words.map(word => (word, 1.0)) //Make this double so that the division by 2 works properly\n",
    "\n",
    "//Total words in the microbatch (10 seconds)\n",
    "// pairs.count.print\n",
    "\n",
    "//All the words in a 20 second window\n",
    "val window_data = words.window(Seconds(20))\n",
    "\n",
    "//Each word count, in a 60 second window, sliding every 20 seconds. The word counts are incremented by\n",
    "// the new words that enter every 20 seconds and decreased by half the count of the departing words (i.e., \n",
    "//of the words that go out of the window\n",
    "// val windowedWordCounts = pairs.reduceByKeyAndWindow((x, y)=> x + y,(x,y)=> x - 0.25*y, Seconds(60), Seconds(20))\n",
    "\n",
    "\n",
    "window_data.foreachRDD((r,time) => print(time.toString.takeRight(7)))\n",
    "// pairs.print\n",
    "// windowedWordCounts.print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000 ms22/12/06 16:33:22 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/12/06 16:33:22 WARN BlockManager: Block input-0-1670362402400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/12/06 16:33:26 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/12/06 16:33:26 WARN BlockManager: Block input-0-1670362405800 replicated to only 0 peer(s) instead of 1 peers\n",
      "0000 ms0000 ms"
     ]
    }
   ],
   "source": [
    "ssc.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/06 16:33:43 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "22/12/06 16:33:43 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/12/06 16:33:43 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/12/06 16:33:43 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "22/12/06 16:33:44 WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
