{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">DStream transformations</h1>\n",
    "<br><br>\n",
    "<li><b>map</b>: map, what else!</li>\n",
    "<li><b>flatMap</b>: flatMap, what else!</li>\n",
    "<li><b>filter</b>: filter</li>\n",
    "<li><b>repartition</b>: changes the number of partitions (increase or decrease) for the DStream</li>\n",
    "<li><b>count</b>: the number of elements in the RDD of the source dstream</li>\n",
    "<li><b>countByValue</b>: computes the frequency of each key and returns a DStream of (key,count) pairs</li>\n",
    "<li><b>union</b>: union of two DStreams</li>\n",
    "<li><b>reduceByKey</b>: reduceByKey</li>\n",
    "<li><b>updateStateByKey</b>: applies a function to a DStream to update values for a given key</li>\n",
    "<li><b>transform</b>: transform a dstream into a new dstream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">DStream union</span>\n",
    "<br><br>\n",
    "<li>Returns the union of two DStream objects</li>\n",
    "<li>Use this to combine data arriving from two different streams</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.56.170.160:4043\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1669753030397)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j.Logger\n",
       "import org.apache.log4j.Level\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.Logger\n",
    "import org.apache.log4j.Level\n",
    "\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "Logger.getLogger(\"akka\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@50c58355\n",
       "lines1: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@70b0443c\n",
       "lines2: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@5494de26\n",
       "lines3: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.UnionDStream@194de938\n",
       "words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@3cee1ede\n",
       "pairs: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.MappedDStream@4...\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "\n",
    "val ssc = new StreamingContext(sc,Seconds(10.toLong))\n",
    "val lines1 = ssc.socketTextStream(\"localhost\", 4444)\n",
    "val lines2 = ssc.socketTextStream(\"localhost\",9999)\n",
    "val lines3 = lines1.union(lines2)\n",
    "val words = lines3.flatMap(line => line.split(\" \"))\n",
    "val pairs = words.map(word => (word, 1))\n",
    "val wordCounts = pairs.reduceByKey((x, y) => x + y)\n",
    "wordCounts.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1669753280000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669753290000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669753300000 ms\n",
      "-------------------------------------------\n",
      "(a,1)\n",
      "(is,1)\n",
      "(from,1)\n",
      "(not,1)\n",
      "(it,1)\n",
      "(4,1)\n",
      "(boy,1)\n",
      "(good,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669753310000 ms\n",
      "-------------------------------------------\n",
      "(a,1)\n",
      "(I,1)\n",
      "(9,1)\n",
      "(is,1)\n",
      "(from,1)\n",
      "(it,1)\n",
      "(am,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669753320000 ms\n",
      "-------------------------------------------\n",
      "(a,1)\n",
      "(not,1)\n",
      "(good,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669753330000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669753340000 ms\n",
      "-------------------------------------------\n",
      "(a,2)\n",
      "(I,1)\n",
      "(9,1)\n",
      "(is,1)\n",
      "(from,1)\n",
      "(not,1)\n",
      "(it,1)\n",
      "(am,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669753350000 ms\n",
      "-------------------------------------------\n",
      "(,1)\n",
      "(a,1)\n",
      "(is,1)\n",
      "(from,1)\n",
      "(not,1)\n",
      "(it,1)\n",
      "(4,1)\n",
      "(boy,1)\n",
      "(good,2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669753360000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669753370000 ms\n",
      "-------------------------------------------\n",
      "(,1)\n",
      "(a,3)\n",
      "(I,1)\n",
      "(9,1)\n",
      "(is,2)\n",
      "(from,2)\n",
      "(not,2)\n",
      "(it,2)\n",
      "(4,1)\n",
      "(am,1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669753380000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669753390000 ms\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 15:23:16 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "22/11/29 15:23:16 ERROR ReceiverTracker: Deregistered receiver for stream 1: Stopped by driver\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">Checkpointing</span>\n",
    "<li>A stream runs continuously for a very long time and must be resilient to failures</li>\n",
    "<li>checkpointing is a mechanism for recovering from failures</li>\n",
    "<li>After failure, a stream can be initialized from an existing checkpoint</li>\n",
    "<li>A checkpoint must be created if a <span style=\"color:blue\">stateful</span> transformation is being used</li>\n",
    "<li>Checkpointing, in a stateful transaction, computes the value of an RDD and saves it. Recovery then starts from this value</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Stateful transformations and checkpointing</span>\n",
    "<li>The value of stateful results (e.g., averages/sums) at a point in time depends on all the prior RDDs</li>\n",
    "<li>Spark requires that stateful transformations be checkpointed</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Setting the checkpoint</span>\n",
    "<li>Specify the location where checkpoints will be saved</li>\n",
    "<li>On HDFS, the location will be distributed and fault tolerant</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.checkpoint(\"checkpoint\") //checkpoint is the directory where checkpoint data will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">DStream updateStateByKey</span>\n",
    "<li>updateStateByKey maintains state information that is updated by each batch\n",
    "<li>Need to define a state (runningCount in example below)\n",
    "<li>And define a state update function (updateFunction in example below)\n",
    "<li>Note the ByKey part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Set up the streaming context</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@5f39a1bd\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "val ssc = new StreamingContext(sc, Seconds(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Create a function that updates the state</span>\n",
    "<li>We need to keep a running total for each key</li>\n",
    "<li>So that's what we will update</li>\n",
    "<li>We need to initialize values for each key's running total so we'll use Option[Int] for that (either it exists or it doesn't)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Define an update function</span>\n",
    "<li> Simple function. Set value to 0 if it doesn't already exist\n",
    "<li> then add in the new value and return the updated value\n",
    "<li> This function will be applied to updateStateByKey\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateFunction: (Seq[Int], Option[Int]) => Some[Int] = $Lambda$3737/0x00000008010d3040@5396f126\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val updateFunction = (nv: Seq[Int], rc: Option[Int]) => { //nv = new value; rc = running count\n",
    "    val uc = rc.getOrElse(0) + nv.sum  //If rc does not exist, set it to 0, otherwise use the existing value\n",
    "    Some(uc) //return Some(uc)  \n",
    "//     uc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Some[Int] = Some(17)\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updateFunction(Seq(5,7),Some(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Some[Int] = Some(0)\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updateFunction(Seq(),None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:blue;font-size:large\">Do the transformations (adding updateStateByKey)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@76e93f62\n",
       "lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@b069e4f\n",
       "words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@40d162b5\n",
       "pairs: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.MappedDStream@9221be0\n",
       "runningCount: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.StateDStream@39382edc\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1669754200000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669754220000 ms\n",
      "-------------------------------------------\n",
      "(a,1)\n",
      "(b,2)\n",
      "(c,1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669754240000 ms\n",
      "-------------------------------------------\n",
      "(a,2)\n",
      "(b,4)\n",
      "(c,2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669754260000 ms\n",
      "-------------------------------------------\n",
      "(a,2)\n",
      "(b,4)\n",
      "(c,2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669754280000 ms\n",
      "-------------------------------------------\n",
      "(a,2)\n",
      "(b,4)\n",
      "(c,2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "\n",
    "val updateFunction = (nv: Seq[Int], rc: Option[Int]) => { //nv = new value; rc = running count\n",
    "    val uc = rc.getOrElse(0) + nv.sum  //If rc does not exist, set it to 0, otherwise use the existing value\n",
    "    Some(uc) //return Some(uc)  \n",
    "}\n",
    "\n",
    "val ssc = new StreamingContext(sc, Seconds(20))\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "\n",
    "val words = lines.flatMap(line => line.split(\" \"))\n",
    "val pairs = words.map(word => (word, 1))\n",
    "\n",
    "//val runningCount = pairs.updateStateByKey[Int]((a: Seq[Int],b: Option[Int]) => Some(b.getOrElse(0) + a.sum))\n",
    "val runningCount = pairs.updateStateByKey[Int](updateFunction) //define a state & use update function \n",
    "runningCount.print()\n",
    "ssc.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 15:38:08 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:xx-large\">DStream transform</span>\n",
    "<li>The <b>transform</b> function on a DStream returns a new DStream by applying a function on each rdd in the DStream</li>\n",
    "<li>It is particularly useful for joining a static RDD to the RDDs in a DStream</li>\n",
    "<li>Example: Individual sales of products are coming in on a stream. Calculate revenue on each sale by multiplying by prices in a static RDD</li>\n",
    "<pre>\n",
    "Example Sales\n",
    "A,7\n",
    "B,2\n",
    "A,11\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@6bb3bad7\n",
       "prices: Array[(String, Double)] = Array((A,10.2), (B,7.4))\n",
       "pricesRDD: org.apache.spark.rdd.RDD[(String, Double)] = ParallelCollectionRDD[32] at parallelize at <console>:36\n",
       "lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@3ba37767\n",
       "sales: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.MappedDStream@6af7abd6\n",
       "salesPrices: org.apache.spark.streaming.dstream.DStream[(String, (Int, Double))] = org.apache.spark.streaming.dstream.TransformedDStream@5da96aaf\n",
       "revenue: org.apache.spark.streaming.dstream.DStream[...\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "\n",
    "val ssc = new StreamingContext(sc,Seconds(10.toLong))\n",
    "val prices = Array((\"A\",10.2),(\"B\",7.4))\n",
    "val pricesRDD = sc.parallelize(prices)\n",
    "\n",
    "val lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "val sales = lines.map(line=>line.split(\",\"))\n",
    "        .map(l=>(l(0),l(1).toInt))\n",
    "val salesPrices = sales.transform(rdd => rdd.join(pricesRDD))\n",
    "val revenue = salesPrices.map(t => (t._1,t._2._1*t._2._2))\n",
    "revenue.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1671419310000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "22/12/18 22:08:30 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/12/18 22:08:30 WARN BlockManager: Block input-0-1671419310600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/12/18 22:08:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/12/18 22:08:31 WARN BlockManager: Block input-0-1671419311000 replicated to only 0 peer(s) instead of 1 peers\n",
      "-------------------------------------------\n",
      "Time: 1671419320000 ms\n",
      "-------------------------------------------\n",
      "(A,71.39999999999999)\n",
      "(A,112.19999999999999)\n",
      "(B,14.8)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/18 22:08:45 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "22/12/18 22:08:45 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/12/18 22:08:45 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/12/18 22:08:45 WARN ReceiverSupervisorImpl: Receiver has been stopped\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
