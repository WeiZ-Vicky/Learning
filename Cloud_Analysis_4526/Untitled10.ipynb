{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c8394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://dyn-209-2-225-243.dyn.columbia.edu:4049\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1671636219550)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/21 10:25:00 ERROR Utils: uncaught error in thread Spark Context Cleaner, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/12/21 10:25:00 ERROR Utils: uncaught error in thread spark-listener-group-appStatus, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/12/21 10:25:00 ERROR Utils: throw uncaught fatal error in thread Spark Context Cleaner\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/12/21 10:25:00 ERROR Utils: throw uncaught fatal error in thread spark-listener-group-appStatus\n",
      "java.lang.OutOfMemoryError: Java heap space\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val vertexArray = Array(\n",
    "  (1, \"A\"),\n",
    "  (2, \"B\"),\n",
    "  (3, \"C\"),\n",
    "  (4, \"D\"),\n",
    "  (5, \"E\"),\n",
    "  (6, \"F\"),\n",
    "  (7, \"G\"),\n",
    ")\n",
    "\n",
    "val edgeArray = Array(\n",
    "    (1,2,3),\n",
    "    (1,6,5),\n",
    "    (1,3,7),\n",
    "    (2,6,10),\n",
    "    (2,7,1),\n",
    "    (3,6,2),\n",
    "    (3,4,6),\n",
    "    (4,6,8),\n",
    "    (4,5,5),\n",
    "    (5,6,7),\n",
    "    (5,7,3),\n",
    "    (6,7,6)\n",
    ")\n",
    "\n",
    "val vertexArrayX = vertexArray.map(r => (r._1.toLong,r._2))\n",
    "val edgeArrayX = edgeArray.map(r => Edge(r._1.toLong,r._2.toLong,r._3))\n",
    "\n",
    "//val vertexRDD = sc.parallelize(vertexArrayX)\n",
    "val vertexRDD: RDD[(Long, String)] = sc.parallelize(vertexArrayX)\n",
    "val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArrayX)\n",
    "\n",
    "val graph: Graph[String, Int] = Graph(vertexRDD, edgeRDD)\n",
    "\n",
    "val sourceId: VertexId = 1 //Set the source for the maximal flows\n",
    "val initialGraph =  graph.mapVertices((id, _) =>\n",
    "    if (id == sourceId) Double.PositiveInfinity else 0.0)\n",
    "\n",
    "\n",
    "val vertex_program = (id: VertexId, weight: Double, newWeight: Double) => math.max(weight, newWeight)\n",
    "\n",
    "val sendMsg = (triplet: EdgeTriplet[Double,Int]) => { \n",
    "    if (triplet.srcAttr > triplet.dstAttr & triplet.attr > triplet.dstAttr) {\n",
    "      Iterator((triplet.dstId\n",
    "//                 , List(triplet.srcId):::List(triplet.dstId)\n",
    "                , triplet.dstAttr))\n",
    "    } else {\n",
    "      Iterator.empty\n",
    "    }\n",
    "  }\n",
    "//Send message (return an iterator)\n",
    "\n",
    "// val mrgMsg = (a: (List,Double), b: (List,Double)) => {if (a._2>b._2) a}//merge messages\n",
    "val mrgMsg = (a: Double, b: Double) => math.max(a,b)\n",
    "val initial_message = 0.0//what should the initial message be!\n",
    "\n",
    "val sssp = initialGraph.pregel(initial_message)(\n",
    "  vertex_program,\n",
    "    sendMsg,\n",
    "    mrgMsg)\n",
    "\n",
    "println(sssp.vertices.collect.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85493066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a951fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.packages= [\"graphframes:graphframes:0.8.2-spark3.2-s_2.12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf57eb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.graphframes._\n",
       "vertexArray: Array[(Int, String, Int)] = Array((1,Alice,28), (2,Bob,27), (3,Charlie,65), (4,David,42), (5,Ed,55), (6,Fran,50), (7,Qing,27), (8,Sarika,78), (9,Olafson,17), (10,Birgit,33))\n",
       "edgeArray: Array[(Int, Int, Int)] = Array((2,1,7), (1,2,13), (2,4,2), (3,2,4), (3,6,3), (4,1,1), (5,2,2), (5,3,8), (5,6,3), (7,8,14), (7,9,2), (8,10,8), (9,10,6))\n",
       "vertex_df: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]\n",
       "edge_df: org.apache.spark.sql.DataFrame = [src: int, dst: int ... 1 more field]\n",
       "g: org.graphframes.GraphFrame = GraphFrame(v:[id: int, name: string ... 1 more field], e:[src: int, dst: int ... 1 more field])\n",
       "generateAllPaths: (g_f: org.graphframes.GraphFrame, n: Int)...\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.graphframes._\n",
    "val vertexArray = Array(\n",
    "  (1, \"Alice\", 28),\n",
    "  (2, \"Bob\", 27),\n",
    "  (3, \"Charlie\", 65),\n",
    "  (4, \"David\", 42),\n",
    "  (5, \"Ed\", 55),\n",
    "  (6, \"Fran\", 50),\n",
    "    (7, \"Qing\",27),\n",
    "    (8, \"Sarika\",78),\n",
    "    (9, \"Olafson\",17),\n",
    "    (10, \"Birgit\",33)\n",
    ")\n",
    "\n",
    "val edgeArray = Array(\n",
    "  (2, 1, 7),\n",
    "  (1, 2, 13),\n",
    "  (2, 4, 2),\n",
    "  (3, 2, 4),\n",
    "  (3, 6, 3),\n",
    "  (4, 1, 1),\n",
    "  (5, 2, 2),\n",
    "  (5, 3, 8),\n",
    "  (5, 6, 3),\n",
    "    (7, 8, 14),\n",
    "    (7, 9, 2),\n",
    "    (8, 10, 8),\n",
    "    (9, 10, 6)\n",
    ")\n",
    "\n",
    "val vertex_df = spark.createDataFrame(vertexArray).toDF(\"id\",\"name\",\"age\")\n",
    "val edge_df = spark.createDataFrame(edgeArray).toDF(\"src\",\"dst\",\"attr\")\n",
    "\n",
    "val g = GraphFrame(vertex_df, edge_df)\n",
    "\n",
    "def generateAllPaths(g_f: GraphFrame,n: Int) = {\n",
    "        val path = (1 to n).map(i => s\"(n$i)-[e$i]->(n${i + 1})\").mkString(\";\")\n",
    "        val candidates = g_f.find(path)\n",
    "        candidates\n",
    "    }\n",
    "def removeCycles(df: DataFrame,n: Int) = {\n",
    "    val combs = (1 to n+1).map(i => s\"n$i\").combinations(2) //n+1 nodes for n paths\n",
    "    combs.foldLeft(df)((r,c) => r.filter(c.toSeq(0).toString+\"!=\"+c.toSeq(1).toString) )\n",
    "}\n",
    "\n",
    "val new_g = removeCycles(generateAllPaths(g,2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76ccbfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+----------------+----------+----------------+\n",
      "|              n1|        e1|              n2|        e2|              n3|\n",
      "+----------------+----------+----------------+----------+----------------+\n",
      "|  {1, Alice, 28}|{1, 2, 13}|    {2, Bob, 27}| {2, 4, 2}|  {4, David, 42}|\n",
      "|    {2, Bob, 27}| {2, 4, 2}|  {4, David, 42}| {4, 1, 1}|  {1, Alice, 28}|\n",
      "|{3, Charlie, 65}| {3, 2, 4}|    {2, Bob, 27}| {2, 4, 2}|  {4, David, 42}|\n",
      "|{3, Charlie, 65}| {3, 2, 4}|    {2, Bob, 27}| {2, 1, 7}|  {1, Alice, 28}|\n",
      "|  {4, David, 42}| {4, 1, 1}|  {1, Alice, 28}|{1, 2, 13}|    {2, Bob, 27}|\n",
      "|     {5, Ed, 55}| {5, 3, 8}|{3, Charlie, 65}| {3, 6, 3}|   {6, Fran, 50}|\n",
      "|     {5, Ed, 55}| {5, 3, 8}|{3, Charlie, 65}| {3, 2, 4}|    {2, Bob, 27}|\n",
      "|     {5, Ed, 55}| {5, 2, 2}|    {2, Bob, 27}| {2, 4, 2}|  {4, David, 42}|\n",
      "|     {5, Ed, 55}| {5, 2, 2}|    {2, Bob, 27}| {2, 1, 7}|  {1, Alice, 28}|\n",
      "|   {7, Qing, 27}| {7, 9, 2}|{9, Olafson, 17}|{9, 10, 6}|{10, Birgit, 33}|\n",
      "|   {7, Qing, 27}|{7, 8, 14}| {8, Sarika, 78}|{8, 10, 8}|{10, Birgit, 33}|\n",
      "+----------------+----------+----------------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_g.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f67921b",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Reference 'name' is ambiguous, could be: name, name, name.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Reference 'name' is ambiguous, could be: name, name, name.",
      "  at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:369)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:114)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$resolveExpressionByPlanChildren$1(Analyzer.scala:1811)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$resolveExpression$2(Analyzer.scala:1739)",
      "  at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:60)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.innerResolve$1(Analyzer.scala:1746)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.resolveExpression(Analyzer.scala:1766)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.resolveExpressionByPlanChildren(Analyzer.scala:1817)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$15.$anonfun$applyOrElse$80(Analyzer.scala:1525)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:200)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:200)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:211)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:216)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:216)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:221)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:427)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:221)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$15.applyOrElse(Analyzer.scala:1525)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$15.applyOrElse(Analyzer.scala:1362)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:1362)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:1342)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)",
      "  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)",
      "  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)",
      "  at scala.collection.immutable.List.foldLeft(List.scala:91)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)",
      "  at scala.collection.immutable.List.foreach(List.scala:431)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)",
      "  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)",
      "  at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3887)",
      "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1519)",
      "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1536)",
      "  ... 61 elided",
      ""
     ]
    }
   ],
   "source": [
    "new_g.select($\"n1.name\",$\"e1.attr\" as \"att1\",$\"n2.name\",$\"e2.attr\" as \"att2\",$\"n3.name\")\n",
    ".filter(\"att1>att2\").select(\"name\",\"att2\",\"name\")\n",
    ".union(new_g.select($\"n1.name\",$\"e1.attr\" as \"att1\",$\"n2.name\",$\"e2.attr\" as \"att2\",$\"n3.name\")\n",
    ".filter(\"att1<att2\").select(\"name\",\"att1\",\"name\"))\n",
    ".show\n",
    "// .select($\"a.name\" as \"src\",$\"c.name\" as \"dst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a531e743",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "66: error: not found: value union",
     "output_type": "error",
     "traceback": [
      "<console>:66: error: not found: value union",
      "       union(new_g.select($\"n1.name\" as \"from\",$\"e1.attr\" as \"att1\",$\"n2.name\" as \"via\",$\"e2.attr\" as \"att2\",$\"n3.name\" as \"to\")",
      "       ^",
      ""
     ]
    }
   ],
   "source": [
    "new_g.select($\"n1.name\" as \"from\",$\"e1.attr\" as \"att1\",$\"n2.name\" as \"via\",$\"e2.attr\" as \"att2\",$\"n3.name\" as \"to\")\n",
    ".filter(\"att1<att2\")\n",
    ".select(\"from\",\"via\",\"to\",\"att1\")\n",
    "// .show\n",
    "union(new_g.select($\"n1.name\" as \"from\",$\"e1.attr\" as \"att1\",$\"n2.name\" as \"via\",$\"e2.attr\" as \"att2\",$\"n3.name\" as \"to\")\n",
    ".filter(\"att2<att1\")\n",
    ".select(\"from\",\"via\",\"to\",\"att2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6515113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max(total: Column, likes: Column): Column = {\n",
    "    max(pre,e)\n",
    "}\n",
    "val condition = (1 to n).map(i=>\"e\"+i.toString).toSeq\n",
    "    .foldLeft(lit(0))((pre, e) => max(pre, col(e)(\"attr\")))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
